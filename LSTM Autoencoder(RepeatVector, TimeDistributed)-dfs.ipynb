{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob, os\n",
    "from keras import layers as ly\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras.backend.tensorflow_backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = './dfs_sequence/*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file read\n",
    "all_data = [] (ㅣ낸 \n",
    "sequence_length = []\n",
    "name = []\n",
    "for file in sorted(glob.glob(dir)):\n",
    "    name.append(file.split('/')[2].replace('.txt', ''))\n",
    "    datasets = []\n",
    "    for f in open(file, 'r'):\n",
    "        (u, v, w) = f[1:-2].split(',')\n",
    "        datasets.append([int(u), int(v), float(w)])\n",
    "    sequence_length.append(len(datasets))\n",
    "    all_data.append(datasets)\n",
    "#all_data = np.array(all_data)\n",
    "all_data = np.array([np.array(arr) for arr in all_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, train_name, test_name = train_test_split(all_data, name, test_size=0.3)\n",
    "x_test, x_val, test_name, val_name = train_test_split(x_test, test_name, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 3\n",
    "#batch_size = 32\n",
    "epochs = 500\n",
    "steps_per_epoch = len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "661/661 [==============================] - 15s 23ms/step - loss: 47.3037\n",
      "Epoch 2/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 32.8569\n",
      "Epoch 3/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 29.9195\n",
      "Epoch 4/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 26.4544\n",
      "Epoch 5/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 24.0121\n",
      "Epoch 6/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 22.6473\n",
      "Epoch 7/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 21.4523\n",
      "Epoch 8/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 19.9396\n",
      "Epoch 9/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 18.8446\n",
      "Epoch 10/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 18.6257\n",
      "Epoch 11/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 19.7394\n",
      "Epoch 12/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 18.4078\n",
      "Epoch 13/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 17.3607\n",
      "Epoch 14/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 17.2879\n",
      "Epoch 15/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 17.1149\n",
      "Epoch 16/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 15.9782\n",
      "Epoch 17/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 15.5074\n",
      "Epoch 18/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 14.8916\n",
      "Epoch 19/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 14.2054\n",
      "Epoch 20/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 12.9003\n",
      "Epoch 21/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 12.7898\n",
      "Epoch 22/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 12.5215\n",
      "Epoch 23/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 12.7015\n",
      "Epoch 24/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 11.7094\n",
      "Epoch 25/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 11.1242\n",
      "Epoch 26/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 13.4545\n",
      "Epoch 27/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 11.4947\n",
      "Epoch 28/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 11.3188\n",
      "Epoch 29/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 10.2354\n",
      "Epoch 30/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 10.2866\n",
      "Epoch 31/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 10.6964\n",
      "Epoch 32/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 10.3835\n",
      "Epoch 33/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 10.4511\n",
      "Epoch 34/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 9.0568\n",
      "Epoch 35/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 11.0899\n",
      "Epoch 36/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 8.9182\n",
      "Epoch 37/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 8.9219\n",
      "Epoch 38/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 8.7036\n",
      "Epoch 39/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 8.0847\n",
      "Epoch 40/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 9.9699\n",
      "Epoch 41/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 8.3043\n",
      "Epoch 42/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 8.4162\n",
      "Epoch 43/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 10.1932\n",
      "Epoch 44/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 7.9207\n",
      "Epoch 45/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 8.2288\n",
      "Epoch 46/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 8.7536\n",
      "Epoch 47/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 7.6194\n",
      "Epoch 48/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 6.3663\n",
      "Epoch 49/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 6.2330\n",
      "Epoch 50/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 6.5942\n",
      "Epoch 51/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 7.4563\n",
      "Epoch 52/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 9.4758\n",
      "Epoch 53/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 6.7313\n",
      "Epoch 54/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 8.0244\n",
      "Epoch 55/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 7.2628\n",
      "Epoch 56/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 5.9613\n",
      "Epoch 57/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 5.1995\n",
      "Epoch 58/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 5.9112\n",
      "Epoch 59/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 6.9701\n",
      "Epoch 60/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 6.5286\n",
      "Epoch 61/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 5.3629\n",
      "Epoch 62/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 5.6325\n",
      "Epoch 63/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 5.2223\n",
      "Epoch 64/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 5.3344\n",
      "Epoch 65/500\n",
      "661/661 [==============================] - 12s 18ms/step - loss: 5.5871\n",
      "Epoch 66/500\n",
      "661/661 [==============================] - 12s 19ms/step - loss: 7.8637\n",
      "Epoch 67/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 8.0103\n",
      "Epoch 68/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 4.9043\n",
      "Epoch 69/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 4.8232\n",
      "Epoch 70/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 4.3226\n",
      "Epoch 71/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 4.2489\n",
      "Epoch 72/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 5.1139\n",
      "Epoch 73/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 5.4413\n",
      "Epoch 74/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 5.2630\n",
      "Epoch 75/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 5.7953\n",
      "Epoch 76/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 4.4523\n",
      "Epoch 77/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 4.2413\n",
      "Epoch 78/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 4.5609\n",
      "Epoch 79/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 5.5848\n",
      "Epoch 80/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 4.6941\n",
      "Epoch 81/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 3.5195\n",
      "Epoch 82/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 3.7609\n",
      "Epoch 83/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 3.6244\n",
      "Epoch 84/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 4.1268\n",
      "Epoch 85/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 3.6689\n",
      "Epoch 86/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 4.1171\n",
      "Epoch 87/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 4.9644\n",
      "Epoch 88/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 4.4330\n",
      "Epoch 89/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 3.6777\n",
      "Epoch 90/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 3.4843\n",
      "Epoch 91/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 4.0143\n",
      "Epoch 92/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 2.8311\n",
      "Epoch 93/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 2.8369\n",
      "Epoch 94/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 4.2690\n",
      "Epoch 95/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 4.7675\n",
      "Epoch 96/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 4.3536\n",
      "Epoch 97/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "661/661 [==============================] - 10s 15ms/step - loss: 2.8607\n",
      "Epoch 98/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 2.5616\n",
      "Epoch 99/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 2.5750\n",
      "Epoch 100/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 4.7124\n",
      "Epoch 101/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 4.0930\n",
      "Epoch 102/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 2.7874\n",
      "Epoch 103/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 2.5025\n",
      "Epoch 104/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 2.1881\n",
      "Epoch 105/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 3.3412\n",
      "Epoch 106/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 4.1738\n",
      "Epoch 107/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 3.0216\n",
      "Epoch 108/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 2.6613\n",
      "Epoch 109/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 2.1919\n",
      "Epoch 110/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 2.1471\n",
      "Epoch 111/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 4.5302\n",
      "Epoch 112/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 4.6999\n",
      "Epoch 113/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 3.8511\n",
      "Epoch 114/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 2.3104\n",
      "Epoch 115/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 1.7991\n",
      "Epoch 116/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 1.6716\n",
      "Epoch 117/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 2.0466\n",
      "Epoch 118/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 2.1152\n",
      "Epoch 119/500\n",
      "661/661 [==============================] - 11s 17ms/step - loss: 3.7463\n",
      "Epoch 120/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 3.1848\n",
      "Epoch 121/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 1.9154\n",
      "Epoch 122/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 1.6311\n",
      "Epoch 123/500\n",
      "661/661 [==============================] - 11s 17ms/step - loss: 1.9911\n",
      "Epoch 124/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 3.7025\n",
      "Epoch 125/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 3.7861\n",
      "Epoch 126/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 4.6874\n",
      "Epoch 127/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 3.9714\n",
      "Epoch 128/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 2.6666\n",
      "Epoch 129/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 3.5745\n",
      "Epoch 130/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 2.0499\n",
      "Epoch 131/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 1.5040\n",
      "Epoch 132/500\n",
      "661/661 [==============================] - 11s 17ms/step - loss: 1.3632\n",
      "Epoch 133/500\n",
      "661/661 [==============================] - 11s 17ms/step - loss: 1.3588\n",
      "Epoch 134/500\n",
      "661/661 [==============================] - 11s 17ms/step - loss: 1.6401\n",
      "Epoch 135/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 4.2551\n",
      "Epoch 136/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 3.1376\n",
      "Epoch 137/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 1.7099\n",
      "Epoch 138/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 1.3625\n",
      "Epoch 139/500\n",
      "661/661 [==============================] - 11s 17ms/step - loss: 1.8215\n",
      "Epoch 140/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 2.1286\n",
      "Epoch 141/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 1.5533\n",
      "Epoch 142/500\n",
      "661/661 [==============================] - 11s 17ms/step - loss: 1.9304\n",
      "Epoch 143/500\n",
      "661/661 [==============================] - 11s 17ms/step - loss: 1.9614\n",
      "Epoch 144/500\n",
      "661/661 [==============================] - 11s 17ms/step - loss: 2.3550\n",
      "Epoch 145/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 2.1729\n",
      "Epoch 146/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 1.3788\n",
      "Epoch 147/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 1.3396\n",
      "Epoch 148/500\n",
      "661/661 [==============================] - 11s 17ms/step - loss: 1.2920\n",
      "Epoch 149/500\n",
      "661/661 [==============================] - 11s 17ms/step - loss: 3.8935\n",
      "Epoch 150/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 2.3034\n",
      "Epoch 151/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 1.5101\n",
      "Epoch 152/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 1.2510\n",
      "Epoch 153/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 1.0877\n",
      "Epoch 154/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 2.1682\n",
      "Epoch 155/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 1.7605\n",
      "Epoch 156/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 3.2269\n",
      "Epoch 157/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 1.6117\n",
      "Epoch 158/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 1.0910\n",
      "Epoch 159/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 0.9477\n",
      "Epoch 160/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 0.9780\n",
      "Epoch 161/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 1.1351\n",
      "Epoch 162/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 1.8633\n",
      "Epoch 163/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 2.8545\n",
      "Epoch 164/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 1.5062\n",
      "Epoch 165/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 1.0550\n",
      "Epoch 166/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 0.9211\n",
      "Epoch 167/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 1.3101\n",
      "Epoch 168/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 2.8265\n",
      "Epoch 169/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 1.2857\n",
      "Epoch 170/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 1.7066\n",
      "Epoch 171/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 1.3451\n",
      "Epoch 172/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 0.9339\n",
      "Epoch 173/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 0.8983\n",
      "Epoch 174/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 0.9654\n",
      "Epoch 175/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 2.3043\n",
      "Epoch 176/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 2.1692\n",
      "Epoch 177/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 1.3045\n",
      "Epoch 178/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 0.8691\n",
      "Epoch 179/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 0.7100\n",
      "Epoch 180/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 0.6940\n",
      "Epoch 181/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 2.1743\n",
      "Epoch 182/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 4.1287\n",
      "Epoch 183/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 2.2238\n",
      "Epoch 184/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 2.6809\n",
      "Epoch 185/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 1.2974\n",
      "Epoch 186/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 0.8387\n",
      "Epoch 187/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 0.6705\n",
      "Epoch 188/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 0.5976\n",
      "Epoch 189/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 0.6261\n",
      "Epoch 190/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 1.7561\n",
      "Epoch 191/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 1.6094\n",
      "Epoch 192/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "661/661 [==============================] - 11s 16ms/step - loss: 0.9258\n",
      "Epoch 193/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 1.1437\n",
      "Epoch 194/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 0.8914\n",
      "Epoch 195/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 0.6899\n",
      "Epoch 196/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 0.7247\n",
      "Epoch 197/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 1.0458\n",
      "Epoch 198/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 1.9995\n",
      "Epoch 199/500\n",
      "661/661 [==============================] - 13s 20ms/step - loss: 1.5130\n",
      "Epoch 200/500\n",
      "661/661 [==============================] - 12s 18ms/step - loss: 0.7401\n",
      "Epoch 201/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 0.7354\n",
      "Epoch 202/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 0.8473\n",
      "Epoch 203/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.6461\n",
      "Epoch 204/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 2.8550\n",
      "Epoch 205/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 1.7463\n",
      "Epoch 206/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 0.8631\n",
      "Epoch 207/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.5264\n",
      "Epoch 208/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.4396\n",
      "Epoch 209/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 0.4181\n",
      "Epoch 210/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.4994\n",
      "Epoch 211/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 0.9790\n",
      "Epoch 212/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 1.0531\n",
      "Epoch 213/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 1.4613\n",
      "Epoch 214/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 0.7117\n",
      "Epoch 215/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.6276\n",
      "Epoch 216/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 0.4762\n",
      "Epoch 217/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 0.4819\n",
      "Epoch 218/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.6736\n",
      "Epoch 219/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 1.1842\n",
      "Epoch 220/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 1.2781\n",
      "Epoch 221/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.6793\n",
      "Epoch 222/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 0.9366\n",
      "Epoch 223/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 1.7550\n",
      "Epoch 224/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 0.9408\n",
      "Epoch 225/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 1.8752\n",
      "Epoch 226/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 1.1556\n",
      "Epoch 227/500\n",
      "661/661 [==============================] - 11s 17ms/step - loss: 0.5316\n",
      "Epoch 228/500\n",
      "661/661 [==============================] - 11s 17ms/step - loss: 0.3697\n",
      "Epoch 229/500\n",
      "661/661 [==============================] - 11s 17ms/step - loss: 0.3157\n",
      "Epoch 230/500\n",
      "661/661 [==============================] - 11s 17ms/step - loss: 0.2999\n",
      "Epoch 231/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 0.3464\n",
      "Epoch 232/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 1.0924\n",
      "Epoch 233/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 3.0651\n",
      "Epoch 234/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 1.3994\n",
      "Epoch 235/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 0.5300\n",
      "Epoch 236/500\n",
      "661/661 [==============================] - 8s 13ms/step - loss: 0.3631\n",
      "Epoch 237/500\n",
      "661/661 [==============================] - 8s 12ms/step - loss: 0.3087\n",
      "Epoch 238/500\n",
      "661/661 [==============================] - 8s 12ms/step - loss: 0.2936\n",
      "Epoch 239/500\n",
      "661/661 [==============================] - 8s 12ms/step - loss: 0.3535\n",
      "Epoch 240/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 2.4186\n",
      "Epoch 241/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.9658\n",
      "Epoch 242/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.4550\n",
      "Epoch 243/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.3361\n",
      "Epoch 244/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.2878\n",
      "Epoch 245/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.3108\n",
      "Epoch 246/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.9211\n",
      "Epoch 247/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 1.0838\n",
      "Epoch 248/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.6705\n",
      "Epoch 249/500\n",
      "661/661 [==============================] - 8s 12ms/step - loss: 0.5815\n",
      "Epoch 250/500\n",
      "661/661 [==============================] - 8s 12ms/step - loss: 0.3932\n",
      "Epoch 251/500\n",
      "661/661 [==============================] - 8s 12ms/step - loss: 0.3623\n",
      "Epoch 252/500\n",
      "661/661 [==============================] - 8s 11ms/step - loss: 0.4000\n",
      "Epoch 253/500\n",
      "661/661 [==============================] - 8s 11ms/step - loss: 0.4812\n",
      "Epoch 254/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 1.8538\n",
      "Epoch 255/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 1.4000\n",
      "Epoch 256/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.5462\n",
      "Epoch 257/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.3000\n",
      "Epoch 258/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.2397\n",
      "Epoch 259/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.2235\n",
      "Epoch 260/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.2773\n",
      "Epoch 261/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 1.5513\n",
      "Epoch 262/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 1.6781\n",
      "Epoch 263/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 1.1794\n",
      "Epoch 264/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.5594\n",
      "Epoch 265/500\n",
      "661/661 [==============================] - 8s 12ms/step - loss: 0.2877\n",
      "Epoch 266/500\n",
      "661/661 [==============================] - 8s 12ms/step - loss: 0.2266\n",
      "Epoch 267/500\n",
      "661/661 [==============================] - 8s 12ms/step - loss: 0.2030\n",
      "Epoch 268/500\n",
      "661/661 [==============================] - 8s 12ms/step - loss: 0.2637\n",
      "Epoch 269/500\n",
      "661/661 [==============================] - 8s 12ms/step - loss: 2.9733\n",
      "Epoch 270/500\n",
      "661/661 [==============================] - 8s 12ms/step - loss: 2.1773\n",
      "Epoch 271/500\n",
      "661/661 [==============================] - 8s 12ms/step - loss: 0.6594\n",
      "Epoch 272/500\n",
      "661/661 [==============================] - 8s 12ms/step - loss: 0.3446\n",
      "Epoch 273/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.2448\n",
      "Epoch 274/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.2040\n",
      "Epoch 275/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.1858\n",
      "Epoch 276/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.1895\n",
      "Epoch 277/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.3700\n",
      "Epoch 278/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 1.6505\n",
      "Epoch 279/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.8831\n",
      "Epoch 280/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.3793\n",
      "Epoch 281/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.2465\n",
      "Epoch 282/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.2086\n",
      "Epoch 283/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.2501\n",
      "Epoch 284/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.6012\n",
      "Epoch 285/500\n",
      "661/661 [==============================] - 8s 11ms/step - loss: 3.0523\n",
      "Epoch 286/500\n",
      "661/661 [==============================] - 8s 12ms/step - loss: 0.9213\n",
      "Epoch 287/500\n",
      "661/661 [==============================] - 8s 12ms/step - loss: 0.3697\n",
      "Epoch 288/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "661/661 [==============================] - 8s 12ms/step - loss: 0.2380\n",
      "Epoch 289/500\n",
      "661/661 [==============================] - 8s 11ms/step - loss: 0.1939\n",
      "Epoch 290/500\n",
      "661/661 [==============================] - 8s 12ms/step - loss: 0.1731\n",
      "Epoch 291/500\n",
      "661/661 [==============================] - 8s 11ms/step - loss: 0.2065\n",
      "Epoch 292/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.8207\n",
      "Epoch 293/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.9591\n",
      "Epoch 294/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.4645\n",
      "Epoch 295/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.2426\n",
      "Epoch 296/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.1869\n",
      "Epoch 297/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.2004\n",
      "Epoch 298/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.3480\n",
      "Epoch 299/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.8164\n",
      "Epoch 300/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.6848\n",
      "Epoch 301/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.3590\n",
      "Epoch 302/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.2184\n",
      "Epoch 303/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.1812\n",
      "Epoch 304/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.2293\n",
      "Epoch 305/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.4333\n",
      "Epoch 306/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.4871\n",
      "Epoch 307/500\n",
      "661/661 [==============================] - 8s 11ms/step - loss: 0.4019\n",
      "Epoch 308/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.3064\n",
      "Epoch 309/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.2306\n",
      "Epoch 310/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.2383\n",
      "Epoch 311/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.9475\n",
      "Epoch 312/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.6274\n",
      "Epoch 313/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.2375\n",
      "Epoch 314/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.1538\n",
      "Epoch 315/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.1222\n",
      "Epoch 316/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.1413\n",
      "Epoch 317/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.3103\n",
      "Epoch 318/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.5096\n",
      "Epoch 319/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.4159\n",
      "Epoch 320/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.2448\n",
      "Epoch 321/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.1943\n",
      "Epoch 322/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.4335\n",
      "Epoch 323/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 1.2428\n",
      "Epoch 324/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.6797\n",
      "Epoch 325/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.2345\n",
      "Epoch 326/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.1426\n",
      "Epoch 327/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.1154\n",
      "Epoch 328/500\n",
      "661/661 [==============================] - 8s 12ms/step - loss: 0.1136\n",
      "Epoch 329/500\n",
      "661/661 [==============================] - 8s 12ms/step - loss: 0.2772\n",
      "Epoch 330/500\n",
      "661/661 [==============================] - 8s 12ms/step - loss: 1.6017\n",
      "Epoch 331/500\n",
      "661/661 [==============================] - 8s 11ms/step - loss: 0.5795\n",
      "Epoch 332/500\n",
      "661/661 [==============================] - 7s 11ms/step - loss: 0.3211\n",
      "Epoch 333/500\n",
      "661/661 [==============================] - 8s 11ms/step - loss: 0.2248\n",
      "Epoch 334/500\n",
      "661/661 [==============================] - 8s 11ms/step - loss: 0.1378\n",
      "Epoch 335/500\n",
      "661/661 [==============================] - 8s 12ms/step - loss: 0.1096\n",
      "Epoch 336/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.1206\n",
      "Epoch 337/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.2292\n",
      "Epoch 338/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 0.4569\n",
      "Epoch 339/500\n",
      "661/661 [==============================] - 11s 17ms/step - loss: 0.5247\n",
      "Epoch 340/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 0.2747\n",
      "Epoch 341/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 1.3788\n",
      "Epoch 342/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 1.2832\n",
      "Epoch 343/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 0.6873\n",
      "Epoch 344/500\n",
      "661/661 [==============================] - 11s 17ms/step - loss: 0.5343\n",
      "Epoch 345/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 0.2548\n",
      "Epoch 346/500\n",
      "661/661 [==============================] - 11s 17ms/step - loss: 0.1589\n",
      "Epoch 347/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 0.1119\n",
      "Epoch 348/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 0.0973\n",
      "Epoch 349/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 0.1036\n",
      "Epoch 350/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 0.1950\n",
      "Epoch 351/500\n",
      "661/661 [==============================] - 12s 18ms/step - loss: 1.0299\n",
      "Epoch 352/500\n",
      "661/661 [==============================] - 11s 17ms/step - loss: 1.5740\n",
      "Epoch 353/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 0.5686\n",
      "Epoch 354/500\n",
      "661/661 [==============================] - 9s 14ms/step - loss: 0.2349\n",
      "Epoch 355/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.1434\n",
      "Epoch 356/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.1067\n",
      "Epoch 357/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 0.0901\n",
      "Epoch 358/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 0.0873\n",
      "Epoch 359/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 0.1614\n",
      "Epoch 360/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 1.1748\n",
      "Epoch 361/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 1.8766\n",
      "Epoch 362/500\n",
      "661/661 [==============================] - 9s 14ms/step - loss: 1.0655\n",
      "Epoch 363/500\n",
      "661/661 [==============================] - 9s 14ms/step - loss: 0.9009\n",
      "Epoch 364/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 1.2701\n",
      "Epoch 365/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 0.7670\n",
      "Epoch 366/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.5205\n",
      "Epoch 367/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.7396\n",
      "Epoch 368/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.3788\n",
      "Epoch 369/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.5608\n",
      "Epoch 370/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 1.2363\n",
      "Epoch 371/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 1.0430\n",
      "Epoch 372/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.7529\n",
      "Epoch 373/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.6809\n",
      "Epoch 374/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 1.4677\n",
      "Epoch 375/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.7647\n",
      "Epoch 376/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.5479\n",
      "Epoch 377/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.4744\n",
      "Epoch 378/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 0.6898\n",
      "Epoch 379/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.7051\n",
      "Epoch 380/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.4291\n",
      "Epoch 381/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.2929\n",
      "Epoch 382/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.1893\n",
      "Epoch 383/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.1951\n",
      "Epoch 384/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "661/661 [==============================] - 10s 15ms/step - loss: 0.2087\n",
      "Epoch 385/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.2813\n",
      "Epoch 386/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.2933\n",
      "Epoch 387/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.2652\n",
      "Epoch 388/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.2367\n",
      "Epoch 389/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.2678\n",
      "Epoch 390/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 1.8253\n",
      "Epoch 391/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 1.0596\n",
      "Epoch 392/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.4532\n",
      "Epoch 393/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.2349\n",
      "Epoch 394/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.1266\n",
      "Epoch 395/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.0959\n",
      "Epoch 396/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.0862\n",
      "Epoch 397/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.1095\n",
      "Epoch 398/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.2704\n",
      "Epoch 399/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.9264\n",
      "Epoch 400/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.5348\n",
      "Epoch 401/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.1958\n",
      "Epoch 402/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.1091\n",
      "Epoch 403/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.0837\n",
      "Epoch 404/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.0836\n",
      "Epoch 405/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.1856\n",
      "Epoch 406/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.9273\n",
      "Epoch 407/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.6001\n",
      "Epoch 408/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.5577\n",
      "Epoch 409/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 0.2867\n",
      "Epoch 410/500\n",
      "661/661 [==============================] - 11s 17ms/step - loss: 0.1351\n",
      "Epoch 411/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 0.0914\n",
      "Epoch 412/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 0.0819\n",
      "Epoch 413/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 0.1289\n",
      "Epoch 414/500\n",
      "661/661 [==============================] - 11s 17ms/step - loss: 0.6336\n",
      "Epoch 415/500\n",
      "661/661 [==============================] - 11s 17ms/step - loss: 0.6866\n",
      "Epoch 416/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 0.3528\n",
      "Epoch 417/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 0.1423\n",
      "Epoch 418/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 0.0900\n",
      "Epoch 419/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 0.0736\n",
      "Epoch 420/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.0866\n",
      "Epoch 421/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.1865\n",
      "Epoch 422/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 0.3614\n",
      "Epoch 423/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 0.3049\n",
      "Epoch 424/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.1774\n",
      "Epoch 425/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 0.1504\n",
      "Epoch 426/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 0.1550\n",
      "Epoch 427/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 0.2471\n",
      "Epoch 428/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 0.3878\n",
      "Epoch 429/500\n",
      "661/661 [==============================] - 11s 17ms/step - loss: 1.7332\n",
      "Epoch 430/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 1.6685\n",
      "Epoch 431/500\n",
      "661/661 [==============================] - 11s 17ms/step - loss: 0.5041\n",
      "Epoch 432/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 0.3783\n",
      "Epoch 433/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 0.1799\n",
      "Epoch 434/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 0.1273\n",
      "Epoch 435/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 0.0979\n",
      "Epoch 436/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.0841\n",
      "Epoch 437/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 0.0740\n",
      "Epoch 438/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 0.0751\n",
      "Epoch 439/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 0.1421\n",
      "Epoch 440/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 1.7938\n",
      "Epoch 441/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 1.1262\n",
      "Epoch 442/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.2799\n",
      "Epoch 443/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.1386\n",
      "Epoch 444/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.0968\n",
      "Epoch 445/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.0771\n",
      "Epoch 446/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.0695\n",
      "Epoch 447/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.0804\n",
      "Epoch 448/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.1770\n",
      "Epoch 449/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.3644\n",
      "Epoch 450/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.6908\n",
      "Epoch 451/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.6570\n",
      "Epoch 452/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.3204\n",
      "Epoch 453/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.1576\n",
      "Epoch 454/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.0979\n",
      "Epoch 455/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.0696\n",
      "Epoch 456/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.0672\n",
      "Epoch 457/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.1060\n",
      "Epoch 458/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.6039\n",
      "Epoch 459/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.6243\n",
      "Epoch 460/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 0.4961\n",
      "Epoch 461/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.4964\n",
      "Epoch 462/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 0.5408\n",
      "Epoch 463/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 0.1684\n",
      "Epoch 464/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 0.1039\n",
      "Epoch 465/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 0.0997\n",
      "Epoch 466/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 0.0927\n",
      "Epoch 467/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 0.1176\n",
      "Epoch 468/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 0.3238\n",
      "Epoch 469/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 0.3528\n",
      "Epoch 470/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 0.2316\n",
      "Epoch 471/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.1088\n",
      "Epoch 472/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 0.0857\n",
      "Epoch 473/500\n",
      "661/661 [==============================] - 11s 17ms/step - loss: 0.1198\n",
      "Epoch 474/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 0.1927\n",
      "Epoch 475/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 0.2289\n",
      "Epoch 476/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 0.1710\n",
      "Epoch 477/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 0.1240\n",
      "Epoch 478/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 0.1589\n",
      "Epoch 479/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "661/661 [==============================] - 10s 16ms/step - loss: 0.2662\n",
      "Epoch 480/500\n",
      "661/661 [==============================] - 10s 16ms/step - loss: 1.6743\n",
      "Epoch 481/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 1.3653\n",
      "Epoch 482/500\n",
      "661/661 [==============================] - 11s 16ms/step - loss: 0.5010\n",
      "Epoch 483/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.2269\n",
      "Epoch 484/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.1584\n",
      "Epoch 485/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.1182\n",
      "Epoch 486/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.0902\n",
      "Epoch 487/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.0793\n",
      "Epoch 488/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.0832\n",
      "Epoch 489/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.0960\n",
      "Epoch 490/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.1215\n",
      "Epoch 491/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.1814\n",
      "Epoch 492/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 1.0951\n",
      "Epoch 493/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.5768\n",
      "Epoch 494/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.2452\n",
      "Epoch 495/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.1082\n",
      "Epoch 496/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.0709\n",
      "Epoch 497/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.0577\n",
      "Epoch 498/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.0545\n",
      "Epoch 499/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.0768\n",
      "Epoch 500/500\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.2411\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' result\\n    epoch 500, 0.0624\\n    epoch 1000, 0.0448\\n    (256, 128, 64, 64, 128, 256) epoch 100 5.2650 \\n    (128, 64, 32, 32, 64, 128) epoch 100 2.6025 / 500 0.1083 # activation = tanh\\n    (128, 64, 32, 32, 64, 128) epoch 100 ? / 500 ? # activation = relu\\n    \\n    new data set\\n    (128, 64, 32, 32, 64, 128) epoch 100 1.9166 / 500 \\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, LSTM, RepeatVector, Lambda\n",
    "from keras.models import Model\n",
    "\n",
    "def repeat_vector(args):\n",
    "    layer_to_repeat = args[0]\n",
    "    sequence_layer = args[1]\n",
    "    return RepeatVector(K.shape(sequence_layer)[1])(layer_to_repeat)\n",
    "    \n",
    "\n",
    "inputs = Input(shape=(None, n_features))\n",
    "encoded = LSTM(128, return_sequences=True)(inputs)  #activation 안적으면 tanh\n",
    "encoded = LSTM(64, return_sequences=True)(encoded)\n",
    "encoded = LSTM(32, activation='relu')(encoded)\n",
    "\n",
    "decoded = Lambda(repeat_vector, output_shape=(None, 32)) ([encoded, inputs]) # inputs의 shape[1] 만큼 encoded 를 반복 생성\n",
    "\n",
    "decoded = LSTM(32, return_sequences=True)(decoded)\n",
    "decoded = LSTM(64, return_sequences=True)(decoded)\n",
    "decoded = LSTM(128, return_sequences=True)(decoded)\n",
    "decoded = TimeDistributed(Dense(n_features))(decoded)\n",
    "encoder = Model(inputs, encoded)\n",
    "\n",
    "lstm_autoencoder = Model(inputs, decoded)\n",
    "lstm_autoencoder.compile(loss='mean_squared_error', optimizer=Adam(lr=0.001))\n",
    "\n",
    "def train_generator(x_train):\n",
    "    idx = 0\n",
    "    while True:\n",
    "        yield np.array([x_train[idx]]), np.array([x_train[idx]])\n",
    "        idx +=1\n",
    "        if idx >= len(x_train):\n",
    "            idx = 0\n",
    "lstm_autoencoder.fit_generator(train_generator(x_train), epochs=epochs, steps_per_epoch=steps_per_epoch, verbose=1)#, validation_data=val_generator(x_val))\n",
    "\n",
    "''' result\n",
    "    epoch 500, 0.0624\n",
    "    epoch 1000, 0.0448\n",
    "    (256, 128, 64, 64, 128, 256) epoch 100 5.2650 \n",
    "    (128, 64, 32, 32, 64, 128) epoch 100 2.6025 / 500 0.1083 # activation = tanh\n",
    "    (128, 64, 32, 32, 64, 128) epoch 100 4.7124 / 500 ? # activation = relu\n",
    "    \n",
    "    new data set\n",
    "    (128, 64, 32, 32, 64, 128) epoch 100 1.9166 / 500  0.0135\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "# model save\n",
    "lstm_autoencoder.save('lstm_autoencoder_prev_dfs.h5')\n",
    "loaded_model = load_model('lstm_autoencoder_prev_dfs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(inputs, encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_16\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, None, 3)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_35 (LSTM)                  (None, None, 128)    67584       input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_36 (LSTM)                  (None, None, 64)     49408       lstm_35[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_37 (LSTM)                  (None, 32)           12416       lstm_36[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, None, 32)     0           lstm_37[0][0]                    \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_38 (LSTM)                  (None, None, 32)     8320        lambda_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_39 (LSTM)                  (None, None, 64)     24832       lstm_38[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_40 (LSTM)                  (None, None, 128)    98816       lstm_39[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_7 (TimeDistrib (None, None, 3)      387         lstm_40[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 261,763\n",
      "Trainable params: 261,763\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_14\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, None, 3)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_29 (LSTM)                  (None, None, 128)    67584       input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_30 (LSTM)                  (None, None, 64)     49408       lstm_29[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_31 (LSTM)                  (None, 32)           12416       lstm_30[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, None, 32)     0           lstm_31[0][0]                    \n",
      "                                                                 input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_32 (LSTM)                  (None, None, 32)     8320        lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_33 (LSTM)                  (None, None, 64)     24832       lstm_32[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_34 (LSTM)                  (None, None, 128)    98816       lstm_33[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_6 (TimeDistrib (None, None, 3)      387         lstm_34[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 261,763\n",
      "Trainable params: 261,763\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function keras.activations.relu(x, alpha=0.0, max_value=None, threshold=0.0)>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_autoencoder.summary()\n",
    "#lstm_autoencoder.layers[3].activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, None, 3)           0         \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, None, 128)         67584     \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 64)                49408     \n",
      "=================================================================\n",
      "Total params: 116,992\n",
      "Trainable params: 116,992\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.  ,  1.  ,  7.84],\n",
       "       [ 1.  ,  3.  ,  6.79],\n",
       "       [ 3.  ,  2.  , 22.  ],\n",
       "       [ 1.  ,  4.  , 11.79],\n",
       "       [ 2.  ,  5.  , 11.09]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 8.931337 ,  7.4579573, 31.943764 ],\n",
       "        [ 8.431695 ,  7.737891 ,  8.375352 ],\n",
       "        [ 7.3948007,  5.0614414, 13.785835 ],\n",
       "        [ 7.925072 ,  5.933628 , 14.485416 ],\n",
       "        [ 2.0842404,  1.931783 ,  8.705457 ],\n",
       "        [ 2.2869372,  3.9760273,  7.5900183],\n",
       "        [ 7.5529623,  5.3252344, 26.848215 ],\n",
       "        [ 2.7739074,  3.8996158,  8.020057 ],\n",
       "        [ 1.5346651,  4.882212 , 11.469937 ]]], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_autoencoder.predict(x_test[0].reshape(1, x_test[0].shape[0], x_test[0].shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = x_test[0].reshape(1, 9, 3)\n",
    "latent_vector = []\n",
    "for x in x_test:\n",
    "    x = x.reshape(1, x.shape[0], x.shape[1])\n",
    "    latent_vector.append(encoder.predict(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printMatrix(matrix):\n",
    "    length = int(max(max(matrix[:, 0]), max(matrix[:, 1])))\n",
    "    adj = [[0 for i in range(length)] for i in range(length)]\n",
    "    \n",
    "    for m in matrix:\n",
    "        (i, j, w) = m\n",
    "        i = int(i) -1\n",
    "        j = int(j) -1\n",
    "        adj[i][j] = adj[j][i] = w\n",
    "    for i in range(length):\n",
    "        for j in range(length):\n",
    "            print(str(adj[i][j]), end=',')\n",
    "        print()\n",
    "    print(\"===============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 0\n",
      "0,10.16,7.92,9.56,0,0,0,0,0,\n",
      "10.16,0,0,0,0,0,0,0,0,\n",
      "7.92,0,0,0,8.17,18.05,0,0,0,\n",
      "9.56,0,0,0,0,0,20.05,0,0,\n",
      "0,0,8.17,0,0,0,0,0,0,\n",
      "0,0,18.05,0,0,0,0,32.09,0,\n",
      "0,0,0,20.05,0,0,0,0,20.27,\n",
      "0,0,0,0,0,32.09,0,0,9.01,\n",
      "0,0,0,0,0,0,20.27,9.01,0,\n",
      "===============\n",
      "# 1\n",
      "0,8.79,11.79,0,0,0,\n",
      "8.79,0,0,10.06,6.86,0,\n",
      "11.79,0,0,0,0,7.86,\n",
      "0,10.06,0,0,0,0,\n",
      "0,6.86,0,0,0,0,\n",
      "0,0,7.86,0,0,0,\n",
      "===============\n",
      "# 2\n",
      "0,3.3,6.3,0,0,\n",
      "3.3,0,0,12.72,0,\n",
      "6.3,0,0,0,23.87,\n",
      "0,12.72,0,0,41.95,\n",
      "0,0,23.87,41.95,0,\n",
      "===============\n",
      "# 3\n",
      "0,5.7,7.22,0,0,\n",
      "5.7,0,0,12.22,0,\n",
      "7.22,0,0,0,23.54,\n",
      "0,12.22,0,0,42.39,\n",
      "0,0,23.54,42.39,0,\n",
      "===============\n",
      "# 4\n",
      "0,12.52,17.52,28.77,0,0,0,0,0,\n",
      "12.52,0,0,0,0,0,0,0,0,\n",
      "17.52,0,0,0,2.53,4.53,0,0,0,\n",
      "28.77,0,0,0,0,0,10.86,0,0,\n",
      "0,0,2.53,0,0,0,0,0,0,\n",
      "0,0,4.53,0,0,0,0,9.01,0,\n",
      "0,0,0,10.86,0,0,0,0,10.01,\n",
      "0,0,0,0,0,9.01,0,0,20.01,\n",
      "0,0,0,0,0,0,10.01,20.01,0,\n",
      "===============\n",
      "# 5\n",
      "0,4.27,9.29,0,0,\n",
      "4.27,0,0,12.29,0,\n",
      "9.29,0,0,0,17.29,\n",
      "0,12.29,0,0,3.95,\n",
      "0,0,17.29,3.95,0,\n",
      "===============\n",
      "# 6\n",
      "0,10.16,7.92,9.56,0,0,0,0,0,\n",
      "10.16,0,0,0,0,0,0,0,0,\n",
      "7.92,0,0,0,8.17,18.05,0,0,0,\n",
      "9.56,0,0,0,0,0,20.05,0,0,\n",
      "0,0,8.17,0,0,0,0,0,0,\n",
      "0,0,18.05,0,0,0,0,32.09,0,\n",
      "0,0,0,20.05,0,0,0,0,20.27,\n",
      "0,0,0,0,0,32.09,0,0,9.01,\n",
      "0,0,0,0,0,0,20.27,9.01,0,\n",
      "===============\n",
      "# 7\n",
      "0,9.1,12.1,23.12,0,0,\n",
      "9.1,0,11.14,0,21.31,0,\n",
      "12.1,11.14,0,0,0,38.69,\n",
      "23.12,0,0,0,0,0,\n",
      "0,21.31,0,0,0,76.3,\n",
      "0,0,38.69,0,76.3,0,\n",
      "===============\n",
      "# 8\n",
      "0,5.68,4.49,0,0,\n",
      "5.68,0,0,10.64,0,\n",
      "4.49,0,0,0,20.73,\n",
      "0,10.64,0,0,23.73,\n",
      "0,0,20.73,23.73,0,\n",
      "===============\n",
      "# 9\n",
      "0,2.81,9.59,11.34,0,\n",
      "2.81,0,21.48,0,8.05,\n",
      "9.59,21.48,0,0,0,\n",
      "11.34,0,0,0,0,\n",
      "0,8.05,0,0,0,\n",
      "===============\n",
      "# 10\n",
      "0,5.11,6.66,8.4,\n",
      "5.11,0,18.0,0,\n",
      "6.66,18.0,0,0,\n",
      "8.4,0,0,0,\n",
      "===============\n",
      "# 11\n",
      "0,5.95,15.4,29.79,0,0,\n",
      "5.95,0,14.98,0,4.4,0,\n",
      "15.4,14.98,0,0,0,13.89,\n",
      "29.79,0,0,0,0,0,\n",
      "0,4.4,0,0,0,24.58,\n",
      "0,0,13.89,0,24.58,0,\n",
      "===============\n",
      "# 12\n",
      "0,6.41,4.09,0,0,0,\n",
      "6.41,0,0,8.09,16.24,0,\n",
      "4.09,0,0,0,0,4.33,\n",
      "0,8.09,0,0,0,0,\n",
      "0,16.24,0,0,0,0,\n",
      "0,0,4.33,0,0,0,\n",
      "===============\n",
      "# 13\n",
      "0,5.46,10.39,0,0,0,0,0,0,0,\n",
      "5.46,0,0,4.69,6.99,0,0,0,0,0,\n",
      "10.39,0,0,0,0,6.57,0,0,0,0,\n",
      "0,4.69,0,0,0,0,5.81,8.41,0,0,\n",
      "0,6.99,0,0,0,0,0,0,0,0,\n",
      "0,0,6.57,0,0,0,0,0,11.41,0,\n",
      "0,0,0,5.81,0,0,0,0,0,0,\n",
      "0,0,0,8.41,0,0,0,0,0,0,\n",
      "0,0,0,0,0,11.41,0,0,0,22.01,\n",
      "0,0,0,0,0,0,0,0,22.01,0,\n",
      "===============\n",
      "# 14\n",
      "0,7.91,9.61,0,0,0,0,\n",
      "7.91,0,0,19.41,7.36,0,0,\n",
      "9.61,0,0,0,0,15.61,11.39,\n",
      "0,19.41,0,0,0,0,0,\n",
      "0,7.36,0,0,0,0,0,\n",
      "0,0,15.61,0,0,0,0,\n",
      "0,0,11.39,0,0,0,0,\n",
      "===============\n",
      "# 15\n",
      "0,2.49,10.47,0,0,0,0,\n",
      "2.49,0,0,20.49,24.49,0,0,\n",
      "10.47,0,0,0,0,0,0,\n",
      "0,20.49,0,0,0,44.3,60.58,\n",
      "0,24.49,0,0,0,0,0,\n",
      "0,0,0,44.3,0,0,0,\n",
      "0,0,0,60.58,0,0,0,\n",
      "===============\n",
      "# 16\n",
      "0,9.84,14.84,0,0,0,0,0,0,0,\n",
      "9.84,0,0,28.34,52.46,0,0,0,0,0,\n",
      "14.84,0,0,0,0,20.74,0,0,0,0,\n",
      "0,28.34,0,0,0,0,1.8,8.07,0,0,\n",
      "0,52.46,0,0,0,0,0,0,0,0,\n",
      "0,0,20.74,0,0,0,0,0,10.07,0,\n",
      "0,0,0,1.8,0,0,0,0,0,0,\n",
      "0,0,0,8.07,0,0,0,0,0,0,\n",
      "0,0,0,0,0,10.07,0,0,0,20.12,\n",
      "0,0,0,0,0,0,0,0,20.12,0,\n",
      "===============\n",
      "# 17\n",
      "0,5.68,4.49,0,0,\n",
      "5.68,0,0,10.64,0,\n",
      "4.49,0,0,0,20.73,\n",
      "0,10.64,0,0,23.73,\n",
      "0,0,20.73,23.73,0,\n",
      "===============\n",
      "# 18\n",
      "0,7.44,9.44,7.01,0,0,\n",
      "7.44,0,9.01,0,9.95,0,\n",
      "9.44,9.01,0,0,0,6.37,\n",
      "7.01,0,0,0,0,0,\n",
      "0,9.95,0,0,0,5.54,\n",
      "0,0,6.37,0,5.54,0,\n",
      "===============\n",
      "# 19\n",
      "0,11.21,1.03,3.72,0,\n",
      "11.21,0,7.76,0,6.56,\n",
      "1.03,7.76,0,0,0,\n",
      "3.72,0,0,0,0,\n",
      "0,6.56,0,0,0,\n",
      "===============\n",
      "# 20\n",
      "0,4.71,7.93,0,0,0,0,0,0,0,\n",
      "4.71,0,0,16.93,3.43,0,0,0,0,0,\n",
      "7.93,0,0,0,0,9.66,0,0,0,0,\n",
      "0,16.93,0,0,0,0,8.09,18.05,0,0,\n",
      "0,3.43,0,0,0,0,0,0,0,0,\n",
      "0,0,9.66,0,0,0,0,0,30.86,0,\n",
      "0,0,0,8.09,0,0,0,0,0,0,\n",
      "0,0,0,18.05,0,0,0,0,0,0,\n",
      "0,0,0,0,0,30.86,0,0,0,31.86,\n",
      "0,0,0,0,0,0,0,0,31.86,0,\n",
      "===============\n",
      "# 21\n",
      "0,4.54,12.82,23.2,0,0,\n",
      "4.54,0,12.56,0,12.48,0,\n",
      "12.82,12.56,0,0,0,2.44,\n",
      "23.2,0,0,0,0,0,\n",
      "0,12.48,0,0,0,6.95,\n",
      "0,0,2.44,0,6.95,0,\n",
      "===============\n",
      "# 22\n",
      "0,9.7,5.1,5.12,0,0,0,0,0,\n",
      "9.7,0,0,0,0,0,0,0,0,\n",
      "5.1,0,0,0,13.29,10.61,0,0,0,\n",
      "5.12,0,0,0,0,0,20.94,0,0,\n",
      "0,0,13.29,0,0,0,0,0,0,\n",
      "0,0,10.61,0,0,0,0,8.35,0,\n",
      "0,0,0,20.94,0,0,0,0,6.14,\n",
      "0,0,0,0,0,8.35,0,0,14.26,\n",
      "0,0,0,0,0,0,6.14,14.26,0,\n",
      "===============\n",
      "# 23\n",
      "0,6.81,0,0,\n",
      "6.81,0,9.81,11.46,\n",
      "0,9.81,0,21.99,\n",
      "0,11.46,21.99,0,\n",
      "===============\n",
      "# 24\n",
      "0,8.12,16.82,0,0,\n",
      "8.12,0,0,3.04,0,\n",
      "16.82,0,0,0,9.05,\n",
      "0,3.04,0,0,6.2,\n",
      "0,0,9.05,6.2,0,\n",
      "===============\n",
      "# 25\n",
      "0,5.09,2.06,4.17,0,0,\n",
      "5.09,0,11.95,0,13.95,0,\n",
      "2.06,11.95,0,0,0,2.21,\n",
      "4.17,0,0,0,0,0,\n",
      "0,13.95,0,0,0,11.03,\n",
      "0,0,2.21,0,11.03,0,\n",
      "===============\n",
      "# 26\n",
      "0,5.46,10.39,0,0,0,0,0,0,0,\n",
      "5.46,0,0,4.69,6.99,0,0,0,0,0,\n",
      "10.39,0,0,0,0,6.57,0,0,0,0,\n",
      "0,4.69,0,0,0,0,5.81,8.41,0,0,\n",
      "0,6.99,0,0,0,0,0,0,0,0,\n",
      "0,0,6.57,0,0,0,0,0,11.41,0,\n",
      "0,0,0,5.81,0,0,0,0,0,0,\n",
      "0,0,0,8.41,0,0,0,0,0,0,\n",
      "0,0,0,0,0,11.41,0,0,0,22.01,\n",
      "0,0,0,0,0,0,0,0,22.01,0,\n",
      "===============\n",
      "# 27\n",
      "0,5.09,2.06,4.17,0,0,\n",
      "5.09,0,11.95,0,13.95,0,\n",
      "2.06,11.95,0,0,0,2.21,\n",
      "4.17,0,0,0,0,0,\n",
      "0,13.95,0,0,0,11.03,\n",
      "0,0,2.21,0,11.03,0,\n",
      "===============\n",
      "# 28\n",
      "0,4.16,7.16,0,0,0,0,\n",
      "4.16,0,0,16.83,28.23,0,0,\n",
      "7.16,0,0,0,0,33.23,62.71,\n",
      "0,16.83,0,0,0,0,0,\n",
      "0,28.23,0,0,0,0,0,\n",
      "0,0,33.23,0,0,0,0,\n",
      "0,0,62.71,0,0,0,0,\n",
      "===============\n",
      "# 29\n",
      "0,6.13,5.95,0,0,0,0,0,0,0,\n",
      "6.13,0,0,10.95,21.64,0,0,0,0,0,\n",
      "5.95,0,0,0,0,23.64,0,0,0,0,\n",
      "0,10.95,0,0,0,0,6.23,8.77,0,0,\n",
      "0,21.64,0,0,0,0,0,0,0,0,\n",
      "0,0,23.64,0,0,0,0,0,10.77,0,\n",
      "0,0,0,6.23,0,0,0,0,0,0,\n",
      "0,0,0,8.77,0,0,0,0,0,0,\n",
      "0,0,0,0,0,10.77,0,0,0,1.34,\n",
      "0,0,0,0,0,0,0,0,1.34,0,\n",
      "===============\n",
      "# 30\n",
      "0,10.37,8.48,6.98,0,0,0,0,0,\n",
      "10.37,0,0,0,0,0,0,0,0,\n",
      "8.48,0,0,0,12.67,22.75,0,0,0,\n",
      "6.98,0,0,0,0,0,27.75,0,0,\n",
      "0,0,12.67,0,0,0,0,0,0,\n",
      "0,0,22.75,0,0,0,0,2.0,0,\n",
      "0,0,0,27.75,0,0,0,0,7.78,\n",
      "0,0,0,0,0,2.0,0,0,15.79,\n",
      "0,0,0,0,0,0,7.78,15.79,0,\n",
      "===============\n",
      "# 31\n",
      "0,3.09,4.09,0,0,0,0,0,0,0,\n",
      "3.09,0,0,7.31,6.24,0,0,0,0,0,\n",
      "4.09,0,0,0,0,10.78,0,0,0,0,\n",
      "0,7.31,0,0,0,0,5.75,8.53,0,0,\n",
      "0,6.24,0,0,0,0,0,0,0,0,\n",
      "0,0,10.78,0,0,0,0,0,6.23,0,\n",
      "0,0,0,5.75,0,0,0,0,0,0,\n",
      "0,0,0,8.53,0,0,0,0,0,0,\n",
      "0,0,0,0,0,6.23,0,0,0,9.5,\n",
      "0,0,0,0,0,0,0,0,9.5,0,\n",
      "===============\n",
      "# 32\n",
      "0,7.38,6.51,0,0,0,0,0,0,0,\n",
      "7.38,0,0,9.96,10.96,0,0,0,0,0,\n",
      "6.51,0,0,0,0,21.85,0,0,0,0,\n",
      "0,9.96,0,0,0,0,38.35,27.88,0,0,\n",
      "0,10.96,0,0,0,0,0,0,0,0,\n",
      "0,0,21.85,0,0,0,0,0,37.89,0,\n",
      "0,0,0,38.35,0,0,0,0,0,0,\n",
      "0,0,0,27.88,0,0,0,0,0,0,\n",
      "0,0,0,0,0,37.89,0,0,0,39.89,\n",
      "0,0,0,0,0,0,0,0,39.89,0,\n",
      "===============\n",
      "# 33\n",
      "0,8.46,9.46,0,0,0,0,\n",
      "8.46,0,0,8.28,11.91,0,0,\n",
      "9.46,0,0,0,0,0,0,\n",
      "0,8.28,0,0,0,1.22,9.42,\n",
      "0,11.91,0,0,0,0,0,\n",
      "0,0,0,1.22,0,0,0,\n",
      "0,0,0,9.42,0,0,0,\n",
      "===============\n",
      "# 34\n",
      "0,4.35,3.05,12.89,0,\n",
      "4.35,0,17.89,0,28.52,\n",
      "3.05,17.89,0,0,0,\n",
      "12.89,0,0,0,0,\n",
      "0,28.52,0,0,0,\n",
      "===============\n",
      "# 35\n",
      "0,7.21,4.47,0,0,0,0,\n",
      "7.21,0,0,13.78,2.09,0,0,\n",
      "4.47,0,0,0,0,4.09,5.11,\n",
      "0,13.78,0,0,0,0,0,\n",
      "0,2.09,0,0,0,0,0,\n",
      "0,0,4.09,0,0,0,0,\n",
      "0,0,5.11,0,0,0,0,\n",
      "===============\n",
      "# 36\n",
      "0,8.54,12.54,0,0,0,0,\n",
      "8.54,0,0,1.84,9.02,0,0,\n",
      "12.54,0,0,0,0,7.16,12.16,\n",
      "0,1.84,0,0,0,0,0,\n",
      "0,9.02,0,0,0,0,0,\n",
      "0,0,7.16,0,0,0,0,\n",
      "0,0,12.16,0,0,0,0,\n",
      "===============\n",
      "# 37\n",
      "0,4.65,9.65,0,0,0,\n",
      "4.65,0,0,6.96,4.92,0,\n",
      "9.65,0,0,0,0,5.92,\n",
      "0,6.96,0,0,0,0,\n",
      "0,4.92,0,0,0,0,\n",
      "0,0,5.92,0,0,0,\n",
      "===============\n",
      "# 38\n",
      "0,8.6,8.71,18.02,0,0,0,0,0,\n",
      "8.6,0,0,0,0,0,0,0,0,\n",
      "8.71,0,0,0,1.3,9.61,0,0,0,\n",
      "18.02,0,0,0,0,0,19.5,0,0,\n",
      "0,0,1.3,0,0,0,0,0,0,\n",
      "0,0,9.61,0,0,0,0,6.15,0,\n",
      "0,0,0,19.5,0,0,0,0,4.59,\n",
      "0,0,0,0,0,6.15,0,0,5.8,\n",
      "0,0,0,0,0,0,4.59,5.8,0,\n",
      "===============\n",
      "# 39\n",
      "0,11.2,9.42,0,0,0,0,\n",
      "11.2,0,0,8.63,11.63,0,0,\n",
      "9.42,0,0,0,0,0,0,\n",
      "0,8.63,0,0,0,6.9,10.69,\n",
      "0,11.63,0,0,0,0,0,\n",
      "0,0,0,6.9,0,0,0,\n",
      "0,0,0,10.69,0,0,0,\n",
      "===============\n",
      "# 40\n",
      "0,11.32,22.47,7.57,0,0,\n",
      "11.32,0,15.33,0,5.29,0,\n",
      "22.47,15.33,0,0,0,5.21,\n",
      "7.57,0,0,0,0,0,\n",
      "0,5.29,0,0,0,13.0,\n",
      "0,0,5.21,0,13.0,0,\n",
      "===============\n",
      "# 41\n",
      "0,7.64,5.28,10.28,0,0,\n",
      "7.64,0,9.69,0,10.17,0,\n",
      "5.28,9.69,0,0,0,20.32,\n",
      "10.28,0,0,0,0,0,\n",
      "0,10.17,0,0,0,21.32,\n",
      "0,0,20.32,0,21.32,0,\n",
      "===============\n",
      "# 42\n",
      "0,2.73,7.31,10.0,0,0,0,0,0,\n",
      "2.73,0,0,0,0,0,0,0,0,\n",
      "7.31,0,0,0,6.1,11.5,0,0,0,\n",
      "10.0,0,0,0,0,0,1.37,0,0,\n",
      "0,0,6.1,0,0,0,0,0,0,\n",
      "0,0,11.5,0,0,0,0,10.51,0,\n",
      "0,0,0,1.37,0,0,0,0,9.33,\n",
      "0,0,0,0,0,10.51,0,0,11.07,\n",
      "0,0,0,0,0,0,9.33,11.07,0,\n",
      "===============\n",
      "# 43\n",
      "0,10.47,15.47,26.68,0,\n",
      "10.47,0,5.92,0,9.13,\n",
      "15.47,5.92,0,0,0,\n",
      "26.68,0,0,0,0,\n",
      "0,9.13,0,0,0,\n",
      "===============\n",
      "# 44\n",
      "0,5.13,3.73,0,0,0,0,\n",
      "5.13,0,0,12.26,14.26,0,0,\n",
      "3.73,0,0,0,0,6.24,14.5,\n",
      "0,12.26,0,0,0,0,0,\n",
      "0,14.26,0,0,0,0,0,\n",
      "0,0,6.24,0,0,0,0,\n",
      "0,0,14.5,0,0,0,0,\n",
      "===============\n",
      "# 45\n",
      "0,7.44,9.44,7.01,0,0,\n",
      "7.44,0,9.01,0,9.95,0,\n",
      "9.44,9.01,0,0,0,6.37,\n",
      "7.01,0,0,0,0,0,\n",
      "0,9.95,0,0,0,5.54,\n",
      "0,0,6.37,0,5.54,0,\n",
      "===============\n",
      "# 46\n",
      "0,3.57,9.11,0,0,0,0,\n",
      "3.57,0,0,8.53,12.29,0,0,\n",
      "9.11,0,0,0,0,0,0,\n",
      "0,8.53,0,0,0,2.17,9.34,\n",
      "0,12.29,0,0,0,0,0,\n",
      "0,0,0,2.17,0,0,0,\n",
      "0,0,0,9.34,0,0,0,\n",
      "===============\n",
      "# 47\n",
      "0,6.13,5.95,0,0,0,0,0,0,0,\n",
      "6.13,0,0,10.95,21.64,0,0,0,0,0,\n",
      "5.95,0,0,0,0,23.64,0,0,0,0,\n",
      "0,10.95,0,0,0,0,6.23,8.77,0,0,\n",
      "0,21.64,0,0,0,0,0,0,0,0,\n",
      "0,0,23.64,0,0,0,0,0,10.77,0,\n",
      "0,0,0,6.23,0,0,0,0,0,0,\n",
      "0,0,0,8.77,0,0,0,0,0,0,\n",
      "0,0,0,0,0,10.77,0,0,0,1.34,\n",
      "0,0,0,0,0,0,0,0,1.34,0,\n",
      "===============\n",
      "# 48\n",
      "0,6.67,11.67,0,0,0,0,\n",
      "6.67,0,0,22.32,39.93,0,0,\n",
      "11.67,0,0,0,0,0,0,\n",
      "0,22.32,0,0,0,41.93,3.97,\n",
      "0,39.93,0,0,0,0,0,\n",
      "0,0,0,41.93,0,0,0,\n",
      "0,0,0,3.97,0,0,0,\n",
      "===============\n",
      "# 49\n",
      "0,5.97,10.97,21.23,0,0,\n",
      "5.97,0,23.23,0,10.81,0,\n",
      "10.97,23.23,0,0,0,6.14,\n",
      "21.23,0,0,0,0,0,\n",
      "0,10.81,0,0,0,6.19,\n",
      "0,0,6.14,0,6.19,0,\n",
      "===============\n",
      "# 50\n",
      "0,5.87,4.42,0,0,\n",
      "5.87,0,0,11.89,0,\n",
      "4.42,0,0,0,1.67,\n",
      "0,11.89,0,0,6.67,\n",
      "0,0,1.67,6.67,0,\n",
      "===============\n",
      "# 51\n",
      "0,6.45,15.17,2.95,\n",
      "6.45,0,7.95,0,\n",
      "15.17,7.95,0,0,\n",
      "2.95,0,0,0,\n",
      "===============\n",
      "# 52\n",
      "0,3.71,10.29,0,0,0,\n",
      "3.71,0,0,10.61,15.61,0,\n",
      "10.29,0,0,0,0,27.3,\n",
      "0,10.61,0,0,0,0,\n",
      "0,15.61,0,0,0,0,\n",
      "0,0,27.3,0,0,0,\n",
      "===============\n",
      "# 53\n",
      "0,11.2,9.42,0,0,0,0,\n",
      "11.2,0,0,8.63,11.63,0,0,\n",
      "9.42,0,0,0,0,0,0,\n",
      "0,8.63,0,0,0,6.9,10.69,\n",
      "0,11.63,0,0,0,0,0,\n",
      "0,0,0,6.9,0,0,0,\n",
      "0,0,0,10.69,0,0,0,\n",
      "===============\n",
      "# 54\n",
      "0,10.37,8.48,6.98,0,0,0,0,0,\n",
      "10.37,0,0,0,0,0,0,0,0,\n",
      "8.48,0,0,0,12.67,22.75,0,0,0,\n",
      "6.98,0,0,0,0,0,27.75,0,0,\n",
      "0,0,12.67,0,0,0,0,0,0,\n",
      "0,0,22.75,0,0,0,0,2.0,0,\n",
      "0,0,0,27.75,0,0,0,0,7.78,\n",
      "0,0,0,0,0,2.0,0,0,15.79,\n",
      "0,0,0,0,0,0,7.78,15.79,0,\n",
      "===============\n",
      "# 55\n",
      "0,11.34,22.67,0,0,0,\n",
      "11.34,0,0,23.67,42.83,0,\n",
      "22.67,0,0,0,0,45.83,\n",
      "0,23.67,0,0,0,0,\n",
      "0,42.83,0,0,0,0,\n",
      "0,0,45.83,0,0,0,\n",
      "===============\n",
      "# 56\n",
      "0,6.82,0,0,\n",
      "6.82,0,16.66,18.66,\n",
      "0,16.66,0,5.64,\n",
      "0,18.66,5.64,0,\n",
      "===============\n",
      "# 57\n",
      "0,6.0,8.84,10.6,0,0,\n",
      "6.0,0,10.99,0,12.99,0,\n",
      "8.84,10.99,0,0,0,1.37,\n",
      "10.6,0,0,0,0,0,\n",
      "0,12.99,0,0,0,3.77,\n",
      "0,0,1.37,0,3.77,0,\n",
      "===============\n",
      "# 58\n",
      "0,8.17,17.68,0,0,0,0,0,0,0,\n",
      "8.17,0,0,28.82,14.87,0,0,0,0,0,\n",
      "17.68,0,0,0,0,17.87,0,0,0,0,\n",
      "0,28.82,0,0,0,0,34.08,64.24,0,0,\n",
      "0,14.87,0,0,0,0,0,0,0,0,\n",
      "0,0,17.87,0,0,0,0,0,52.76,0,\n",
      "0,0,0,34.08,0,0,0,0,0,0,\n",
      "0,0,0,64.24,0,0,0,0,0,0,\n",
      "0,0,0,0,0,52.76,0,0,0,74.48,\n",
      "0,0,0,0,0,0,0,0,74.48,0,\n",
      "===============\n",
      "# 59\n",
      "0,12.31,13.31,0,0,0,\n",
      "12.31,0,0,1.24,9.71,0,\n",
      "13.31,0,0,0,0,19.48,\n",
      "0,1.24,0,0,0,0,\n",
      "0,9.71,0,0,0,0,\n",
      "0,0,19.48,0,0,0,\n",
      "===============\n",
      "# 60\n",
      "0,6.92,7.92,4.57,\n",
      "6.92,0,8.77,0,\n",
      "7.92,8.77,0,0,\n",
      "4.57,0,0,0,\n",
      "===============\n",
      "# 61\n",
      "0,9.7,5.1,5.12,0,0,0,0,0,\n",
      "9.7,0,0,0,0,0,0,0,0,\n",
      "5.1,0,0,0,13.29,10.61,0,0,0,\n",
      "5.12,0,0,0,0,0,20.94,0,0,\n",
      "0,0,13.29,0,0,0,0,0,0,\n",
      "0,0,10.61,0,0,0,0,8.35,0,\n",
      "0,0,0,20.94,0,0,0,0,6.14,\n",
      "0,0,0,0,0,8.35,0,0,14.26,\n",
      "0,0,0,0,0,0,6.14,14.26,0,\n",
      "===============\n",
      "# 62\n",
      "0,8.12,16.82,0,0,\n",
      "8.12,0,0,3.04,0,\n",
      "16.82,0,0,0,9.05,\n",
      "0,3.04,0,0,6.2,\n",
      "0,0,9.05,6.2,0,\n",
      "===============\n",
      "# 63\n",
      "0,8.17,17.68,0,0,0,0,0,0,0,\n",
      "8.17,0,0,28.82,14.87,0,0,0,0,0,\n",
      "17.68,0,0,0,0,17.87,0,0,0,0,\n",
      "0,28.82,0,0,0,0,34.08,64.24,0,0,\n",
      "0,14.87,0,0,0,0,0,0,0,0,\n",
      "0,0,17.87,0,0,0,0,0,52.76,0,\n",
      "0,0,0,34.08,0,0,0,0,0,0,\n",
      "0,0,0,64.24,0,0,0,0,0,0,\n",
      "0,0,0,0,0,52.76,0,0,0,74.48,\n",
      "0,0,0,0,0,0,0,0,74.48,0,\n",
      "===============\n"
     ]
    }
   ],
   "source": [
    "# sequence to matrix\n",
    "for index, x in enumerate(x_test):\n",
    "    print(\"#\", str(index))\n",
    "    printMatrix(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard = latent_vector[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)  (2, 38)  (3, 53)  (4, 5)  (5, 15)  (6, 44)  (7, 48)  (8, 21)  (9, 12)  (10, 43)  (11, 9)  (12, 18)  (13, 31)  (14, 59)  (15, 58)  (16, 4)  (17, 41)  (18, 27)  (19, 29)  (20, 33)  (21, 6)  (22, 3)  (23, 51)  (24, 35)  (25, 54)  (26, 55)  (27, 26)  (28, 62)  (29, 14)  (30, 46)  (31, 11)  (32, 45)  (33, 7)  (34, 50)  (35, 57)  (36, 8)  (37, 63)  (38, 60)  (39, 24)  (40, 52)  (41, 20)  (42, 13)  (43, 37)  (44, 23)  (45, 10)  (46, 1)  (47, 61)  (48, 42)  (49, 56)  (50, 34)  (51, 16)  (52, 32)  (53, 40)  (54, 39)  (55, 19)  (56, 17)  (57, 22)  (58, 49)  (59, 30)  (60, 36)  (61, 47)  (62, 28)  (63, 25)  "
     ]
    }
   ],
   "source": [
    "import copy\n",
    "dist = []\n",
    "left = 100000\n",
    "right = -10000\n",
    "for index, v in enumerate(latent_vector[1:]):\n",
    "    d = round(euclidean_distance(standard, v), 8)\n",
    "    dist.append(d)\n",
    "    if d > right:\n",
    "        right= d\n",
    "    if left > d:\n",
    "        left = d\n",
    "dist1 = copy.deepcopy(dist)\n",
    "dist1.sort()\n",
    "#print(dist, dist1)\n",
    "for index, d in enumerate(dist1):\n",
    "    print( (index+1, dist.index(d)+1), end='  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(a, b):\n",
    "    return np.sqrt(np.sum(a-b) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

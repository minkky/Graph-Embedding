{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob, os\n",
    "from keras import layers as ly\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras.backend.tensorflow_backend as K\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Lambda\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir = './sequence/*'\n",
    "dir = './latest_sequence/bfs/*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file read\n",
    "all_data = []\n",
    "sequence_length = []\n",
    "for file in sorted(glob.glob(dir)):\n",
    "    datasets = []\n",
    "    for f in open(file, 'r'):\n",
    "        f = f.replace(']', '').replace('[', '').replace('\\n','')\n",
    "        (u, v, w) = f.split(',')\n",
    "        datasets.append([int(u), int(v), float(w)])\n",
    "    sequence_length.append(len(datasets))\n",
    "    all_data.append(datasets)\n",
    "#all_data = np.array(all_data)\n",
    "all_data = np.array([np.array(arr) for arr in all_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = train_test_split(all_data, test_size = 0.2)\n",
    "x_test, x_val = train_test_split(x_test, test_size = 0.66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = max(sequence_length)\n",
    "n_features = 3\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "steps_per_epoch = len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def repeat_vector(args):\n",
    "    layer_to_repeat = args[0]\n",
    "    sequence_layer = args[1]\n",
    "    return RepeatVector(K.shape(sequence_layer)[1])(layer_to_repeat)\n",
    "    \n",
    "\n",
    "inputs = Input(shape=(None, 3))\n",
    "encoded = LSTM(128, return_sequences=True)(inputs)  #activation 안적으면 tanh\n",
    "encoded = LSTM(64)(encoded)\n",
    "\n",
    "decoded = Lambda(repeat_vector, output_shape=(None, 64)) ([encoded, inputs]) # inputs의 shape[1] 만큼 encoded 를 반복 생성\n",
    "\n",
    "decoded = LSTM(64, return_sequences=True)(decoded)\n",
    "decoded = LSTM(128, return_sequences=True)(decoded)\n",
    "decoded = TimeDistributed(Dense(3))(decoded)\n",
    "encoder = Model(inputs, encoded)\n",
    "\n",
    "lstm_autoencoder = Model(inputs, decoded)\n",
    "lstm_autoencoder.compile(loss='mean_squared_error', optimizer=Adam(lr=0.0006))\n",
    "lstm_autoencoder_250 = lstm_autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(x_train):\n",
    "    idx = 0\n",
    "    while True:\n",
    "        yield np.array([x_train[idx]]), np.array([x_train[idx]])\n",
    "        idx +=1\n",
    "        if idx >= len(x_train):\n",
    "            idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2688/2688 [==============================] - 24s 9ms/step - loss: 37.5512\n",
      "Epoch 2/200\n",
      "2688/2688 [==============================] - 22s 8ms/step - loss: 36.3563\n",
      "Epoch 3/200\n",
      "2688/2688 [==============================] - 22s 8ms/step - loss: 21.9553\n",
      "Epoch 4/200\n",
      "2688/2688 [==============================] - 24s 9ms/step - loss: 13.4853\n",
      "Epoch 5/200\n",
      "2688/2688 [==============================] - 25s 9ms/step - loss: 11.8289\n",
      "Epoch 6/200\n",
      "2688/2688 [==============================] - 24s 9ms/step - loss: 10.5707\n",
      "Epoch 7/200\n",
      "2688/2688 [==============================] - 27s 10ms/step - loss: 9.6544\n",
      "Epoch 8/200\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 8.9367\n",
      "Epoch 9/200\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 8.1741\n",
      "Epoch 10/200\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 7.1135\n",
      "Epoch 11/200\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 6.3514\n",
      "Epoch 12/200\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 5.7745\n",
      "Epoch 13/200\n",
      "2688/2688 [==============================] - 34s 13ms/step - loss: 5.4119\n",
      "Epoch 14/200\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 4.6558\n",
      "Epoch 15/200\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 4.3518\n",
      "Epoch 16/200\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 4.0956\n",
      "Epoch 17/200\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 3.7293\n",
      "Epoch 18/200\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 3.7469\n",
      "Epoch 19/200\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 3.5352\n",
      "Epoch 20/200\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 3.2048\n",
      "Epoch 21/200\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 3.0690\n",
      "Epoch 22/200\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 2.7241\n",
      "Epoch 23/200\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 2.9371\n",
      "Epoch 24/200\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 2.7871\n",
      "Epoch 25/200\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 2.5405\n",
      "Epoch 26/200\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 2.5505\n",
      "Epoch 27/200\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 2.4883\n",
      "Epoch 28/200\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 2.3237\n",
      "Epoch 29/200\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 2.4148\n",
      "Epoch 30/200\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 2.5965\n",
      "Epoch 31/200\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 2.0054\n",
      "Epoch 32/200\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 2.1048\n",
      "Epoch 33/200\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 2.0736\n",
      "Epoch 34/200\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 1.9789\n",
      "Epoch 35/200\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 2.0764\n",
      "Epoch 36/200\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 1.7174\n",
      "Epoch 37/200\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 1.5334\n",
      "Epoch 38/200\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 1.6143\n",
      "Epoch 39/200\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 1.5541\n",
      "Epoch 40/200\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 1.6304\n",
      "Epoch 41/200\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 1.5486\n",
      "Epoch 42/200\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 1.2414\n",
      "Epoch 43/200\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 1.3592\n",
      "Epoch 44/200\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 1.4656\n",
      "Epoch 45/200\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 1.3531\n",
      "Epoch 46/200\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 1.2411\n",
      "Epoch 47/200\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 1.3699\n",
      "Epoch 48/200\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 1.0677\n",
      "Epoch 49/200\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 1.1840\n",
      "Epoch 50/200\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 1.0227\n",
      "Epoch 51/200\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 1.2333\n",
      "Epoch 52/200\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 1.3219\n",
      "Epoch 53/200\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 1.0562\n",
      "Epoch 54/200\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.7908\n",
      "Epoch 55/200\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 1.0510\n",
      "Epoch 56/200\n",
      "2688/2688 [==============================] - 34s 13ms/step - loss: 1.0227\n",
      "Epoch 57/200\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.8109\n",
      "Epoch 58/200\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.7104\n",
      "Epoch 59/200\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.8982\n",
      "Epoch 60/200\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.7587\n",
      "Epoch 61/200\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.8900\n",
      "Epoch 62/200\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.6373\n",
      "Epoch 63/200\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.5818\n",
      "Epoch 64/200\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.6161\n",
      "Epoch 65/200\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.9332\n",
      "Epoch 66/200\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.5612\n",
      "Epoch 67/200\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.7867\n",
      "Epoch 68/200\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.8026\n",
      "Epoch 69/200\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.6306\n",
      "Epoch 70/200\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.6488\n",
      "Epoch 71/200\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.5437\n",
      "Epoch 72/200\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.4229\n",
      "Epoch 73/200\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.4936\n",
      "Epoch 74/200\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.6530\n",
      "Epoch 75/200\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.5228\n",
      "Epoch 76/200\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.4920\n",
      "Epoch 77/200\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.5926\n",
      "Epoch 78/200\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.3904\n",
      "Epoch 79/200\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.3690\n",
      "Epoch 80/200\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.4916\n",
      "Epoch 81/200\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.5590\n",
      "Epoch 82/200\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.3792\n",
      "Epoch 83/200\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.3024\n",
      "Epoch 84/200\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.6571\n",
      "Epoch 85/200\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.3422\n",
      "Epoch 86/200\n",
      "2688/2688 [==============================] - 34s 13ms/step - loss: 0.2970\n",
      "Epoch 87/200\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.3964\n",
      "Epoch 88/200\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.2579\n",
      "Epoch 89/200\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.2720\n",
      "Epoch 90/200\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.4328\n",
      "Epoch 91/200\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.3855\n",
      "Epoch 92/200\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.4093\n",
      "Epoch 93/200\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.2802\n",
      "Epoch 94/200\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.5139\n",
      "Epoch 95/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.5175\n",
      "Epoch 96/200\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.5175\n",
      "Epoch 97/200\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.4889\n",
      "Epoch 98/200\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.3013\n",
      "Epoch 99/200\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.2131\n",
      "Epoch 100/200\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.2731\n",
      "Epoch 101/200\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.4394\n",
      "Epoch 102/200\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.3274\n",
      "Epoch 103/200\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.2613\n",
      "Epoch 104/200\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.2239\n",
      "Epoch 105/200\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.2175\n",
      "Epoch 106/200\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.2381\n",
      "Epoch 107/200\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.2680\n",
      "Epoch 108/200\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.3037\n",
      "Epoch 109/200\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.2619\n",
      "Epoch 110/200\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.1814\n",
      "Epoch 111/200\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.1899\n",
      "Epoch 112/200\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.1938\n",
      "Epoch 113/200\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.1900\n",
      "Epoch 114/200\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.1826\n",
      "Epoch 115/200\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 0.3042\n",
      "Epoch 116/200\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.1885\n",
      "Epoch 117/200\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.1369\n",
      "Epoch 118/200\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.1529\n",
      "Epoch 119/200\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.1706\n",
      "Epoch 120/200\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.1643\n",
      "Epoch 121/200\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.1606\n",
      "Epoch 122/200\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.1718\n",
      "Epoch 123/200\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.1645\n",
      "Epoch 124/200\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.2366\n",
      "Epoch 125/200\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.1801\n",
      "Epoch 126/200\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.1292\n",
      "Epoch 127/200\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.2643\n",
      "Epoch 128/200\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.1011\n",
      "Epoch 129/200\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.1262\n",
      "Epoch 130/200\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.1447\n",
      "Epoch 131/200\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.1519\n",
      "Epoch 132/200\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.1723\n",
      "Epoch 133/200\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.1313\n",
      "Epoch 134/200\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.1410\n",
      "Epoch 135/200\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.1680\n",
      "Epoch 136/200\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 0.1763\n",
      "Epoch 137/200\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.1183\n",
      "Epoch 138/200\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.1437\n",
      "Epoch 139/200\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.4061\n",
      "Epoch 140/200\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.2415\n",
      "Epoch 141/200\n",
      " 193/2688 [=>............................] - ETA: 28s - loss: 0.1144"
     ]
    }
   ],
   "source": [
    "lstm_autoencoder.fit_generator(train_generator(x_train), epochs=200, steps_per_epoch=steps_per_epoch, verbose=1)#, validation_data=val_generator(x_val))\n",
    "\n",
    "'''\n",
    "- epochs 467 - 0.1900\n",
    "- epochs 100 해서 확인해보면 loss 가 너무 들쑥날쑥한 걸 확인할 수 있음 1.22\n",
    "- epochs 500 0.2346\n",
    "\n",
    "- 데이터 셋 변경 epochs 500, loss - 0.0572, lr-0.001\n",
    "  lr-0.0006 0.0547\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_autoencoder_250.fit_generator(train_generator(x_train), epochs=500, steps_per_epoch=steps_per_epoch, verbose=1)#, validation_data=val_generator(x_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model save\n",
    "lstm_autoencoder.save('model/lstm_autoencoder200_bfs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model('model/lstm_autoencoder_bfs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean= 0\n",
    "for xt in x_test:\n",
    "    xt = xt.reshape(1, xt.shape[0], xt.shape[1])\n",
    "    out = loaded_model.predict(xt)\n",
    "    mean += ((xt-out)**2).mean(axis=None)\n",
    "print(mean/len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(loaded_model.input, loaded_model.layers[3].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Layer time_distributed_3 has multiple inbound nodes, hence the notion of \"layer output\" is ill-defined. Use `get_output_at(node_index)` instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-b399cc0be62b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdecoder_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m decoder = Model(loaded_model.layers[2].output, \n\u001b[0;32m----> 3\u001b[0;31m                         loaded_model.layers[6].output)\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36moutput\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inbound_nodes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m             raise AttributeError('Layer ' + self.name +\n\u001b[0;32m--> 807\u001b[0;31m                                  \u001b[0;34m' has multiple inbound nodes, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m                                  \u001b[0;34m'hence the notion of \"layer output\" '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m                                  \u001b[0;34m'is ill-defined. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Layer time_distributed_3 has multiple inbound nodes, hence the notion of \"layer output\" is ill-defined. Use `get_output_at(node_index)` instead."
     ]
    }
   ],
   "source": [
    "decoder_layer = loaded_model.layers[-1]\n",
    "decoder = Model(loaded_model.layers[2].output, \n",
    "                        loaded_model.layers[6].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = x_test[0].reshape(1, 9, 3)\n",
    "latent_vector = []\n",
    "for x in x_test:\n",
    "    x = x.reshape(1, x.shape[0], x.shape[1])\n",
    "    latent_vector.append(encoder.predict(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.0,\n",
       " -0.00020211579,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.74070954,\n",
       " -0.029288653,\n",
       " -0.74629986,\n",
       " 0.0,\n",
       " 0.12747759,\n",
       " -0.0014260766,\n",
       " -0.0,\n",
       " 0.10494814,\n",
       " 0.2187337,\n",
       " -0.18827741,\n",
       " 0.040405355,\n",
       " -0.5645764,\n",
       " 0.0022893408,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.2676836,\n",
       " -0.7395759,\n",
       " 0.2642008,\n",
       " -0.41065842,\n",
       " 0.19330677,\n",
       " -0.3758831,\n",
       " -0.21791713,\n",
       " 0.055199105,\n",
       " -0.25927162,\n",
       " 0.23109113,\n",
       " 0.8425839,\n",
       " -0.03407281,\n",
       " -0.8132615,\n",
       " 0.0,\n",
       " 0.49131706,\n",
       " 0.16162786,\n",
       " 0.35315764,\n",
       " -0.0,\n",
       " -0.7594612,\n",
       " -0.6364968,\n",
       " 0.0,\n",
       " 0.009271684,\n",
       " 0.0,\n",
       " 0.30094486,\n",
       " 0.0,\n",
       " -0.0027280087,\n",
       " -0.25117594,\n",
       " 0.0,\n",
       " 0.19940257,\n",
       " 0.32026434,\n",
       " 0.5843708,\n",
       " 0.032265846,\n",
       " -0.0,\n",
       " -0.0140618635,\n",
       " -0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.0,\n",
       " 0.254268,\n",
       " -0.0,\n",
       " 0.0,\n",
       " -0.0,\n",
       " 0.19169566,\n",
       " 0.0,\n",
       " 0.7144473]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_vector = np.array(latent_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printMatrix(matrix):\n",
    "    length = int(max(max(matrix[:, 0]), max(matrix[:, 1])))\n",
    "    adj = [[0 for i in range(length)] for i in range(length)]\n",
    "    \n",
    "    for m in matrix:\n",
    "        (i, j, w) = m\n",
    "        i = int(i) -1\n",
    "        j = int(j) -1\n",
    "        adj[i][j] = adj[j][i] = w\n",
    "    for i in range(length):\n",
    "        for j in range(length):\n",
    "            print(str(adj[i][j]), end=',')\n",
    "        print()\n",
    "    print(\"===============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 0\n",
      "0,10.16,7.92,9.56,0,0,0,0,0,\n",
      "10.16,0,0,0,0,0,0,0,0,\n",
      "7.92,0,0,0,8.17,18.05,0,0,0,\n",
      "9.56,0,0,0,0,0,20.05,0,0,\n",
      "0,0,8.17,0,0,0,0,0,0,\n",
      "0,0,18.05,0,0,0,0,32.09,0,\n",
      "0,0,0,20.05,0,0,0,0,20.27,\n",
      "0,0,0,0,0,32.09,0,0,9.01,\n",
      "0,0,0,0,0,0,20.27,9.01,0,\n",
      "===============\n",
      "# 1\n",
      "0,8.79,11.79,0,0,0,\n",
      "8.79,0,0,10.06,6.86,0,\n",
      "11.79,0,0,0,0,7.86,\n",
      "0,10.06,0,0,0,0,\n",
      "0,6.86,0,0,0,0,\n",
      "0,0,7.86,0,0,0,\n",
      "===============\n",
      "# 2\n",
      "0,3.3,6.3,0,0,\n",
      "3.3,0,0,12.72,0,\n",
      "6.3,0,0,0,23.87,\n",
      "0,12.72,0,0,41.95,\n",
      "0,0,23.87,41.95,0,\n",
      "===============\n",
      "# 3\n",
      "0,5.7,7.22,0,0,\n",
      "5.7,0,0,12.22,0,\n",
      "7.22,0,0,0,23.54,\n",
      "0,12.22,0,0,42.39,\n",
      "0,0,23.54,42.39,0,\n",
      "===============\n",
      "# 4\n",
      "0,12.52,17.52,28.77,0,0,0,0,0,\n",
      "12.52,0,0,0,0,0,0,0,0,\n",
      "17.52,0,0,0,2.53,4.53,0,0,0,\n",
      "28.77,0,0,0,0,0,10.86,0,0,\n",
      "0,0,2.53,0,0,0,0,0,0,\n",
      "0,0,4.53,0,0,0,0,9.01,0,\n",
      "0,0,0,10.86,0,0,0,0,10.01,\n",
      "0,0,0,0,0,9.01,0,0,20.01,\n",
      "0,0,0,0,0,0,10.01,20.01,0,\n",
      "===============\n",
      "# 5\n",
      "0,4.27,9.29,0,0,\n",
      "4.27,0,0,12.29,0,\n",
      "9.29,0,0,0,17.29,\n",
      "0,12.29,0,0,3.95,\n",
      "0,0,17.29,3.95,0,\n",
      "===============\n",
      "# 6\n",
      "0,10.16,7.92,9.56,0,0,0,0,0,\n",
      "10.16,0,0,0,0,0,0,0,0,\n",
      "7.92,0,0,0,8.17,18.05,0,0,0,\n",
      "9.56,0,0,0,0,0,20.05,0,0,\n",
      "0,0,8.17,0,0,0,0,0,0,\n",
      "0,0,18.05,0,0,0,0,32.09,0,\n",
      "0,0,0,20.05,0,0,0,0,20.27,\n",
      "0,0,0,0,0,32.09,0,0,9.01,\n",
      "0,0,0,0,0,0,20.27,9.01,0,\n",
      "===============\n",
      "# 7\n",
      "0,9.1,12.1,23.12,0,0,\n",
      "9.1,0,11.14,0,21.31,0,\n",
      "12.1,11.14,0,0,0,38.69,\n",
      "23.12,0,0,0,0,0,\n",
      "0,21.31,0,0,0,76.3,\n",
      "0,0,38.69,0,76.3,0,\n",
      "===============\n",
      "# 8\n",
      "0,5.68,4.49,0,0,\n",
      "5.68,0,0,10.64,0,\n",
      "4.49,0,0,0,20.73,\n",
      "0,10.64,0,0,23.73,\n",
      "0,0,20.73,23.73,0,\n",
      "===============\n",
      "# 9\n",
      "0,2.81,9.59,11.34,0,\n",
      "2.81,0,21.48,0,8.05,\n",
      "9.59,21.48,0,0,0,\n",
      "11.34,0,0,0,0,\n",
      "0,8.05,0,0,0,\n",
      "===============\n",
      "# 10\n",
      "0,5.11,6.66,8.4,\n",
      "5.11,0,18.0,0,\n",
      "6.66,18.0,0,0,\n",
      "8.4,0,0,0,\n",
      "===============\n",
      "# 11\n",
      "0,5.95,15.4,29.79,0,0,\n",
      "5.95,0,14.98,0,4.4,0,\n",
      "15.4,14.98,0,0,0,13.89,\n",
      "29.79,0,0,0,0,0,\n",
      "0,4.4,0,0,0,24.58,\n",
      "0,0,13.89,0,24.58,0,\n",
      "===============\n",
      "# 12\n",
      "0,6.41,4.09,0,0,0,\n",
      "6.41,0,0,8.09,16.24,0,\n",
      "4.09,0,0,0,0,4.33,\n",
      "0,8.09,0,0,0,0,\n",
      "0,16.24,0,0,0,0,\n",
      "0,0,4.33,0,0,0,\n",
      "===============\n",
      "# 13\n",
      "0,5.46,10.39,0,0,0,0,0,0,0,\n",
      "5.46,0,0,4.69,6.99,0,0,0,0,0,\n",
      "10.39,0,0,0,0,6.57,0,0,0,0,\n",
      "0,4.69,0,0,0,0,5.81,8.41,0,0,\n",
      "0,6.99,0,0,0,0,0,0,0,0,\n",
      "0,0,6.57,0,0,0,0,0,11.41,0,\n",
      "0,0,0,5.81,0,0,0,0,0,0,\n",
      "0,0,0,8.41,0,0,0,0,0,0,\n",
      "0,0,0,0,0,11.41,0,0,0,22.01,\n",
      "0,0,0,0,0,0,0,0,22.01,0,\n",
      "===============\n",
      "# 14\n",
      "0,7.91,9.61,0,0,0,0,\n",
      "7.91,0,0,19.41,7.36,0,0,\n",
      "9.61,0,0,0,0,15.61,11.39,\n",
      "0,19.41,0,0,0,0,0,\n",
      "0,7.36,0,0,0,0,0,\n",
      "0,0,15.61,0,0,0,0,\n",
      "0,0,11.39,0,0,0,0,\n",
      "===============\n",
      "# 15\n",
      "0,2.49,10.47,0,0,0,0,\n",
      "2.49,0,0,20.49,24.49,0,0,\n",
      "10.47,0,0,0,0,0,0,\n",
      "0,20.49,0,0,0,44.3,60.58,\n",
      "0,24.49,0,0,0,0,0,\n",
      "0,0,0,44.3,0,0,0,\n",
      "0,0,0,60.58,0,0,0,\n",
      "===============\n",
      "# 16\n",
      "0,9.84,14.84,0,0,0,0,0,0,0,\n",
      "9.84,0,0,28.34,52.46,0,0,0,0,0,\n",
      "14.84,0,0,0,0,20.74,0,0,0,0,\n",
      "0,28.34,0,0,0,0,1.8,8.07,0,0,\n",
      "0,52.46,0,0,0,0,0,0,0,0,\n",
      "0,0,20.74,0,0,0,0,0,10.07,0,\n",
      "0,0,0,1.8,0,0,0,0,0,0,\n",
      "0,0,0,8.07,0,0,0,0,0,0,\n",
      "0,0,0,0,0,10.07,0,0,0,20.12,\n",
      "0,0,0,0,0,0,0,0,20.12,0,\n",
      "===============\n",
      "# 17\n",
      "0,5.68,4.49,0,0,\n",
      "5.68,0,0,10.64,0,\n",
      "4.49,0,0,0,20.73,\n",
      "0,10.64,0,0,23.73,\n",
      "0,0,20.73,23.73,0,\n",
      "===============\n",
      "# 18\n",
      "0,7.44,9.44,7.01,0,0,\n",
      "7.44,0,9.01,0,9.95,0,\n",
      "9.44,9.01,0,0,0,6.37,\n",
      "7.01,0,0,0,0,0,\n",
      "0,9.95,0,0,0,5.54,\n",
      "0,0,6.37,0,5.54,0,\n",
      "===============\n",
      "# 19\n",
      "0,11.21,1.03,3.72,0,\n",
      "11.21,0,7.76,0,6.56,\n",
      "1.03,7.76,0,0,0,\n",
      "3.72,0,0,0,0,\n",
      "0,6.56,0,0,0,\n",
      "===============\n",
      "# 20\n",
      "0,4.71,7.93,0,0,0,0,0,0,0,\n",
      "4.71,0,0,16.93,3.43,0,0,0,0,0,\n",
      "7.93,0,0,0,0,9.66,0,0,0,0,\n",
      "0,16.93,0,0,0,0,8.09,18.05,0,0,\n",
      "0,3.43,0,0,0,0,0,0,0,0,\n",
      "0,0,9.66,0,0,0,0,0,30.86,0,\n",
      "0,0,0,8.09,0,0,0,0,0,0,\n",
      "0,0,0,18.05,0,0,0,0,0,0,\n",
      "0,0,0,0,0,30.86,0,0,0,31.86,\n",
      "0,0,0,0,0,0,0,0,31.86,0,\n",
      "===============\n",
      "# 21\n",
      "0,4.54,12.82,23.2,0,0,\n",
      "4.54,0,12.56,0,12.48,0,\n",
      "12.82,12.56,0,0,0,2.44,\n",
      "23.2,0,0,0,0,0,\n",
      "0,12.48,0,0,0,6.95,\n",
      "0,0,2.44,0,6.95,0,\n",
      "===============\n",
      "# 22\n",
      "0,9.7,5.1,5.12,0,0,0,0,0,\n",
      "9.7,0,0,0,0,0,0,0,0,\n",
      "5.1,0,0,0,13.29,10.61,0,0,0,\n",
      "5.12,0,0,0,0,0,20.94,0,0,\n",
      "0,0,13.29,0,0,0,0,0,0,\n",
      "0,0,10.61,0,0,0,0,8.35,0,\n",
      "0,0,0,20.94,0,0,0,0,6.14,\n",
      "0,0,0,0,0,8.35,0,0,14.26,\n",
      "0,0,0,0,0,0,6.14,14.26,0,\n",
      "===============\n",
      "# 23\n",
      "0,6.81,0,0,\n",
      "6.81,0,9.81,11.46,\n",
      "0,9.81,0,21.99,\n",
      "0,11.46,21.99,0,\n",
      "===============\n",
      "# 24\n",
      "0,8.12,16.82,0,0,\n",
      "8.12,0,0,3.04,0,\n",
      "16.82,0,0,0,9.05,\n",
      "0,3.04,0,0,6.2,\n",
      "0,0,9.05,6.2,0,\n",
      "===============\n",
      "# 25\n",
      "0,5.09,2.06,4.17,0,0,\n",
      "5.09,0,11.95,0,13.95,0,\n",
      "2.06,11.95,0,0,0,2.21,\n",
      "4.17,0,0,0,0,0,\n",
      "0,13.95,0,0,0,11.03,\n",
      "0,0,2.21,0,11.03,0,\n",
      "===============\n",
      "# 26\n",
      "0,5.46,10.39,0,0,0,0,0,0,0,\n",
      "5.46,0,0,4.69,6.99,0,0,0,0,0,\n",
      "10.39,0,0,0,0,6.57,0,0,0,0,\n",
      "0,4.69,0,0,0,0,5.81,8.41,0,0,\n",
      "0,6.99,0,0,0,0,0,0,0,0,\n",
      "0,0,6.57,0,0,0,0,0,11.41,0,\n",
      "0,0,0,5.81,0,0,0,0,0,0,\n",
      "0,0,0,8.41,0,0,0,0,0,0,\n",
      "0,0,0,0,0,11.41,0,0,0,22.01,\n",
      "0,0,0,0,0,0,0,0,22.01,0,\n",
      "===============\n",
      "# 27\n",
      "0,5.09,2.06,4.17,0,0,\n",
      "5.09,0,11.95,0,13.95,0,\n",
      "2.06,11.95,0,0,0,2.21,\n",
      "4.17,0,0,0,0,0,\n",
      "0,13.95,0,0,0,11.03,\n",
      "0,0,2.21,0,11.03,0,\n",
      "===============\n",
      "# 28\n",
      "0,4.16,7.16,0,0,0,0,\n",
      "4.16,0,0,16.83,28.23,0,0,\n",
      "7.16,0,0,0,0,33.23,62.71,\n",
      "0,16.83,0,0,0,0,0,\n",
      "0,28.23,0,0,0,0,0,\n",
      "0,0,33.23,0,0,0,0,\n",
      "0,0,62.71,0,0,0,0,\n",
      "===============\n",
      "# 29\n",
      "0,6.13,5.95,0,0,0,0,0,0,0,\n",
      "6.13,0,0,10.95,21.64,0,0,0,0,0,\n",
      "5.95,0,0,0,0,23.64,0,0,0,0,\n",
      "0,10.95,0,0,0,0,6.23,8.77,0,0,\n",
      "0,21.64,0,0,0,0,0,0,0,0,\n",
      "0,0,23.64,0,0,0,0,0,10.77,0,\n",
      "0,0,0,6.23,0,0,0,0,0,0,\n",
      "0,0,0,8.77,0,0,0,0,0,0,\n",
      "0,0,0,0,0,10.77,0,0,0,1.34,\n",
      "0,0,0,0,0,0,0,0,1.34,0,\n",
      "===============\n",
      "# 30\n",
      "0,10.37,8.48,6.98,0,0,0,0,0,\n",
      "10.37,0,0,0,0,0,0,0,0,\n",
      "8.48,0,0,0,12.67,22.75,0,0,0,\n",
      "6.98,0,0,0,0,0,27.75,0,0,\n",
      "0,0,12.67,0,0,0,0,0,0,\n",
      "0,0,22.75,0,0,0,0,2.0,0,\n",
      "0,0,0,27.75,0,0,0,0,7.78,\n",
      "0,0,0,0,0,2.0,0,0,15.79,\n",
      "0,0,0,0,0,0,7.78,15.79,0,\n",
      "===============\n",
      "# 31\n",
      "0,3.09,4.09,0,0,0,0,0,0,0,\n",
      "3.09,0,0,7.31,6.24,0,0,0,0,0,\n",
      "4.09,0,0,0,0,10.78,0,0,0,0,\n",
      "0,7.31,0,0,0,0,5.75,8.53,0,0,\n",
      "0,6.24,0,0,0,0,0,0,0,0,\n",
      "0,0,10.78,0,0,0,0,0,6.23,0,\n",
      "0,0,0,5.75,0,0,0,0,0,0,\n",
      "0,0,0,8.53,0,0,0,0,0,0,\n",
      "0,0,0,0,0,6.23,0,0,0,9.5,\n",
      "0,0,0,0,0,0,0,0,9.5,0,\n",
      "===============\n",
      "# 32\n",
      "0,7.38,6.51,0,0,0,0,0,0,0,\n",
      "7.38,0,0,9.96,10.96,0,0,0,0,0,\n",
      "6.51,0,0,0,0,21.85,0,0,0,0,\n",
      "0,9.96,0,0,0,0,38.35,27.88,0,0,\n",
      "0,10.96,0,0,0,0,0,0,0,0,\n",
      "0,0,21.85,0,0,0,0,0,37.89,0,\n",
      "0,0,0,38.35,0,0,0,0,0,0,\n",
      "0,0,0,27.88,0,0,0,0,0,0,\n",
      "0,0,0,0,0,37.89,0,0,0,39.89,\n",
      "0,0,0,0,0,0,0,0,39.89,0,\n",
      "===============\n",
      "# 33\n",
      "0,8.46,9.46,0,0,0,0,\n",
      "8.46,0,0,8.28,11.91,0,0,\n",
      "9.46,0,0,0,0,0,0,\n",
      "0,8.28,0,0,0,1.22,9.42,\n",
      "0,11.91,0,0,0,0,0,\n",
      "0,0,0,1.22,0,0,0,\n",
      "0,0,0,9.42,0,0,0,\n",
      "===============\n",
      "# 34\n",
      "0,4.35,3.05,12.89,0,\n",
      "4.35,0,17.89,0,28.52,\n",
      "3.05,17.89,0,0,0,\n",
      "12.89,0,0,0,0,\n",
      "0,28.52,0,0,0,\n",
      "===============\n",
      "# 35\n",
      "0,7.21,4.47,0,0,0,0,\n",
      "7.21,0,0,13.78,2.09,0,0,\n",
      "4.47,0,0,0,0,4.09,5.11,\n",
      "0,13.78,0,0,0,0,0,\n",
      "0,2.09,0,0,0,0,0,\n",
      "0,0,4.09,0,0,0,0,\n",
      "0,0,5.11,0,0,0,0,\n",
      "===============\n",
      "# 36\n",
      "0,8.54,12.54,0,0,0,0,\n",
      "8.54,0,0,1.84,9.02,0,0,\n",
      "12.54,0,0,0,0,7.16,12.16,\n",
      "0,1.84,0,0,0,0,0,\n",
      "0,9.02,0,0,0,0,0,\n",
      "0,0,7.16,0,0,0,0,\n",
      "0,0,12.16,0,0,0,0,\n",
      "===============\n",
      "# 37\n",
      "0,4.65,9.65,0,0,0,\n",
      "4.65,0,0,6.96,4.92,0,\n",
      "9.65,0,0,0,0,5.92,\n",
      "0,6.96,0,0,0,0,\n",
      "0,4.92,0,0,0,0,\n",
      "0,0,5.92,0,0,0,\n",
      "===============\n",
      "# 38\n",
      "0,8.6,8.71,18.02,0,0,0,0,0,\n",
      "8.6,0,0,0,0,0,0,0,0,\n",
      "8.71,0,0,0,1.3,9.61,0,0,0,\n",
      "18.02,0,0,0,0,0,19.5,0,0,\n",
      "0,0,1.3,0,0,0,0,0,0,\n",
      "0,0,9.61,0,0,0,0,6.15,0,\n",
      "0,0,0,19.5,0,0,0,0,4.59,\n",
      "0,0,0,0,0,6.15,0,0,5.8,\n",
      "0,0,0,0,0,0,4.59,5.8,0,\n",
      "===============\n",
      "# 39\n",
      "0,11.2,9.42,0,0,0,0,\n",
      "11.2,0,0,8.63,11.63,0,0,\n",
      "9.42,0,0,0,0,0,0,\n",
      "0,8.63,0,0,0,6.9,10.69,\n",
      "0,11.63,0,0,0,0,0,\n",
      "0,0,0,6.9,0,0,0,\n",
      "0,0,0,10.69,0,0,0,\n",
      "===============\n",
      "# 40\n",
      "0,11.32,22.47,7.57,0,0,\n",
      "11.32,0,15.33,0,5.29,0,\n",
      "22.47,15.33,0,0,0,5.21,\n",
      "7.57,0,0,0,0,0,\n",
      "0,5.29,0,0,0,13.0,\n",
      "0,0,5.21,0,13.0,0,\n",
      "===============\n",
      "# 41\n",
      "0,7.64,5.28,10.28,0,0,\n",
      "7.64,0,9.69,0,10.17,0,\n",
      "5.28,9.69,0,0,0,20.32,\n",
      "10.28,0,0,0,0,0,\n",
      "0,10.17,0,0,0,21.32,\n",
      "0,0,20.32,0,21.32,0,\n",
      "===============\n",
      "# 42\n",
      "0,2.73,7.31,10.0,0,0,0,0,0,\n",
      "2.73,0,0,0,0,0,0,0,0,\n",
      "7.31,0,0,0,6.1,11.5,0,0,0,\n",
      "10.0,0,0,0,0,0,1.37,0,0,\n",
      "0,0,6.1,0,0,0,0,0,0,\n",
      "0,0,11.5,0,0,0,0,10.51,0,\n",
      "0,0,0,1.37,0,0,0,0,9.33,\n",
      "0,0,0,0,0,10.51,0,0,11.07,\n",
      "0,0,0,0,0,0,9.33,11.07,0,\n",
      "===============\n",
      "# 43\n",
      "0,10.47,15.47,26.68,0,\n",
      "10.47,0,5.92,0,9.13,\n",
      "15.47,5.92,0,0,0,\n",
      "26.68,0,0,0,0,\n",
      "0,9.13,0,0,0,\n",
      "===============\n",
      "# 44\n",
      "0,5.13,3.73,0,0,0,0,\n",
      "5.13,0,0,12.26,14.26,0,0,\n",
      "3.73,0,0,0,0,6.24,14.5,\n",
      "0,12.26,0,0,0,0,0,\n",
      "0,14.26,0,0,0,0,0,\n",
      "0,0,6.24,0,0,0,0,\n",
      "0,0,14.5,0,0,0,0,\n",
      "===============\n",
      "# 45\n",
      "0,7.44,9.44,7.01,0,0,\n",
      "7.44,0,9.01,0,9.95,0,\n",
      "9.44,9.01,0,0,0,6.37,\n",
      "7.01,0,0,0,0,0,\n",
      "0,9.95,0,0,0,5.54,\n",
      "0,0,6.37,0,5.54,0,\n",
      "===============\n",
      "# 46\n",
      "0,3.57,9.11,0,0,0,0,\n",
      "3.57,0,0,8.53,12.29,0,0,\n",
      "9.11,0,0,0,0,0,0,\n",
      "0,8.53,0,0,0,2.17,9.34,\n",
      "0,12.29,0,0,0,0,0,\n",
      "0,0,0,2.17,0,0,0,\n",
      "0,0,0,9.34,0,0,0,\n",
      "===============\n",
      "# 47\n",
      "0,6.13,5.95,0,0,0,0,0,0,0,\n",
      "6.13,0,0,10.95,21.64,0,0,0,0,0,\n",
      "5.95,0,0,0,0,23.64,0,0,0,0,\n",
      "0,10.95,0,0,0,0,6.23,8.77,0,0,\n",
      "0,21.64,0,0,0,0,0,0,0,0,\n",
      "0,0,23.64,0,0,0,0,0,10.77,0,\n",
      "0,0,0,6.23,0,0,0,0,0,0,\n",
      "0,0,0,8.77,0,0,0,0,0,0,\n",
      "0,0,0,0,0,10.77,0,0,0,1.34,\n",
      "0,0,0,0,0,0,0,0,1.34,0,\n",
      "===============\n",
      "# 48\n",
      "0,6.67,11.67,0,0,0,0,\n",
      "6.67,0,0,22.32,39.93,0,0,\n",
      "11.67,0,0,0,0,0,0,\n",
      "0,22.32,0,0,0,41.93,3.97,\n",
      "0,39.93,0,0,0,0,0,\n",
      "0,0,0,41.93,0,0,0,\n",
      "0,0,0,3.97,0,0,0,\n",
      "===============\n",
      "# 49\n",
      "0,5.97,10.97,21.23,0,0,\n",
      "5.97,0,23.23,0,10.81,0,\n",
      "10.97,23.23,0,0,0,6.14,\n",
      "21.23,0,0,0,0,0,\n",
      "0,10.81,0,0,0,6.19,\n",
      "0,0,6.14,0,6.19,0,\n",
      "===============\n",
      "# 50\n",
      "0,5.87,4.42,0,0,\n",
      "5.87,0,0,11.89,0,\n",
      "4.42,0,0,0,1.67,\n",
      "0,11.89,0,0,6.67,\n",
      "0,0,1.67,6.67,0,\n",
      "===============\n",
      "# 51\n",
      "0,6.45,15.17,2.95,\n",
      "6.45,0,7.95,0,\n",
      "15.17,7.95,0,0,\n",
      "2.95,0,0,0,\n",
      "===============\n",
      "# 52\n",
      "0,3.71,10.29,0,0,0,\n",
      "3.71,0,0,10.61,15.61,0,\n",
      "10.29,0,0,0,0,27.3,\n",
      "0,10.61,0,0,0,0,\n",
      "0,15.61,0,0,0,0,\n",
      "0,0,27.3,0,0,0,\n",
      "===============\n",
      "# 53\n",
      "0,11.2,9.42,0,0,0,0,\n",
      "11.2,0,0,8.63,11.63,0,0,\n",
      "9.42,0,0,0,0,0,0,\n",
      "0,8.63,0,0,0,6.9,10.69,\n",
      "0,11.63,0,0,0,0,0,\n",
      "0,0,0,6.9,0,0,0,\n",
      "0,0,0,10.69,0,0,0,\n",
      "===============\n",
      "# 54\n",
      "0,10.37,8.48,6.98,0,0,0,0,0,\n",
      "10.37,0,0,0,0,0,0,0,0,\n",
      "8.48,0,0,0,12.67,22.75,0,0,0,\n",
      "6.98,0,0,0,0,0,27.75,0,0,\n",
      "0,0,12.67,0,0,0,0,0,0,\n",
      "0,0,22.75,0,0,0,0,2.0,0,\n",
      "0,0,0,27.75,0,0,0,0,7.78,\n",
      "0,0,0,0,0,2.0,0,0,15.79,\n",
      "0,0,0,0,0,0,7.78,15.79,0,\n",
      "===============\n",
      "# 55\n",
      "0,11.34,22.67,0,0,0,\n",
      "11.34,0,0,23.67,42.83,0,\n",
      "22.67,0,0,0,0,45.83,\n",
      "0,23.67,0,0,0,0,\n",
      "0,42.83,0,0,0,0,\n",
      "0,0,45.83,0,0,0,\n",
      "===============\n",
      "# 56\n",
      "0,6.82,0,0,\n",
      "6.82,0,16.66,18.66,\n",
      "0,16.66,0,5.64,\n",
      "0,18.66,5.64,0,\n",
      "===============\n",
      "# 57\n",
      "0,6.0,8.84,10.6,0,0,\n",
      "6.0,0,10.99,0,12.99,0,\n",
      "8.84,10.99,0,0,0,1.37,\n",
      "10.6,0,0,0,0,0,\n",
      "0,12.99,0,0,0,3.77,\n",
      "0,0,1.37,0,3.77,0,\n",
      "===============\n",
      "# 58\n",
      "0,8.17,17.68,0,0,0,0,0,0,0,\n",
      "8.17,0,0,28.82,14.87,0,0,0,0,0,\n",
      "17.68,0,0,0,0,17.87,0,0,0,0,\n",
      "0,28.82,0,0,0,0,34.08,64.24,0,0,\n",
      "0,14.87,0,0,0,0,0,0,0,0,\n",
      "0,0,17.87,0,0,0,0,0,52.76,0,\n",
      "0,0,0,34.08,0,0,0,0,0,0,\n",
      "0,0,0,64.24,0,0,0,0,0,0,\n",
      "0,0,0,0,0,52.76,0,0,0,74.48,\n",
      "0,0,0,0,0,0,0,0,74.48,0,\n",
      "===============\n",
      "# 59\n",
      "0,12.31,13.31,0,0,0,\n",
      "12.31,0,0,1.24,9.71,0,\n",
      "13.31,0,0,0,0,19.48,\n",
      "0,1.24,0,0,0,0,\n",
      "0,9.71,0,0,0,0,\n",
      "0,0,19.48,0,0,0,\n",
      "===============\n",
      "# 60\n",
      "0,6.92,7.92,4.57,\n",
      "6.92,0,8.77,0,\n",
      "7.92,8.77,0,0,\n",
      "4.57,0,0,0,\n",
      "===============\n",
      "# 61\n",
      "0,9.7,5.1,5.12,0,0,0,0,0,\n",
      "9.7,0,0,0,0,0,0,0,0,\n",
      "5.1,0,0,0,13.29,10.61,0,0,0,\n",
      "5.12,0,0,0,0,0,20.94,0,0,\n",
      "0,0,13.29,0,0,0,0,0,0,\n",
      "0,0,10.61,0,0,0,0,8.35,0,\n",
      "0,0,0,20.94,0,0,0,0,6.14,\n",
      "0,0,0,0,0,8.35,0,0,14.26,\n",
      "0,0,0,0,0,0,6.14,14.26,0,\n",
      "===============\n",
      "# 62\n",
      "0,8.12,16.82,0,0,\n",
      "8.12,0,0,3.04,0,\n",
      "16.82,0,0,0,9.05,\n",
      "0,3.04,0,0,6.2,\n",
      "0,0,9.05,6.2,0,\n",
      "===============\n",
      "# 63\n",
      "0,8.17,17.68,0,0,0,0,0,0,0,\n",
      "8.17,0,0,28.82,14.87,0,0,0,0,0,\n",
      "17.68,0,0,0,0,17.87,0,0,0,0,\n",
      "0,28.82,0,0,0,0,34.08,64.24,0,0,\n",
      "0,14.87,0,0,0,0,0,0,0,0,\n",
      "0,0,17.87,0,0,0,0,0,52.76,0,\n",
      "0,0,0,34.08,0,0,0,0,0,0,\n",
      "0,0,0,64.24,0,0,0,0,0,0,\n",
      "0,0,0,0,0,52.76,0,0,0,74.48,\n",
      "0,0,0,0,0,0,0,0,74.48,0,\n",
      "===============\n"
     ]
    }
   ],
   "source": [
    "# sequence to matrix\n",
    "for index, x in enumerate(x_test):\n",
    "    print(\"#\", str(index))\n",
    "    printMatrix(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard = latent_vector[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)  (2, 38)  (3, 53)  (4, 5)  (5, 15)  (6, 44)  (7, 48)  (8, 21)  (9, 12)  (10, 43)  (11, 9)  (12, 18)  (13, 31)  (14, 59)  (15, 58)  (16, 4)  (17, 41)  (18, 27)  (19, 29)  (20, 33)  (21, 6)  (22, 3)  (23, 51)  (24, 35)  (25, 54)  (26, 55)  (27, 26)  (28, 62)  (29, 14)  (30, 46)  (31, 11)  (32, 45)  (33, 7)  (34, 50)  (35, 57)  (36, 8)  (37, 63)  (38, 60)  (39, 24)  (40, 52)  (41, 20)  (42, 13)  (43, 37)  (44, 23)  (45, 10)  (46, 1)  (47, 61)  (48, 42)  (49, 56)  (50, 34)  (51, 16)  (52, 32)  (53, 40)  (54, 39)  (55, 19)  (56, 17)  (57, 22)  (58, 49)  (59, 30)  (60, 36)  (61, 47)  (62, 28)  (63, 25)  "
     ]
    }
   ],
   "source": [
    "import copy\n",
    "dist = []\n",
    "left = 100000\n",
    "right = -10000\n",
    "for index, v in enumerate(latent_vector[1:]):\n",
    "    d = round(euclidean_distance(standard, v), 8)\n",
    "    dist.append(d)\n",
    "    if d > right:\n",
    "        right= d\n",
    "    if left > d:\n",
    "        left = d\n",
    "dist1 = copy.deepcopy(dist)\n",
    "dist1.sort()\n",
    "#print(dist, dist1)\n",
    "for index, d in enumerate(dist1):\n",
    "    print( (index+1, dist.index(d)+1), end='  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(a, b):\n",
    "    return np.sqrt(np.sum(a-b) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

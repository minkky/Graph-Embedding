{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob, os\n",
    "from keras import layers as ly\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras.backend.tensorflow_backend as K\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Lambda\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir = './sequence/*'\n",
    "dir = './latest_sequence/bfs/*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file read\n",
    "all_data = []\n",
    "sequence_length = []\n",
    "for file in sorted(glob.glob(dir)):\n",
    "    datasets = []\n",
    "    for f in open(file, 'r'):\n",
    "        f = f.replace(']', '').replace('[', '').replace('\\n','')\n",
    "        (u, v, w) = f.split(',')\n",
    "        datasets.append([int(u), int(v), float(w)])\n",
    "    sequence_length.append(len(datasets))\n",
    "    all_data.append(datasets)\n",
    "#all_data = np.array(all_data)\n",
    "all_data = np.array([np.array(arr) for arr in all_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = train_test_split(all_data, test_size = 0.2)\n",
    "x_test, x_val = train_test_split(x_test, test_size = 0.66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = max(sequence_length)\n",
    "n_features = 3\n",
    "batch_size = 32\n",
    "epochs = 500\n",
    "steps_per_epoch = len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def repeat_vector(args):\n",
    "    layer_to_repeat = args[0]\n",
    "    sequence_layer = args[1]\n",
    "    return RepeatVector(K.shape(sequence_layer)[1])(layer_to_repeat)\n",
    "    \n",
    "\n",
    "inputs = Input(shape=(None, 3))\n",
    "encoded = LSTM(128, return_sequences=True)(inputs)  #activation 안적으면 tanh\n",
    "encoded = LSTM(64)(encoded)\n",
    "\n",
    "decoded = Lambda(repeat_vector, output_shape=(None, 64)) ([encoded, inputs]) # inputs의 shape[1] 만큼 encoded 를 반복 생성\n",
    "\n",
    "decoded = LSTM(64, return_sequences=True)(decoded)\n",
    "decoded = LSTM(128, return_sequences=True)(decoded)\n",
    "decoded = TimeDistributed(Dense(3))(decoded)\n",
    "encoder = Model(inputs, encoded)\n",
    "\n",
    "lstm_autoencoder = Model(inputs, decoded)\n",
    "lstm_autoencoder.compile(loss='mean_squared_error', optimizer=Adam(lr=0.0006))\n",
    "lstm_autoencoder_500 = lstm_autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(x_train):\n",
    "    idx = 0\n",
    "    while True:\n",
    "        yield np.array([x_train[idx]]), np.array([x_train[idx]])\n",
    "        idx +=1\n",
    "        if idx >= len(x_train):\n",
    "            idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "2688/2688 [==============================] - 25s 9ms/step - loss: 37.4970\n",
      "Epoch 2/500\n",
      "2688/2688 [==============================] - 23s 8ms/step - loss: 36.3269\n",
      "Epoch 3/500\n",
      "2688/2688 [==============================] - 23s 9ms/step - loss: 36.3154\n",
      "Epoch 4/500\n",
      "2688/2688 [==============================] - 23s 9ms/step - loss: 23.7019\n",
      "Epoch 5/500\n",
      "2688/2688 [==============================] - 23s 8ms/step - loss: 13.2223\n",
      "Epoch 6/500\n",
      "2688/2688 [==============================] - 22s 8ms/step - loss: 11.1689\n",
      "Epoch 7/500\n",
      "2688/2688 [==============================] - 22s 8ms/step - loss: 10.4129\n",
      "Epoch 8/500\n",
      "2688/2688 [==============================] - 22s 8ms/step - loss: 9.2601\n",
      "Epoch 9/500\n",
      "2688/2688 [==============================] - 22s 8ms/step - loss: 8.2604\n",
      "Epoch 10/500\n",
      "2688/2688 [==============================] - 22s 8ms/step - loss: 6.8476\n",
      "Epoch 11/500\n",
      "2688/2688 [==============================] - 23s 8ms/step - loss: 6.0562\n",
      "Epoch 12/500\n",
      "2688/2688 [==============================] - 22s 8ms/step - loss: 5.0123\n",
      "Epoch 13/500\n",
      "2688/2688 [==============================] - 23s 8ms/step - loss: 4.5896\n",
      "Epoch 14/500\n",
      "2688/2688 [==============================] - 23s 9ms/step - loss: 4.2102\n",
      "Epoch 15/500\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 3.6978\n",
      "Epoch 16/500\n",
      "2688/2688 [==============================] - 34s 13ms/step - loss: 3.6002\n",
      "Epoch 17/500\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 2.9992\n",
      "Epoch 18/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 2.9895\n",
      "Epoch 19/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 2.6623\n",
      "Epoch 20/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 2.4461\n",
      "Epoch 21/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 2.4368\n",
      "Epoch 22/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 2.2566\n",
      "Epoch 23/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 2.1388\n",
      "Epoch 24/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 2.2243\n",
      "Epoch 25/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 2.2726\n",
      "Epoch 26/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 2.0462\n",
      "Epoch 27/500\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 1.9532\n",
      "Epoch 28/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 1.8904\n",
      "Epoch 29/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 1.6822\n",
      "Epoch 30/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 1.4391\n",
      "Epoch 31/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 1.6463\n",
      "Epoch 32/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 1.4238\n",
      "Epoch 33/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 1.4521\n",
      "Epoch 34/500\n",
      "2688/2688 [==============================] - 35s 13ms/step - loss: 1.1975\n",
      "Epoch 35/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 1.2574\n",
      "Epoch 36/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 1.0419\n",
      "Epoch 37/500\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 1.0174\n",
      "Epoch 38/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 1.0287\n",
      "Epoch 39/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 1.0188\n",
      "Epoch 40/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 1.0769\n",
      "Epoch 41/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 1.2795\n",
      "Epoch 42/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.8385\n",
      "Epoch 43/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.7893\n",
      "Epoch 44/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.8224\n",
      "Epoch 45/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.9223\n",
      "Epoch 46/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.7399\n",
      "Epoch 47/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.9809\n",
      "Epoch 48/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.8236\n",
      "Epoch 49/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.5090\n",
      "Epoch 50/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.5401\n",
      "Epoch 51/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.5340\n",
      "Epoch 52/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.6191\n",
      "Epoch 53/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.6362\n",
      "Epoch 54/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.4360\n",
      "Epoch 55/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.4363\n",
      "Epoch 56/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.5810\n",
      "Epoch 57/500\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.5337\n",
      "Epoch 58/500\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.5527\n",
      "Epoch 59/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.4715\n",
      "Epoch 60/500\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.5229\n",
      "Epoch 61/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.5483\n",
      "Epoch 62/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.2787\n",
      "Epoch 63/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.6186\n",
      "Epoch 64/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.3178\n",
      "Epoch 65/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.2598\n",
      "Epoch 66/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.3516\n",
      "Epoch 67/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.2991\n",
      "Epoch 68/500\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.3292\n",
      "Epoch 69/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.2511\n",
      "Epoch 70/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.2373\n",
      "Epoch 71/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.2568\n",
      "Epoch 72/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.4477\n",
      "Epoch 73/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.3297\n",
      "Epoch 74/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.5125\n",
      "Epoch 75/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.4224\n",
      "Epoch 76/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.1981\n",
      "Epoch 77/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.3360\n",
      "Epoch 78/500\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.2920\n",
      "Epoch 79/500\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.2739\n",
      "Epoch 80/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.2807\n",
      "Epoch 81/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.4134\n",
      "Epoch 82/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.2226\n",
      "Epoch 83/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.1933\n",
      "Epoch 84/500\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.2307\n",
      "Epoch 85/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.3680\n",
      "Epoch 86/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.3972\n",
      "Epoch 87/500\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.2062\n",
      "Epoch 88/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.1542\n",
      "Epoch 89/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.1696\n",
      "Epoch 90/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.1997\n",
      "Epoch 91/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.1769\n",
      "Epoch 92/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.1701\n",
      "Epoch 93/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.1616\n",
      "Epoch 94/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.1814\n",
      "Epoch 95/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.2075\n",
      "Epoch 96/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.1320\n",
      "Epoch 97/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.1569\n",
      "Epoch 98/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.1456\n",
      "Epoch 99/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.1370\n",
      "Epoch 100/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.1493\n",
      "Epoch 101/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.1311\n",
      "Epoch 102/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.1339\n",
      "Epoch 103/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.1218\n",
      "Epoch 104/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.1310\n",
      "Epoch 105/500\n",
      "2688/2688 [==============================] - 34s 13ms/step - loss: 0.1360\n",
      "Epoch 106/500\n",
      "2688/2688 [==============================] - 34s 13ms/step - loss: 0.1245\n",
      "Epoch 107/500\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.1535\n",
      "Epoch 108/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.1007\n",
      "Epoch 109/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.1106\n",
      "Epoch 110/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.2147\n",
      "Epoch 111/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.1338\n",
      "Epoch 112/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.1016\n",
      "Epoch 113/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.1146\n",
      "Epoch 114/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.1511\n",
      "Epoch 115/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0885\n",
      "Epoch 116/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0996\n",
      "Epoch 117/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.1086\n",
      "Epoch 118/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.1056\n",
      "Epoch 119/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.1139\n",
      "Epoch 120/500\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.0999\n",
      "Epoch 121/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0985\n",
      "Epoch 122/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0959\n",
      "Epoch 123/500\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.1136\n",
      "Epoch 124/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0920\n",
      "Epoch 125/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0929\n",
      "Epoch 126/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0961\n",
      "Epoch 127/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.0940\n",
      "Epoch 128/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.0845\n",
      "Epoch 129/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.0857\n",
      "Epoch 130/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.0897\n",
      "Epoch 131/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0858\n",
      "Epoch 132/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.0890\n",
      "Epoch 133/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0971\n",
      "Epoch 134/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.1091\n",
      "Epoch 135/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.2069\n",
      "Epoch 136/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.1098\n",
      "Epoch 137/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.0703\n",
      "Epoch 138/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.0747\n",
      "Epoch 139/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.0829\n",
      "Epoch 140/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.0957\n",
      "Epoch 141/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0826\n",
      "Epoch 142/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0746\n",
      "Epoch 143/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.0759\n",
      "Epoch 144/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0807\n",
      "Epoch 145/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0702\n",
      "Epoch 146/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0836\n",
      "Epoch 147/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0751\n",
      "Epoch 148/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0704\n",
      "Epoch 149/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0738\n",
      "Epoch 150/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.0877\n",
      "Epoch 151/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0589\n",
      "Epoch 152/500\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.0763\n",
      "Epoch 153/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.0706\n",
      "Epoch 154/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0645\n",
      "Epoch 155/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0723\n",
      "Epoch 156/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0653\n",
      "Epoch 157/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0659\n",
      "Epoch 158/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0627\n",
      "Epoch 159/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0700\n",
      "Epoch 160/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0660\n",
      "Epoch 161/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0683\n",
      "Epoch 162/500\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 0.0669\n",
      "Epoch 163/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0651\n",
      "Epoch 164/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0635\n",
      "Epoch 165/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0720\n",
      "Epoch 166/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0579\n",
      "Epoch 167/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0610\n",
      "Epoch 168/500\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 0.0607\n",
      "Epoch 169/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0630\n",
      "Epoch 170/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.1248\n",
      "Epoch 171/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0533\n",
      "Epoch 172/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0527\n",
      "Epoch 173/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0628\n",
      "Epoch 174/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0588\n",
      "Epoch 175/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0668\n",
      "Epoch 176/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0533\n",
      "Epoch 177/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0609\n",
      "Epoch 178/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0594\n",
      "Epoch 179/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0611\n",
      "Epoch 180/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0547\n",
      "Epoch 181/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0570\n",
      "Epoch 182/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0572\n",
      "Epoch 183/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0533\n",
      "Epoch 184/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0533\n",
      "Epoch 185/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0556\n",
      "Epoch 186/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0543\n",
      "Epoch 187/500\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 0.0540\n",
      "Epoch 188/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0559\n",
      "Epoch 189/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0533\n",
      "Epoch 190/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0511\n",
      "Epoch 191/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0507\n",
      "Epoch 192/500\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 0.0529\n",
      "Epoch 193/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0514\n",
      "Epoch 194/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0480\n",
      "Epoch 195/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0565\n",
      "Epoch 196/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0516\n",
      "Epoch 197/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0496\n",
      "Epoch 198/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0495\n",
      "Epoch 199/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0512\n",
      "Epoch 200/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0466\n",
      "Epoch 201/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0529\n",
      "Epoch 202/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0469\n",
      "Epoch 203/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0463\n",
      "Epoch 204/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0557\n",
      "Epoch 205/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0473\n",
      "Epoch 206/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0516\n",
      "Epoch 207/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0818\n",
      "Epoch 208/500\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 0.0604\n",
      "Epoch 209/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0795\n",
      "Epoch 210/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0641\n",
      "Epoch 211/500\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 0.0413\n",
      "Epoch 212/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0524\n",
      "Epoch 213/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0443\n",
      "Epoch 214/500\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 0.0462\n",
      "Epoch 215/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0483\n",
      "Epoch 216/500\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 0.0464\n",
      "Epoch 217/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0450\n",
      "Epoch 218/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0458\n",
      "Epoch 219/500\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 0.0440\n",
      "Epoch 220/500\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 0.0481\n",
      "Epoch 221/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0446\n",
      "Epoch 222/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0435\n",
      "Epoch 223/500\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 0.0448\n",
      "Epoch 224/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0445\n",
      "Epoch 225/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0445\n",
      "Epoch 226/500\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 0.0427\n",
      "Epoch 227/500\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 0.0485\n",
      "Epoch 228/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0402\n",
      "Epoch 229/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0421\n",
      "Epoch 230/500\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 0.0445\n",
      "Epoch 231/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0419\n",
      "Epoch 232/500\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 0.0409\n",
      "Epoch 233/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0471\n",
      "Epoch 234/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0377\n",
      "Epoch 235/500\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 0.0419\n",
      "Epoch 236/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0447\n",
      "Epoch 237/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0429\n",
      "Epoch 238/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0399\n",
      "Epoch 239/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0416\n",
      "Epoch 240/500\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 0.0404\n",
      "Epoch 241/500\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 0.0410\n",
      "Epoch 242/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0418\n",
      "Epoch 243/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0523\n",
      "Epoch 244/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0348\n",
      "Epoch 245/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0409\n",
      "Epoch 246/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0396\n",
      "Epoch 247/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0388\n",
      "Epoch 248/500\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 0.0396\n",
      "Epoch 249/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0446\n",
      "Epoch 250/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0353\n",
      "Epoch 251/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0400\n",
      "Epoch 252/500\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 0.0394\n",
      "Epoch 253/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0408\n",
      "Epoch 254/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0401\n",
      "Epoch 255/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0379\n",
      "Epoch 256/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0405\n",
      "Epoch 257/500\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 0.0396\n",
      "Epoch 258/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0379\n",
      "Epoch 259/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0389\n",
      "Epoch 260/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0349\n",
      "Epoch 261/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0369\n",
      "Epoch 262/500\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 0.0383\n",
      "Epoch 263/500\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 0.0389\n",
      "Epoch 264/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0360\n",
      "Epoch 265/500\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 0.3915\n",
      "Epoch 266/500\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 0.3664\n",
      "Epoch 267/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.1354\n",
      "Epoch 268/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0637\n",
      "Epoch 269/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0298\n",
      "Epoch 270/500\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 0.0365\n",
      "Epoch 271/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0411\n",
      "Epoch 272/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0385\n",
      "Epoch 273/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0383\n",
      "Epoch 274/500\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 0.0386\n",
      "Epoch 275/500\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 0.0367\n",
      "Epoch 276/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0397\n",
      "Epoch 277/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0367\n",
      "Epoch 278/500\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 0.0382\n",
      "Epoch 279/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0360\n",
      "Epoch 280/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0377\n",
      "Epoch 281/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0378\n",
      "Epoch 282/500\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 0.0340\n",
      "Epoch 283/500\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 0.0356\n",
      "Epoch 284/500\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 0.0346\n",
      "Epoch 285/500\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 0.0359\n",
      "Epoch 286/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0377\n",
      "Epoch 287/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0344\n",
      "Epoch 288/500\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 0.0361\n",
      "Epoch 289/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0338\n",
      "Epoch 290/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0334\n",
      "Epoch 291/500\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 0.0338\n",
      "Epoch 292/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0369\n",
      "Epoch 293/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0365\n",
      "Epoch 294/500\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 0.0330\n",
      "Epoch 295/500\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 0.0344\n",
      "Epoch 296/500\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 0.0330\n",
      "Epoch 297/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0347\n",
      "Epoch 298/500\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 0.0319\n",
      "Epoch 299/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0352\n",
      "Epoch 300/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0313\n",
      "Epoch 301/500\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 0.0328\n",
      "Epoch 302/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0350\n",
      "Epoch 303/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0331\n",
      "Epoch 304/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0346\n",
      "Epoch 305/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0323\n",
      "Epoch 306/500\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 0.0302\n",
      "Epoch 307/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0308\n",
      "Epoch 308/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0342\n",
      "Epoch 309/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0312\n",
      "Epoch 310/500\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 0.0323\n",
      "Epoch 311/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0322\n",
      "Epoch 312/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0327\n",
      "Epoch 313/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0415\n",
      "Epoch 314/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0335\n",
      "Epoch 315/500\n",
      "2688/2688 [==============================] - 30s 11ms/step - loss: 0.0264\n",
      "Epoch 316/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0322\n",
      "Epoch 317/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0306\n",
      "Epoch 318/500\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.0338\n",
      "Epoch 319/500\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.0326\n",
      "Epoch 320/500\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.0315\n",
      "Epoch 321/500\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.0340\n",
      "Epoch 322/500\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.0279\n",
      "Epoch 323/500\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.0314\n",
      "Epoch 324/500\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.0329\n",
      "Epoch 325/500\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.0310\n",
      "Epoch 326/500\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.0313\n",
      "Epoch 327/500\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.0296\n",
      "Epoch 328/500\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.0290\n",
      "Epoch 329/500\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.0318\n",
      "Epoch 330/500\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.0308\n",
      "Epoch 331/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0309\n",
      "Epoch 332/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0341\n",
      "Epoch 333/500\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.0423\n",
      "Epoch 334/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0261\n",
      "Epoch 335/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0268\n",
      "Epoch 336/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.0301\n",
      "Epoch 337/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0298\n",
      "Epoch 338/500\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.0313\n",
      "Epoch 339/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.0289\n",
      "Epoch 340/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.0289\n",
      "Epoch 341/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.0295\n",
      "Epoch 342/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0296\n",
      "Epoch 343/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0291\n",
      "Epoch 344/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0315\n",
      "Epoch 345/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0292\n",
      "Epoch 346/500\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.0285\n",
      "Epoch 347/500\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.0294\n",
      "Epoch 348/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0292\n",
      "Epoch 349/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.0297\n",
      "Epoch 350/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0315\n",
      "Epoch 351/500\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.0246\n",
      "Epoch 352/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0319\n",
      "Epoch 353/500\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.0261\n",
      "Epoch 354/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0321\n",
      "Epoch 355/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0283\n",
      "Epoch 356/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0293\n",
      "Epoch 357/500\n",
      "2688/2688 [==============================] - 34s 12ms/step - loss: 0.0284\n",
      "Epoch 358/500\n",
      "2688/2688 [==============================] - 34s 13ms/step - loss: 0.0293\n",
      "Epoch 359/500\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.0302\n",
      "Epoch 360/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0276\n",
      "Epoch 361/500\n",
      "2688/2688 [==============================] - 34s 12ms/step - loss: 0.0290\n",
      "Epoch 362/500\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.0279\n",
      "Epoch 363/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0273\n",
      "Epoch 364/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0294\n",
      "Epoch 365/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0272\n",
      "Epoch 366/500\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.0276\n",
      "Epoch 367/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0262\n",
      "Epoch 368/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.0272\n",
      "Epoch 369/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0270\n",
      "Epoch 370/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0285\n",
      "Epoch 371/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.0268\n",
      "Epoch 372/500\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.0251\n",
      "Epoch 373/500\n",
      "2688/2688 [==============================] - 34s 13ms/step - loss: 0.0294\n",
      "Epoch 374/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.0255\n",
      "Epoch 375/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.0287\n",
      "Epoch 376/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0263\n",
      "Epoch 377/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0279\n",
      "Epoch 378/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.0251\n",
      "Epoch 379/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0317\n",
      "Epoch 380/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0254\n",
      "Epoch 381/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0260\n",
      "Epoch 382/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0269\n",
      "Epoch 383/500\n",
      "2688/2688 [==============================] - 34s 13ms/step - loss: 0.0272\n",
      "Epoch 384/500\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.0244\n",
      "Epoch 385/500\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.0277\n",
      "Epoch 386/500\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.0276\n",
      "Epoch 387/500\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.0261\n",
      "Epoch 388/500\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.0249\n",
      "Epoch 389/500\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.0272\n",
      "Epoch 390/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0306\n",
      "Epoch 391/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0247\n",
      "Epoch 392/500\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.0269\n",
      "Epoch 393/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0245\n",
      "Epoch 394/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0302\n",
      "Epoch 395/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.0244\n",
      "Epoch 396/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0259\n",
      "Epoch 397/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0273\n",
      "Epoch 398/500\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.0243\n",
      "Epoch 399/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0258\n",
      "Epoch 400/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0265\n",
      "Epoch 401/500\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.0246\n",
      "Epoch 402/500\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.0257\n",
      "Epoch 403/500\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.0234\n",
      "Epoch 404/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0249\n",
      "Epoch 405/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.0262\n",
      "Epoch 406/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.0254\n",
      "Epoch 407/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.0257\n",
      "Epoch 408/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0243\n",
      "Epoch 409/500\n",
      "2688/2688 [==============================] - 33s 12ms/step - loss: 0.0267\n",
      "Epoch 410/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0242\n",
      "Epoch 411/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0238\n",
      "Epoch 412/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.0256\n",
      "Epoch 413/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.0238\n",
      "Epoch 414/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.0239\n",
      "Epoch 415/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0258\n",
      "Epoch 416/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0243\n",
      "Epoch 417/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0219\n",
      "Epoch 418/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0251\n",
      "Epoch 419/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.0231\n",
      "Epoch 420/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0257\n",
      "Epoch 421/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0267\n",
      "Epoch 422/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.0243\n",
      "Epoch 423/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0230\n",
      "Epoch 424/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.0227\n",
      "Epoch 425/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.0251\n",
      "Epoch 426/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0241\n",
      "Epoch 427/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0273\n",
      "Epoch 428/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.0213\n",
      "Epoch 429/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.0253\n",
      "Epoch 430/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.0230\n",
      "Epoch 431/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0244\n",
      "Epoch 432/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.0229\n",
      "Epoch 433/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0264\n",
      "Epoch 434/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0221\n",
      "Epoch 435/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0228\n",
      "Epoch 436/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0302\n",
      "Epoch 437/500\n",
      "2688/2688 [==============================] - 32s 12ms/step - loss: 0.0534\n",
      "Epoch 438/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.0445\n",
      "Epoch 439/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.0227\n",
      "Epoch 440/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.0192\n",
      "Epoch 441/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.0250\n",
      "Epoch 442/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.0228\n",
      "Epoch 443/500\n",
      "2688/2688 [==============================] - 31s 12ms/step - loss: 0.0230\n",
      "Epoch 444/500\n",
      "2688/2688 [==============================] - 31s 11ms/step - loss: 0.0244\n",
      "Epoch 445/500\n",
      "2688/2688 [==============================] - 23s 8ms/step - loss: 0.0242\n",
      "Epoch 446/500\n",
      "2688/2688 [==============================] - 22s 8ms/step - loss: 0.0254\n",
      "Epoch 447/500\n",
      "2688/2688 [==============================] - 22s 8ms/step - loss: 0.0241\n",
      "Epoch 448/500\n",
      "2688/2688 [==============================] - 22s 8ms/step - loss: 0.0241\n",
      "Epoch 449/500\n",
      "2688/2688 [==============================] - 22s 8ms/step - loss: 0.0746\n",
      "Epoch 450/500\n",
      "2688/2688 [==============================] - 22s 8ms/step - loss: 0.0173\n",
      "Epoch 451/500\n",
      "2688/2688 [==============================] - 22s 8ms/step - loss: 0.0198\n",
      "Epoch 452/500\n",
      "2688/2688 [==============================] - 22s 8ms/step - loss: 0.0243\n",
      "Epoch 453/500\n",
      "2688/2688 [==============================] - 22s 8ms/step - loss: 0.0230\n",
      "Epoch 454/500\n",
      "2688/2688 [==============================] - 22s 8ms/step - loss: 0.0239\n",
      "Epoch 455/500\n",
      "2688/2688 [==============================] - 22s 8ms/step - loss: 0.0243\n",
      "Epoch 456/500\n",
      "2688/2688 [==============================] - 22s 8ms/step - loss: 0.0217\n",
      "Epoch 457/500\n",
      "2688/2688 [==============================] - 22s 8ms/step - loss: 0.0234\n",
      "Epoch 458/500\n",
      "2688/2688 [==============================] - 23s 8ms/step - loss: 0.0296\n",
      "Epoch 459/500\n",
      "2688/2688 [==============================] - 23s 8ms/step - loss: 0.0223\n",
      "Epoch 460/500\n",
      "2688/2688 [==============================] - 22s 8ms/step - loss: 0.0236\n",
      "Epoch 461/500\n",
      "2688/2688 [==============================] - 23s 8ms/step - loss: 0.0225\n",
      "Epoch 462/500\n",
      "2688/2688 [==============================] - 23s 9ms/step - loss: 0.0238\n",
      "Epoch 463/500\n",
      "2688/2688 [==============================] - 23s 9ms/step - loss: 0.0228\n",
      "Epoch 464/500\n",
      "2688/2688 [==============================] - 22s 8ms/step - loss: 0.0233\n",
      "Epoch 465/500\n",
      "2688/2688 [==============================] - 23s 8ms/step - loss: 0.0234\n",
      "Epoch 466/500\n",
      "2688/2688 [==============================] - 22s 8ms/step - loss: 0.0220\n",
      "Epoch 467/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2688/2688 [==============================] - 23s 8ms/step - loss: 0.0231\n",
      "Epoch 468/500\n",
      "2688/2688 [==============================] - 23s 8ms/step - loss: 0.0245\n",
      "Epoch 469/500\n",
      "2688/2688 [==============================] - 23s 8ms/step - loss: 0.0236\n",
      "Epoch 470/500\n",
      "2688/2688 [==============================] - 23s 9ms/step - loss: 0.0210\n",
      "Epoch 471/500\n",
      "2688/2688 [==============================] - 26s 10ms/step - loss: 0.0221\n",
      "Epoch 472/500\n",
      "2688/2688 [==============================] - 23s 9ms/step - loss: 0.0245\n",
      "Epoch 473/500\n",
      "2688/2688 [==============================] - 23s 9ms/step - loss: 0.0219\n",
      "Epoch 474/500\n",
      "2688/2688 [==============================] - 23s 8ms/step - loss: 0.0218\n",
      "Epoch 475/500\n",
      "2688/2688 [==============================] - 23s 8ms/step - loss: 0.0215\n",
      "Epoch 476/500\n",
      "2688/2688 [==============================] - 23s 8ms/step - loss: 0.0225\n",
      "Epoch 477/500\n",
      "2688/2688 [==============================] - 22s 8ms/step - loss: 0.0218\n",
      "Epoch 478/500\n",
      "2688/2688 [==============================] - 22s 8ms/step - loss: 0.0222\n",
      "Epoch 479/500\n",
      "2688/2688 [==============================] - 22s 8ms/step - loss: 0.0223\n",
      "Epoch 480/500\n",
      "2688/2688 [==============================] - 23s 8ms/step - loss: 0.0215\n",
      "Epoch 481/500\n",
      "2688/2688 [==============================] - 23s 8ms/step - loss: 0.0236\n",
      "Epoch 482/500\n",
      "2688/2688 [==============================] - 23s 9ms/step - loss: 0.0209\n",
      "Epoch 483/500\n",
      "2688/2688 [==============================] - 22s 8ms/step - loss: 0.0241\n",
      "Epoch 484/500\n",
      "2688/2688 [==============================] - 23s 8ms/step - loss: 0.0207\n",
      "Epoch 485/500\n",
      "2688/2688 [==============================] - 23s 8ms/step - loss: 0.0214\n",
      "Epoch 486/500\n",
      "2688/2688 [==============================] - 22s 8ms/step - loss: 0.0221\n",
      "Epoch 487/500\n",
      "2688/2688 [==============================] - 23s 8ms/step - loss: 0.0214\n",
      "Epoch 488/500\n",
      "2688/2688 [==============================] - 22s 8ms/step - loss: 0.0225\n",
      "Epoch 489/500\n",
      "2688/2688 [==============================] - 22s 8ms/step - loss: 0.0217\n",
      "Epoch 490/500\n",
      "2688/2688 [==============================] - 22s 8ms/step - loss: 0.0392\n",
      "Epoch 491/500\n",
      "2688/2688 [==============================] - 22s 8ms/step - loss: 0.0174\n",
      "Epoch 492/500\n",
      "2688/2688 [==============================] - 23s 8ms/step - loss: 0.0198\n",
      "Epoch 493/500\n",
      "2688/2688 [==============================] - 23s 8ms/step - loss: 0.0243\n",
      "Epoch 494/500\n",
      "2688/2688 [==============================] - 23s 8ms/step - loss: 0.0199\n",
      "Epoch 495/500\n",
      "2688/2688 [==============================] - 24s 9ms/step - loss: 0.0218\n",
      "Epoch 496/500\n",
      "2688/2688 [==============================] - 24s 9ms/step - loss: 0.0252\n",
      "Epoch 497/500\n",
      "2688/2688 [==============================] - 24s 9ms/step - loss: 0.0190\n",
      "Epoch 498/500\n",
      "2688/2688 [==============================] - 23s 9ms/step - loss: 0.0198\n",
      "Epoch 499/500\n",
      "2688/2688 [==============================] - 23s 9ms/step - loss: 0.0227\n",
      "Epoch 500/500\n",
      "2688/2688 [==============================] - 23s 9ms/step - loss: 0.0242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n- epochs 467 - 0.1900\\n- epochs 100 해서 확인해보면 loss 가 너무 들쑥날쑥한 걸 확인할 수 있음 1.22\\n- epochs 500 0.2346\\n\\n- 데이터 셋 변경 epochs 500, loss - 0.0572, lr-0.001\\n  lr-0.0006 0.0547\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_autoencoder.fit_generator(train_generator(x_train), epochs=epochs, steps_per_epoch=steps_per_epoch, verbose=1)#, validation_data=val_generator(x_val))\n",
    "\n",
    "'''\n",
    "- epochs 467 - 0.1900\n",
    "- epochs 100 해서 확인해보면 loss 가 너무 들쑥날쑥한 걸 확인할 수 있음 1.22\n",
    "- epochs 500 0.2346\n",
    "\n",
    "- 데이터 셋 변경 epochs 500, loss - 0.0572, lr-0.001\n",
    "  lr-0.0006 0.0547\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lstm_autoencoder_250.fit_generator(train_generator(x_train), epochs=500, steps_per_epoch=steps_per_epoch, verbose=1)#, validation_data=val_generator(x_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model save\n",
    "lstm_autoencoder.save('model/lstm_autoencoder500_bfs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model('model/lstm_autoencoder500_bfs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, None, 3)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_17 (LSTM)                  (None, None, 128)    67584       input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_18 (LSTM)                  (None, 64)           49408       lstm_17[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, None, 64)     0           lstm_18[0][0]                    \n",
      "                                                                 input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_19 (LSTM)                  (None, None, 64)     33024       lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_20 (LSTM)                  (None, None, 128)    98816       lstm_19[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_5 (TimeDistrib (None, None, 3)      387         lstm_20[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 249,219\n",
      "Trainable params: 249,219\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10804047178671322\n"
     ]
    }
   ],
   "source": [
    "mean= 0\n",
    "for xt in x_test:\n",
    "    xt = xt.reshape(1, xt.shape[0], xt.shape[1])\n",
    "    out = loaded_model.predict(xt)\n",
    "    mean += ((xt-out)**2).mean(axis=None)\n",
    "print(mean/len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(loaded_model.input, loaded_model.layers[3].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Layer time_distributed_3 has multiple inbound nodes, hence the notion of \"layer output\" is ill-defined. Use `get_output_at(node_index)` instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-b399cc0be62b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdecoder_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m decoder = Model(loaded_model.layers[2].output, \n\u001b[0;32m----> 3\u001b[0;31m                         loaded_model.layers[6].output)\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36moutput\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inbound_nodes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m             raise AttributeError('Layer ' + self.name +\n\u001b[0;32m--> 807\u001b[0;31m                                  \u001b[0;34m' has multiple inbound nodes, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m                                  \u001b[0;34m'hence the notion of \"layer output\" '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m                                  \u001b[0;34m'is ill-defined. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Layer time_distributed_3 has multiple inbound nodes, hence the notion of \"layer output\" is ill-defined. Use `get_output_at(node_index)` instead."
     ]
    }
   ],
   "source": [
    "decoder_layer = loaded_model.layers[-1]\n",
    "decoder = Model(loaded_model.layers[2].output, \n",
    "                        loaded_model.layers[6].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = x_test[0].reshape(1, 9, 3)\n",
    "latent_vector = []\n",
    "for x in x_test:\n",
    "    x = x.reshape(1, x.shape[0], x.shape[1])\n",
    "    latent_vector.append(encoder.predict(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.0,\n",
       " -0.00020211579,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.74070954,\n",
       " -0.029288653,\n",
       " -0.74629986,\n",
       " 0.0,\n",
       " 0.12747759,\n",
       " -0.0014260766,\n",
       " -0.0,\n",
       " 0.10494814,\n",
       " 0.2187337,\n",
       " -0.18827741,\n",
       " 0.040405355,\n",
       " -0.5645764,\n",
       " 0.0022893408,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.2676836,\n",
       " -0.7395759,\n",
       " 0.2642008,\n",
       " -0.41065842,\n",
       " 0.19330677,\n",
       " -0.3758831,\n",
       " -0.21791713,\n",
       " 0.055199105,\n",
       " -0.25927162,\n",
       " 0.23109113,\n",
       " 0.8425839,\n",
       " -0.03407281,\n",
       " -0.8132615,\n",
       " 0.0,\n",
       " 0.49131706,\n",
       " 0.16162786,\n",
       " 0.35315764,\n",
       " -0.0,\n",
       " -0.7594612,\n",
       " -0.6364968,\n",
       " 0.0,\n",
       " 0.009271684,\n",
       " 0.0,\n",
       " 0.30094486,\n",
       " 0.0,\n",
       " -0.0027280087,\n",
       " -0.25117594,\n",
       " 0.0,\n",
       " 0.19940257,\n",
       " 0.32026434,\n",
       " 0.5843708,\n",
       " 0.032265846,\n",
       " -0.0,\n",
       " -0.0140618635,\n",
       " -0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.0,\n",
       " 0.254268,\n",
       " -0.0,\n",
       " 0.0,\n",
       " -0.0,\n",
       " 0.19169566,\n",
       " 0.0,\n",
       " 0.7144473]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_vector = np.array(latent_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printMatrix(matrix):\n",
    "    length = int(max(max(matrix[:, 0]), max(matrix[:, 1])))\n",
    "    adj = [[0 for i in range(length)] for i in range(length)]\n",
    "    \n",
    "    for m in matrix:\n",
    "        (i, j, w) = m\n",
    "        i = int(i) -1\n",
    "        j = int(j) -1\n",
    "        adj[i][j] = adj[j][i] = w\n",
    "    for i in range(length):\n",
    "        for j in range(length):\n",
    "            print(str(adj[i][j]), end=',')\n",
    "        print()\n",
    "    print(\"===============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 0\n",
      "0,10.16,7.92,9.56,0,0,0,0,0,\n",
      "10.16,0,0,0,0,0,0,0,0,\n",
      "7.92,0,0,0,8.17,18.05,0,0,0,\n",
      "9.56,0,0,0,0,0,20.05,0,0,\n",
      "0,0,8.17,0,0,0,0,0,0,\n",
      "0,0,18.05,0,0,0,0,32.09,0,\n",
      "0,0,0,20.05,0,0,0,0,20.27,\n",
      "0,0,0,0,0,32.09,0,0,9.01,\n",
      "0,0,0,0,0,0,20.27,9.01,0,\n",
      "===============\n",
      "# 1\n",
      "0,8.79,11.79,0,0,0,\n",
      "8.79,0,0,10.06,6.86,0,\n",
      "11.79,0,0,0,0,7.86,\n",
      "0,10.06,0,0,0,0,\n",
      "0,6.86,0,0,0,0,\n",
      "0,0,7.86,0,0,0,\n",
      "===============\n",
      "# 2\n",
      "0,3.3,6.3,0,0,\n",
      "3.3,0,0,12.72,0,\n",
      "6.3,0,0,0,23.87,\n",
      "0,12.72,0,0,41.95,\n",
      "0,0,23.87,41.95,0,\n",
      "===============\n",
      "# 3\n",
      "0,5.7,7.22,0,0,\n",
      "5.7,0,0,12.22,0,\n",
      "7.22,0,0,0,23.54,\n",
      "0,12.22,0,0,42.39,\n",
      "0,0,23.54,42.39,0,\n",
      "===============\n",
      "# 4\n",
      "0,12.52,17.52,28.77,0,0,0,0,0,\n",
      "12.52,0,0,0,0,0,0,0,0,\n",
      "17.52,0,0,0,2.53,4.53,0,0,0,\n",
      "28.77,0,0,0,0,0,10.86,0,0,\n",
      "0,0,2.53,0,0,0,0,0,0,\n",
      "0,0,4.53,0,0,0,0,9.01,0,\n",
      "0,0,0,10.86,0,0,0,0,10.01,\n",
      "0,0,0,0,0,9.01,0,0,20.01,\n",
      "0,0,0,0,0,0,10.01,20.01,0,\n",
      "===============\n",
      "# 5\n",
      "0,4.27,9.29,0,0,\n",
      "4.27,0,0,12.29,0,\n",
      "9.29,0,0,0,17.29,\n",
      "0,12.29,0,0,3.95,\n",
      "0,0,17.29,3.95,0,\n",
      "===============\n",
      "# 6\n",
      "0,10.16,7.92,9.56,0,0,0,0,0,\n",
      "10.16,0,0,0,0,0,0,0,0,\n",
      "7.92,0,0,0,8.17,18.05,0,0,0,\n",
      "9.56,0,0,0,0,0,20.05,0,0,\n",
      "0,0,8.17,0,0,0,0,0,0,\n",
      "0,0,18.05,0,0,0,0,32.09,0,\n",
      "0,0,0,20.05,0,0,0,0,20.27,\n",
      "0,0,0,0,0,32.09,0,0,9.01,\n",
      "0,0,0,0,0,0,20.27,9.01,0,\n",
      "===============\n",
      "# 7\n",
      "0,9.1,12.1,23.12,0,0,\n",
      "9.1,0,11.14,0,21.31,0,\n",
      "12.1,11.14,0,0,0,38.69,\n",
      "23.12,0,0,0,0,0,\n",
      "0,21.31,0,0,0,76.3,\n",
      "0,0,38.69,0,76.3,0,\n",
      "===============\n",
      "# 8\n",
      "0,5.68,4.49,0,0,\n",
      "5.68,0,0,10.64,0,\n",
      "4.49,0,0,0,20.73,\n",
      "0,10.64,0,0,23.73,\n",
      "0,0,20.73,23.73,0,\n",
      "===============\n",
      "# 9\n",
      "0,2.81,9.59,11.34,0,\n",
      "2.81,0,21.48,0,8.05,\n",
      "9.59,21.48,0,0,0,\n",
      "11.34,0,0,0,0,\n",
      "0,8.05,0,0,0,\n",
      "===============\n",
      "# 10\n",
      "0,5.11,6.66,8.4,\n",
      "5.11,0,18.0,0,\n",
      "6.66,18.0,0,0,\n",
      "8.4,0,0,0,\n",
      "===============\n",
      "# 11\n",
      "0,5.95,15.4,29.79,0,0,\n",
      "5.95,0,14.98,0,4.4,0,\n",
      "15.4,14.98,0,0,0,13.89,\n",
      "29.79,0,0,0,0,0,\n",
      "0,4.4,0,0,0,24.58,\n",
      "0,0,13.89,0,24.58,0,\n",
      "===============\n",
      "# 12\n",
      "0,6.41,4.09,0,0,0,\n",
      "6.41,0,0,8.09,16.24,0,\n",
      "4.09,0,0,0,0,4.33,\n",
      "0,8.09,0,0,0,0,\n",
      "0,16.24,0,0,0,0,\n",
      "0,0,4.33,0,0,0,\n",
      "===============\n",
      "# 13\n",
      "0,5.46,10.39,0,0,0,0,0,0,0,\n",
      "5.46,0,0,4.69,6.99,0,0,0,0,0,\n",
      "10.39,0,0,0,0,6.57,0,0,0,0,\n",
      "0,4.69,0,0,0,0,5.81,8.41,0,0,\n",
      "0,6.99,0,0,0,0,0,0,0,0,\n",
      "0,0,6.57,0,0,0,0,0,11.41,0,\n",
      "0,0,0,5.81,0,0,0,0,0,0,\n",
      "0,0,0,8.41,0,0,0,0,0,0,\n",
      "0,0,0,0,0,11.41,0,0,0,22.01,\n",
      "0,0,0,0,0,0,0,0,22.01,0,\n",
      "===============\n",
      "# 14\n",
      "0,7.91,9.61,0,0,0,0,\n",
      "7.91,0,0,19.41,7.36,0,0,\n",
      "9.61,0,0,0,0,15.61,11.39,\n",
      "0,19.41,0,0,0,0,0,\n",
      "0,7.36,0,0,0,0,0,\n",
      "0,0,15.61,0,0,0,0,\n",
      "0,0,11.39,0,0,0,0,\n",
      "===============\n",
      "# 15\n",
      "0,2.49,10.47,0,0,0,0,\n",
      "2.49,0,0,20.49,24.49,0,0,\n",
      "10.47,0,0,0,0,0,0,\n",
      "0,20.49,0,0,0,44.3,60.58,\n",
      "0,24.49,0,0,0,0,0,\n",
      "0,0,0,44.3,0,0,0,\n",
      "0,0,0,60.58,0,0,0,\n",
      "===============\n",
      "# 16\n",
      "0,9.84,14.84,0,0,0,0,0,0,0,\n",
      "9.84,0,0,28.34,52.46,0,0,0,0,0,\n",
      "14.84,0,0,0,0,20.74,0,0,0,0,\n",
      "0,28.34,0,0,0,0,1.8,8.07,0,0,\n",
      "0,52.46,0,0,0,0,0,0,0,0,\n",
      "0,0,20.74,0,0,0,0,0,10.07,0,\n",
      "0,0,0,1.8,0,0,0,0,0,0,\n",
      "0,0,0,8.07,0,0,0,0,0,0,\n",
      "0,0,0,0,0,10.07,0,0,0,20.12,\n",
      "0,0,0,0,0,0,0,0,20.12,0,\n",
      "===============\n",
      "# 17\n",
      "0,5.68,4.49,0,0,\n",
      "5.68,0,0,10.64,0,\n",
      "4.49,0,0,0,20.73,\n",
      "0,10.64,0,0,23.73,\n",
      "0,0,20.73,23.73,0,\n",
      "===============\n",
      "# 18\n",
      "0,7.44,9.44,7.01,0,0,\n",
      "7.44,0,9.01,0,9.95,0,\n",
      "9.44,9.01,0,0,0,6.37,\n",
      "7.01,0,0,0,0,0,\n",
      "0,9.95,0,0,0,5.54,\n",
      "0,0,6.37,0,5.54,0,\n",
      "===============\n",
      "# 19\n",
      "0,11.21,1.03,3.72,0,\n",
      "11.21,0,7.76,0,6.56,\n",
      "1.03,7.76,0,0,0,\n",
      "3.72,0,0,0,0,\n",
      "0,6.56,0,0,0,\n",
      "===============\n",
      "# 20\n",
      "0,4.71,7.93,0,0,0,0,0,0,0,\n",
      "4.71,0,0,16.93,3.43,0,0,0,0,0,\n",
      "7.93,0,0,0,0,9.66,0,0,0,0,\n",
      "0,16.93,0,0,0,0,8.09,18.05,0,0,\n",
      "0,3.43,0,0,0,0,0,0,0,0,\n",
      "0,0,9.66,0,0,0,0,0,30.86,0,\n",
      "0,0,0,8.09,0,0,0,0,0,0,\n",
      "0,0,0,18.05,0,0,0,0,0,0,\n",
      "0,0,0,0,0,30.86,0,0,0,31.86,\n",
      "0,0,0,0,0,0,0,0,31.86,0,\n",
      "===============\n",
      "# 21\n",
      "0,4.54,12.82,23.2,0,0,\n",
      "4.54,0,12.56,0,12.48,0,\n",
      "12.82,12.56,0,0,0,2.44,\n",
      "23.2,0,0,0,0,0,\n",
      "0,12.48,0,0,0,6.95,\n",
      "0,0,2.44,0,6.95,0,\n",
      "===============\n",
      "# 22\n",
      "0,9.7,5.1,5.12,0,0,0,0,0,\n",
      "9.7,0,0,0,0,0,0,0,0,\n",
      "5.1,0,0,0,13.29,10.61,0,0,0,\n",
      "5.12,0,0,0,0,0,20.94,0,0,\n",
      "0,0,13.29,0,0,0,0,0,0,\n",
      "0,0,10.61,0,0,0,0,8.35,0,\n",
      "0,0,0,20.94,0,0,0,0,6.14,\n",
      "0,0,0,0,0,8.35,0,0,14.26,\n",
      "0,0,0,0,0,0,6.14,14.26,0,\n",
      "===============\n",
      "# 23\n",
      "0,6.81,0,0,\n",
      "6.81,0,9.81,11.46,\n",
      "0,9.81,0,21.99,\n",
      "0,11.46,21.99,0,\n",
      "===============\n",
      "# 24\n",
      "0,8.12,16.82,0,0,\n",
      "8.12,0,0,3.04,0,\n",
      "16.82,0,0,0,9.05,\n",
      "0,3.04,0,0,6.2,\n",
      "0,0,9.05,6.2,0,\n",
      "===============\n",
      "# 25\n",
      "0,5.09,2.06,4.17,0,0,\n",
      "5.09,0,11.95,0,13.95,0,\n",
      "2.06,11.95,0,0,0,2.21,\n",
      "4.17,0,0,0,0,0,\n",
      "0,13.95,0,0,0,11.03,\n",
      "0,0,2.21,0,11.03,0,\n",
      "===============\n",
      "# 26\n",
      "0,5.46,10.39,0,0,0,0,0,0,0,\n",
      "5.46,0,0,4.69,6.99,0,0,0,0,0,\n",
      "10.39,0,0,0,0,6.57,0,0,0,0,\n",
      "0,4.69,0,0,0,0,5.81,8.41,0,0,\n",
      "0,6.99,0,0,0,0,0,0,0,0,\n",
      "0,0,6.57,0,0,0,0,0,11.41,0,\n",
      "0,0,0,5.81,0,0,0,0,0,0,\n",
      "0,0,0,8.41,0,0,0,0,0,0,\n",
      "0,0,0,0,0,11.41,0,0,0,22.01,\n",
      "0,0,0,0,0,0,0,0,22.01,0,\n",
      "===============\n",
      "# 27\n",
      "0,5.09,2.06,4.17,0,0,\n",
      "5.09,0,11.95,0,13.95,0,\n",
      "2.06,11.95,0,0,0,2.21,\n",
      "4.17,0,0,0,0,0,\n",
      "0,13.95,0,0,0,11.03,\n",
      "0,0,2.21,0,11.03,0,\n",
      "===============\n",
      "# 28\n",
      "0,4.16,7.16,0,0,0,0,\n",
      "4.16,0,0,16.83,28.23,0,0,\n",
      "7.16,0,0,0,0,33.23,62.71,\n",
      "0,16.83,0,0,0,0,0,\n",
      "0,28.23,0,0,0,0,0,\n",
      "0,0,33.23,0,0,0,0,\n",
      "0,0,62.71,0,0,0,0,\n",
      "===============\n",
      "# 29\n",
      "0,6.13,5.95,0,0,0,0,0,0,0,\n",
      "6.13,0,0,10.95,21.64,0,0,0,0,0,\n",
      "5.95,0,0,0,0,23.64,0,0,0,0,\n",
      "0,10.95,0,0,0,0,6.23,8.77,0,0,\n",
      "0,21.64,0,0,0,0,0,0,0,0,\n",
      "0,0,23.64,0,0,0,0,0,10.77,0,\n",
      "0,0,0,6.23,0,0,0,0,0,0,\n",
      "0,0,0,8.77,0,0,0,0,0,0,\n",
      "0,0,0,0,0,10.77,0,0,0,1.34,\n",
      "0,0,0,0,0,0,0,0,1.34,0,\n",
      "===============\n",
      "# 30\n",
      "0,10.37,8.48,6.98,0,0,0,0,0,\n",
      "10.37,0,0,0,0,0,0,0,0,\n",
      "8.48,0,0,0,12.67,22.75,0,0,0,\n",
      "6.98,0,0,0,0,0,27.75,0,0,\n",
      "0,0,12.67,0,0,0,0,0,0,\n",
      "0,0,22.75,0,0,0,0,2.0,0,\n",
      "0,0,0,27.75,0,0,0,0,7.78,\n",
      "0,0,0,0,0,2.0,0,0,15.79,\n",
      "0,0,0,0,0,0,7.78,15.79,0,\n",
      "===============\n",
      "# 31\n",
      "0,3.09,4.09,0,0,0,0,0,0,0,\n",
      "3.09,0,0,7.31,6.24,0,0,0,0,0,\n",
      "4.09,0,0,0,0,10.78,0,0,0,0,\n",
      "0,7.31,0,0,0,0,5.75,8.53,0,0,\n",
      "0,6.24,0,0,0,0,0,0,0,0,\n",
      "0,0,10.78,0,0,0,0,0,6.23,0,\n",
      "0,0,0,5.75,0,0,0,0,0,0,\n",
      "0,0,0,8.53,0,0,0,0,0,0,\n",
      "0,0,0,0,0,6.23,0,0,0,9.5,\n",
      "0,0,0,0,0,0,0,0,9.5,0,\n",
      "===============\n",
      "# 32\n",
      "0,7.38,6.51,0,0,0,0,0,0,0,\n",
      "7.38,0,0,9.96,10.96,0,0,0,0,0,\n",
      "6.51,0,0,0,0,21.85,0,0,0,0,\n",
      "0,9.96,0,0,0,0,38.35,27.88,0,0,\n",
      "0,10.96,0,0,0,0,0,0,0,0,\n",
      "0,0,21.85,0,0,0,0,0,37.89,0,\n",
      "0,0,0,38.35,0,0,0,0,0,0,\n",
      "0,0,0,27.88,0,0,0,0,0,0,\n",
      "0,0,0,0,0,37.89,0,0,0,39.89,\n",
      "0,0,0,0,0,0,0,0,39.89,0,\n",
      "===============\n",
      "# 33\n",
      "0,8.46,9.46,0,0,0,0,\n",
      "8.46,0,0,8.28,11.91,0,0,\n",
      "9.46,0,0,0,0,0,0,\n",
      "0,8.28,0,0,0,1.22,9.42,\n",
      "0,11.91,0,0,0,0,0,\n",
      "0,0,0,1.22,0,0,0,\n",
      "0,0,0,9.42,0,0,0,\n",
      "===============\n",
      "# 34\n",
      "0,4.35,3.05,12.89,0,\n",
      "4.35,0,17.89,0,28.52,\n",
      "3.05,17.89,0,0,0,\n",
      "12.89,0,0,0,0,\n",
      "0,28.52,0,0,0,\n",
      "===============\n",
      "# 35\n",
      "0,7.21,4.47,0,0,0,0,\n",
      "7.21,0,0,13.78,2.09,0,0,\n",
      "4.47,0,0,0,0,4.09,5.11,\n",
      "0,13.78,0,0,0,0,0,\n",
      "0,2.09,0,0,0,0,0,\n",
      "0,0,4.09,0,0,0,0,\n",
      "0,0,5.11,0,0,0,0,\n",
      "===============\n",
      "# 36\n",
      "0,8.54,12.54,0,0,0,0,\n",
      "8.54,0,0,1.84,9.02,0,0,\n",
      "12.54,0,0,0,0,7.16,12.16,\n",
      "0,1.84,0,0,0,0,0,\n",
      "0,9.02,0,0,0,0,0,\n",
      "0,0,7.16,0,0,0,0,\n",
      "0,0,12.16,0,0,0,0,\n",
      "===============\n",
      "# 37\n",
      "0,4.65,9.65,0,0,0,\n",
      "4.65,0,0,6.96,4.92,0,\n",
      "9.65,0,0,0,0,5.92,\n",
      "0,6.96,0,0,0,0,\n",
      "0,4.92,0,0,0,0,\n",
      "0,0,5.92,0,0,0,\n",
      "===============\n",
      "# 38\n",
      "0,8.6,8.71,18.02,0,0,0,0,0,\n",
      "8.6,0,0,0,0,0,0,0,0,\n",
      "8.71,0,0,0,1.3,9.61,0,0,0,\n",
      "18.02,0,0,0,0,0,19.5,0,0,\n",
      "0,0,1.3,0,0,0,0,0,0,\n",
      "0,0,9.61,0,0,0,0,6.15,0,\n",
      "0,0,0,19.5,0,0,0,0,4.59,\n",
      "0,0,0,0,0,6.15,0,0,5.8,\n",
      "0,0,0,0,0,0,4.59,5.8,0,\n",
      "===============\n",
      "# 39\n",
      "0,11.2,9.42,0,0,0,0,\n",
      "11.2,0,0,8.63,11.63,0,0,\n",
      "9.42,0,0,0,0,0,0,\n",
      "0,8.63,0,0,0,6.9,10.69,\n",
      "0,11.63,0,0,0,0,0,\n",
      "0,0,0,6.9,0,0,0,\n",
      "0,0,0,10.69,0,0,0,\n",
      "===============\n",
      "# 40\n",
      "0,11.32,22.47,7.57,0,0,\n",
      "11.32,0,15.33,0,5.29,0,\n",
      "22.47,15.33,0,0,0,5.21,\n",
      "7.57,0,0,0,0,0,\n",
      "0,5.29,0,0,0,13.0,\n",
      "0,0,5.21,0,13.0,0,\n",
      "===============\n",
      "# 41\n",
      "0,7.64,5.28,10.28,0,0,\n",
      "7.64,0,9.69,0,10.17,0,\n",
      "5.28,9.69,0,0,0,20.32,\n",
      "10.28,0,0,0,0,0,\n",
      "0,10.17,0,0,0,21.32,\n",
      "0,0,20.32,0,21.32,0,\n",
      "===============\n",
      "# 42\n",
      "0,2.73,7.31,10.0,0,0,0,0,0,\n",
      "2.73,0,0,0,0,0,0,0,0,\n",
      "7.31,0,0,0,6.1,11.5,0,0,0,\n",
      "10.0,0,0,0,0,0,1.37,0,0,\n",
      "0,0,6.1,0,0,0,0,0,0,\n",
      "0,0,11.5,0,0,0,0,10.51,0,\n",
      "0,0,0,1.37,0,0,0,0,9.33,\n",
      "0,0,0,0,0,10.51,0,0,11.07,\n",
      "0,0,0,0,0,0,9.33,11.07,0,\n",
      "===============\n",
      "# 43\n",
      "0,10.47,15.47,26.68,0,\n",
      "10.47,0,5.92,0,9.13,\n",
      "15.47,5.92,0,0,0,\n",
      "26.68,0,0,0,0,\n",
      "0,9.13,0,0,0,\n",
      "===============\n",
      "# 44\n",
      "0,5.13,3.73,0,0,0,0,\n",
      "5.13,0,0,12.26,14.26,0,0,\n",
      "3.73,0,0,0,0,6.24,14.5,\n",
      "0,12.26,0,0,0,0,0,\n",
      "0,14.26,0,0,0,0,0,\n",
      "0,0,6.24,0,0,0,0,\n",
      "0,0,14.5,0,0,0,0,\n",
      "===============\n",
      "# 45\n",
      "0,7.44,9.44,7.01,0,0,\n",
      "7.44,0,9.01,0,9.95,0,\n",
      "9.44,9.01,0,0,0,6.37,\n",
      "7.01,0,0,0,0,0,\n",
      "0,9.95,0,0,0,5.54,\n",
      "0,0,6.37,0,5.54,0,\n",
      "===============\n",
      "# 46\n",
      "0,3.57,9.11,0,0,0,0,\n",
      "3.57,0,0,8.53,12.29,0,0,\n",
      "9.11,0,0,0,0,0,0,\n",
      "0,8.53,0,0,0,2.17,9.34,\n",
      "0,12.29,0,0,0,0,0,\n",
      "0,0,0,2.17,0,0,0,\n",
      "0,0,0,9.34,0,0,0,\n",
      "===============\n",
      "# 47\n",
      "0,6.13,5.95,0,0,0,0,0,0,0,\n",
      "6.13,0,0,10.95,21.64,0,0,0,0,0,\n",
      "5.95,0,0,0,0,23.64,0,0,0,0,\n",
      "0,10.95,0,0,0,0,6.23,8.77,0,0,\n",
      "0,21.64,0,0,0,0,0,0,0,0,\n",
      "0,0,23.64,0,0,0,0,0,10.77,0,\n",
      "0,0,0,6.23,0,0,0,0,0,0,\n",
      "0,0,0,8.77,0,0,0,0,0,0,\n",
      "0,0,0,0,0,10.77,0,0,0,1.34,\n",
      "0,0,0,0,0,0,0,0,1.34,0,\n",
      "===============\n",
      "# 48\n",
      "0,6.67,11.67,0,0,0,0,\n",
      "6.67,0,0,22.32,39.93,0,0,\n",
      "11.67,0,0,0,0,0,0,\n",
      "0,22.32,0,0,0,41.93,3.97,\n",
      "0,39.93,0,0,0,0,0,\n",
      "0,0,0,41.93,0,0,0,\n",
      "0,0,0,3.97,0,0,0,\n",
      "===============\n",
      "# 49\n",
      "0,5.97,10.97,21.23,0,0,\n",
      "5.97,0,23.23,0,10.81,0,\n",
      "10.97,23.23,0,0,0,6.14,\n",
      "21.23,0,0,0,0,0,\n",
      "0,10.81,0,0,0,6.19,\n",
      "0,0,6.14,0,6.19,0,\n",
      "===============\n",
      "# 50\n",
      "0,5.87,4.42,0,0,\n",
      "5.87,0,0,11.89,0,\n",
      "4.42,0,0,0,1.67,\n",
      "0,11.89,0,0,6.67,\n",
      "0,0,1.67,6.67,0,\n",
      "===============\n",
      "# 51\n",
      "0,6.45,15.17,2.95,\n",
      "6.45,0,7.95,0,\n",
      "15.17,7.95,0,0,\n",
      "2.95,0,0,0,\n",
      "===============\n",
      "# 52\n",
      "0,3.71,10.29,0,0,0,\n",
      "3.71,0,0,10.61,15.61,0,\n",
      "10.29,0,0,0,0,27.3,\n",
      "0,10.61,0,0,0,0,\n",
      "0,15.61,0,0,0,0,\n",
      "0,0,27.3,0,0,0,\n",
      "===============\n",
      "# 53\n",
      "0,11.2,9.42,0,0,0,0,\n",
      "11.2,0,0,8.63,11.63,0,0,\n",
      "9.42,0,0,0,0,0,0,\n",
      "0,8.63,0,0,0,6.9,10.69,\n",
      "0,11.63,0,0,0,0,0,\n",
      "0,0,0,6.9,0,0,0,\n",
      "0,0,0,10.69,0,0,0,\n",
      "===============\n",
      "# 54\n",
      "0,10.37,8.48,6.98,0,0,0,0,0,\n",
      "10.37,0,0,0,0,0,0,0,0,\n",
      "8.48,0,0,0,12.67,22.75,0,0,0,\n",
      "6.98,0,0,0,0,0,27.75,0,0,\n",
      "0,0,12.67,0,0,0,0,0,0,\n",
      "0,0,22.75,0,0,0,0,2.0,0,\n",
      "0,0,0,27.75,0,0,0,0,7.78,\n",
      "0,0,0,0,0,2.0,0,0,15.79,\n",
      "0,0,0,0,0,0,7.78,15.79,0,\n",
      "===============\n",
      "# 55\n",
      "0,11.34,22.67,0,0,0,\n",
      "11.34,0,0,23.67,42.83,0,\n",
      "22.67,0,0,0,0,45.83,\n",
      "0,23.67,0,0,0,0,\n",
      "0,42.83,0,0,0,0,\n",
      "0,0,45.83,0,0,0,\n",
      "===============\n",
      "# 56\n",
      "0,6.82,0,0,\n",
      "6.82,0,16.66,18.66,\n",
      "0,16.66,0,5.64,\n",
      "0,18.66,5.64,0,\n",
      "===============\n",
      "# 57\n",
      "0,6.0,8.84,10.6,0,0,\n",
      "6.0,0,10.99,0,12.99,0,\n",
      "8.84,10.99,0,0,0,1.37,\n",
      "10.6,0,0,0,0,0,\n",
      "0,12.99,0,0,0,3.77,\n",
      "0,0,1.37,0,3.77,0,\n",
      "===============\n",
      "# 58\n",
      "0,8.17,17.68,0,0,0,0,0,0,0,\n",
      "8.17,0,0,28.82,14.87,0,0,0,0,0,\n",
      "17.68,0,0,0,0,17.87,0,0,0,0,\n",
      "0,28.82,0,0,0,0,34.08,64.24,0,0,\n",
      "0,14.87,0,0,0,0,0,0,0,0,\n",
      "0,0,17.87,0,0,0,0,0,52.76,0,\n",
      "0,0,0,34.08,0,0,0,0,0,0,\n",
      "0,0,0,64.24,0,0,0,0,0,0,\n",
      "0,0,0,0,0,52.76,0,0,0,74.48,\n",
      "0,0,0,0,0,0,0,0,74.48,0,\n",
      "===============\n",
      "# 59\n",
      "0,12.31,13.31,0,0,0,\n",
      "12.31,0,0,1.24,9.71,0,\n",
      "13.31,0,0,0,0,19.48,\n",
      "0,1.24,0,0,0,0,\n",
      "0,9.71,0,0,0,0,\n",
      "0,0,19.48,0,0,0,\n",
      "===============\n",
      "# 60\n",
      "0,6.92,7.92,4.57,\n",
      "6.92,0,8.77,0,\n",
      "7.92,8.77,0,0,\n",
      "4.57,0,0,0,\n",
      "===============\n",
      "# 61\n",
      "0,9.7,5.1,5.12,0,0,0,0,0,\n",
      "9.7,0,0,0,0,0,0,0,0,\n",
      "5.1,0,0,0,13.29,10.61,0,0,0,\n",
      "5.12,0,0,0,0,0,20.94,0,0,\n",
      "0,0,13.29,0,0,0,0,0,0,\n",
      "0,0,10.61,0,0,0,0,8.35,0,\n",
      "0,0,0,20.94,0,0,0,0,6.14,\n",
      "0,0,0,0,0,8.35,0,0,14.26,\n",
      "0,0,0,0,0,0,6.14,14.26,0,\n",
      "===============\n",
      "# 62\n",
      "0,8.12,16.82,0,0,\n",
      "8.12,0,0,3.04,0,\n",
      "16.82,0,0,0,9.05,\n",
      "0,3.04,0,0,6.2,\n",
      "0,0,9.05,6.2,0,\n",
      "===============\n",
      "# 63\n",
      "0,8.17,17.68,0,0,0,0,0,0,0,\n",
      "8.17,0,0,28.82,14.87,0,0,0,0,0,\n",
      "17.68,0,0,0,0,17.87,0,0,0,0,\n",
      "0,28.82,0,0,0,0,34.08,64.24,0,0,\n",
      "0,14.87,0,0,0,0,0,0,0,0,\n",
      "0,0,17.87,0,0,0,0,0,52.76,0,\n",
      "0,0,0,34.08,0,0,0,0,0,0,\n",
      "0,0,0,64.24,0,0,0,0,0,0,\n",
      "0,0,0,0,0,52.76,0,0,0,74.48,\n",
      "0,0,0,0,0,0,0,0,74.48,0,\n",
      "===============\n"
     ]
    }
   ],
   "source": [
    "# sequence to matrix\n",
    "for index, x in enumerate(x_test):\n",
    "    print(\"#\", str(index))\n",
    "    printMatrix(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard = latent_vector[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)  (2, 38)  (3, 53)  (4, 5)  (5, 15)  (6, 44)  (7, 48)  (8, 21)  (9, 12)  (10, 43)  (11, 9)  (12, 18)  (13, 31)  (14, 59)  (15, 58)  (16, 4)  (17, 41)  (18, 27)  (19, 29)  (20, 33)  (21, 6)  (22, 3)  (23, 51)  (24, 35)  (25, 54)  (26, 55)  (27, 26)  (28, 62)  (29, 14)  (30, 46)  (31, 11)  (32, 45)  (33, 7)  (34, 50)  (35, 57)  (36, 8)  (37, 63)  (38, 60)  (39, 24)  (40, 52)  (41, 20)  (42, 13)  (43, 37)  (44, 23)  (45, 10)  (46, 1)  (47, 61)  (48, 42)  (49, 56)  (50, 34)  (51, 16)  (52, 32)  (53, 40)  (54, 39)  (55, 19)  (56, 17)  (57, 22)  (58, 49)  (59, 30)  (60, 36)  (61, 47)  (62, 28)  (63, 25)  "
     ]
    }
   ],
   "source": [
    "import copy\n",
    "dist = []\n",
    "left = 100000\n",
    "right = -10000\n",
    "for index, v in enumerate(latent_vector[1:]):\n",
    "    d = round(euclidean_distance(standard, v), 8)\n",
    "    dist.append(d)\n",
    "    if d > right:\n",
    "        right= d\n",
    "    if left > d:\n",
    "        left = d\n",
    "dist1 = copy.deepcopy(dist)\n",
    "dist1.sort()\n",
    "#print(dist, dist1)\n",
    "for index, d in enumerate(dist1):\n",
    "    print( (index+1, dist.index(d)+1), end='  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(a, b):\n",
    "    return np.sqrt(np.sum(a-b) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

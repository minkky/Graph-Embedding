{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob, os\n",
    "from keras import layers as ly\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import LSTM, Dense, RepeatVector, TimeDistributed\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Lambda\n",
    "from keras import losses\n",
    "from keras.models import model_from_json\n",
    "import keras.backend.tensorflow_backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir = './sequence/*'\n",
    "dir = './datasets/seq/*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./datasets/seq/group1/*\n",
      "./datasets/seq/group2/*\n",
      "./datasets/seq/group3/*\n",
      "./datasets/seq/group4/*\n",
      "./datasets/seq/group5/*\n",
      "./datasets/seq/group6/*\n"
     ]
    }
   ],
   "source": [
    "# file read\n",
    "all_names = []\n",
    "all_data = []\n",
    "sequence_length = []\n",
    "alpha = list(string.ascii_uppercase)\n",
    "data_length = len(glob.glob(dir))\n",
    "file_predix = './datasets/seq/group'\n",
    "for index in range(1, data_length+1):\n",
    "    filename = file_predix + str(index) + \"/*\"\n",
    "    print(filename)\n",
    "    files = glob.glob(filename)\n",
    "    for file in files:\n",
    "        datasets = []\n",
    "        all_names.append(file.split('/')[-1].replace('.txt', ''))\n",
    "        for rf in open(file, 'r'):\n",
    "            (u, v, w) = rf[1:-2].split(', ')\n",
    "            datasets.append([alpha.index(u[1])+1, alpha.index(v[1]) +1, float(w)])\n",
    "        sequence_length.append(len(datasets))\n",
    "        all_data.append(datasets)\n",
    "all_data = np.array([np.array(arr) for arr in all_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, train_name, test_name = train_test_split(all_data, all_names, test_size=0.3)\n",
    "x_test, x_val, test_name, val_name = train_test_split(x_test, test_name, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_name\n",
    "tr_names= []\n",
    "for name in train_name:\n",
    "    tr_names.append(name.split('graph')[0])\n",
    "    #tr_names.append(name.split('-')[0].replace('graph', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = max(sequence_length)\n",
    "n_features = 3\n",
    "batch_size = 32\n",
    "epochs = 500\n",
    "steps_per_epoch = len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    loss1 = losses.mean_squared_error(y_true, y_pred)\n",
    "    loss2 = losses.categorical_crossentropy(y_true, y_pred)\n",
    "    return loss1 * 0.7 + loss2 * 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def repeat_vector(args):\n",
    "    layer_to_repeat = args[0]\n",
    "    sequence_layer = args[1]\n",
    "    return RepeatVector(K.shape(sequence_layer)[1])(layer_to_repeat)\n",
    "    \n",
    "\n",
    "inputs = Input(shape=(None, 3))\n",
    "encoded = LSTM(128, return_sequences=True)(inputs)  #activation 안적으면 tanh\n",
    "encoded = LSTM(64)(encoded)\n",
    "\n",
    "decoded = Lambda(repeat_vector, output_shape=(None, 64)) ([encoded, inputs]) # inputs의 shape[1] 만큼 encoded 를 반복 생성\n",
    "\n",
    "decoded = LSTM(64, return_sequences=True)(decoded)\n",
    "decoded = LSTM(128, return_sequences=True)(decoded)\n",
    "decoded = TimeDistributed(Dense(3))(decoded)\n",
    "encoder = Model(inputs, encoded)\n",
    "\n",
    "lstm_autoencoder = Model(inputs, decoded)\n",
    "lstm_autoencoder.compile(loss=custom_loss, optimizer='adam')#lr=1e-2, decay=0.9))\n",
    "#lstm_autoencoder_500 = lstm_autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_generator(x_val):\n",
    "    idx = 0\n",
    "    while True:\n",
    "        yield np.array([x_val[idx]]), np.array([x_val[idx]])\n",
    "        idx += 1\n",
    "        if idx >= len(x_val):\n",
    "            idx = 0\n",
    "\n",
    "def train_generator(x_train):\n",
    "    idx = 0\n",
    "    while True:\n",
    "        yield np.array([x_train[idx]]), np.array([x_train[idx]])\n",
    "        idx += 1\n",
    "        if idx >= len(x_train):\n",
    "            idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "4297/4297 [==============================] - 53s 12ms/step - loss: 266.0795 - val_loss: 137.4505\n",
      "Epoch 2/200\n",
      "4297/4297 [==============================] - 74s 17ms/step - loss: 146.4483 - val_loss: 125.8190\n",
      "Epoch 3/200\n",
      "4297/4297 [==============================] - 66s 15ms/step - loss: 128.9317 - val_loss: 106.1674\n",
      "Epoch 4/200\n",
      "4297/4297 [==============================] - 65s 15ms/step - loss: 103.1273 - val_loss: 84.7241\n",
      "Epoch 5/200\n",
      "4297/4297 [==============================] - 65s 15ms/step - loss: 90.5080 - val_loss: 71.9816\n",
      "Epoch 6/200\n",
      "4297/4297 [==============================] - 65s 15ms/step - loss: 72.4653 - val_loss: 63.7297\n",
      "Epoch 7/200\n",
      "4297/4297 [==============================] - 65s 15ms/step - loss: 60.7740 - val_loss: 49.9501\n",
      "Epoch 8/200\n",
      "4297/4297 [==============================] - 65s 15ms/step - loss: 55.7268 - val_loss: 45.0877\n",
      "Epoch 9/200\n",
      "4297/4297 [==============================] - 66s 15ms/step - loss: 44.8661 - val_loss: 38.9290\n",
      "Epoch 10/200\n",
      "4297/4297 [==============================] - 66s 15ms/step - loss: 40.6822 - val_loss: 36.2020\n",
      "Epoch 11/200\n",
      "4297/4297 [==============================] - 66s 15ms/step - loss: 36.8648 - val_loss: 34.4980\n",
      "Epoch 12/200\n",
      "4297/4297 [==============================] - 66s 15ms/step - loss: 35.7732 - val_loss: 31.1822\n",
      "Epoch 13/200\n",
      "4297/4297 [==============================] - 66s 15ms/step - loss: 33.6313 - val_loss: 32.7740\n",
      "Epoch 14/200\n",
      "4297/4297 [==============================] - 66s 15ms/step - loss: 31.7668 - val_loss: 29.1242\n",
      "Epoch 15/200\n",
      "4297/4297 [==============================] - 66s 15ms/step - loss: 30.9955 - val_loss: 26.6656\n",
      "Epoch 16/200\n",
      "4297/4297 [==============================] - 66s 15ms/step - loss: 29.2107 - val_loss: 26.3229\n",
      "Epoch 17/200\n",
      "4297/4297 [==============================] - 66s 15ms/step - loss: 28.2525 - val_loss: 27.9274\n",
      "Epoch 18/200\n",
      "4297/4297 [==============================] - 66s 15ms/step - loss: 25.8498 - val_loss: 26.3998\n",
      "Epoch 19/200\n",
      "4297/4297 [==============================] - 66s 15ms/step - loss: 25.7820 - val_loss: 23.5055\n",
      "Epoch 20/200\n",
      "4297/4297 [==============================] - 66s 15ms/step - loss: 24.7296 - val_loss: 21.8062\n",
      "Epoch 21/200\n",
      "4297/4297 [==============================] - 66s 15ms/step - loss: 23.8521 - val_loss: 22.1868\n",
      "Epoch 22/200\n",
      "4297/4297 [==============================] - 66s 15ms/step - loss: 23.5266 - val_loss: 20.3379\n",
      "Epoch 23/200\n",
      "4297/4297 [==============================] - 66s 15ms/step - loss: 21.8650 - val_loss: 25.8567\n",
      "Epoch 24/200\n",
      "4297/4297 [==============================] - 66s 15ms/step - loss: 21.4295 - val_loss: 19.2235\n",
      "Epoch 25/200\n",
      "4297/4297 [==============================] - 66s 15ms/step - loss: 20.3004 - val_loss: 19.4858\n",
      "Epoch 26/200\n",
      "4297/4297 [==============================] - 66s 15ms/step - loss: 20.7942 - val_loss: 18.6175\n",
      "Epoch 27/200\n",
      "4297/4297 [==============================] - 66s 15ms/step - loss: 19.0301 - val_loss: 18.5974\n",
      "Epoch 28/200\n",
      "4297/4297 [==============================] - 66s 15ms/step - loss: 19.8128 - val_loss: 18.2013\n",
      "Epoch 29/200\n",
      "4297/4297 [==============================] - 66s 15ms/step - loss: 18.8669 - val_loss: 18.4591\n",
      "Epoch 30/200\n",
      "4297/4297 [==============================] - 66s 15ms/step - loss: 17.7153 - val_loss: 16.9954\n",
      "Epoch 31/200\n",
      "4297/4297 [==============================] - 66s 15ms/step - loss: 17.7606 - val_loss: 16.3495\n",
      "Epoch 32/200\n",
      "4297/4297 [==============================] - 66s 15ms/step - loss: 17.8121 - val_loss: 16.8584\n",
      "Epoch 33/200\n",
      "4297/4297 [==============================] - 67s 16ms/step - loss: 17.1342 - val_loss: 18.7912\n",
      "Epoch 34/200\n",
      "4297/4297 [==============================] - 59s 14ms/step - loss: 17.6612 - val_loss: 19.6728\n",
      "Epoch 35/200\n",
      "4297/4297 [==============================] - 50s 12ms/step - loss: 16.8547 - val_loss: 18.1905\n",
      "Epoch 36/200\n",
      "4297/4297 [==============================] - 50s 12ms/step - loss: 18.0570 - val_loss: 15.8962\n",
      "Epoch 37/200\n",
      "4297/4297 [==============================] - 49s 11ms/step - loss: 17.0615 - val_loss: 18.6694\n",
      "Epoch 38/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 16.4351 - val_loss: 16.2761\n",
      "Epoch 39/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 16.6398 - val_loss: 16.4822\n",
      "Epoch 40/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 15.5151 - val_loss: 16.1205\n",
      "Epoch 41/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 16.8231 - val_loss: 16.9126\n",
      "Epoch 42/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 16.4482 - val_loss: 16.0194\n",
      "Epoch 43/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 15.6534 - val_loss: 15.4043\n",
      "Epoch 44/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 15.6773 - val_loss: 16.8860\n",
      "Epoch 45/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 15.2863 - val_loss: 15.7081\n",
      "Epoch 46/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 14.8699 - val_loss: 17.4708\n",
      "Epoch 47/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 14.4060 - val_loss: 15.0630\n",
      "Epoch 48/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 14.8659 - val_loss: 14.6388\n",
      "Epoch 49/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 14.3610 - val_loss: 16.2612\n",
      "Epoch 50/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 13.9282 - val_loss: 15.4028\n",
      "Epoch 51/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 14.1247 - val_loss: 15.5781\n",
      "Epoch 52/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 16.7002 - val_loss: 14.5872\n",
      "Epoch 53/200\n",
      "4297/4297 [==============================] - 50s 12ms/step - loss: 15.0979 - val_loss: 14.4195\n",
      "Epoch 54/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 14.4976 - val_loss: 14.5965\n",
      "Epoch 55/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 15.0838 - val_loss: 14.3018\n",
      "Epoch 56/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 14.5560 - val_loss: 17.5943\n",
      "Epoch 57/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 14.3156 - val_loss: 15.5713\n",
      "Epoch 58/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 13.6435 - val_loss: 14.7452\n",
      "Epoch 59/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 13.8734 - val_loss: 13.8087\n",
      "Epoch 60/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 13.8678 - val_loss: 15.0972\n",
      "Epoch 61/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 14.5882 - val_loss: 15.0113\n",
      "Epoch 62/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 13.4731 - val_loss: 14.7272\n",
      "Epoch 63/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 13.5591 - val_loss: 14.7431\n",
      "Epoch 64/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 13.2664 - val_loss: 15.0523\n",
      "Epoch 65/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 13.0925 - val_loss: 14.4740\n",
      "Epoch 66/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 12.8891 - val_loss: 15.5911\n",
      "Epoch 67/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 13.8174 - val_loss: 15.9916\n",
      "Epoch 68/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 13.3845 - val_loss: 13.9635\n",
      "Epoch 69/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 13.3891 - val_loss: 15.7482\n",
      "Epoch 70/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 13.1451 - val_loss: 13.7640\n",
      "Epoch 71/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 13.1748 - val_loss: 15.9188\n",
      "Epoch 72/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 12.3917 - val_loss: 14.4114\n",
      "Epoch 73/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 12.4508 - val_loss: 14.1840\n",
      "Epoch 74/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 12.6015 - val_loss: 14.6820\n",
      "Epoch 75/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 12.5868 - val_loss: 14.1134\n",
      "Epoch 76/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 12.8412 - val_loss: 14.6210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 12.4166 - val_loss: 14.8168\n",
      "Epoch 78/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 12.4660 - val_loss: 13.4425\n",
      "Epoch 79/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 14.0351 - val_loss: 13.9892\n",
      "Epoch 80/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 12.6187 - val_loss: 13.2128\n",
      "Epoch 81/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 12.2934 - val_loss: 13.5088\n",
      "Epoch 82/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 12.2934 - val_loss: 14.1087\n",
      "Epoch 83/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 12.5160 - val_loss: 13.6441\n",
      "Epoch 84/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 12.5670 - val_loss: 13.6754\n",
      "Epoch 85/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 12.0763 - val_loss: 12.9047\n",
      "Epoch 86/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 12.7076 - val_loss: 16.3105\n",
      "Epoch 87/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 11.7767 - val_loss: 14.3637\n",
      "Epoch 88/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 12.3106 - val_loss: 14.5936\n",
      "Epoch 89/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 12.3521 - val_loss: 13.0051\n",
      "Epoch 90/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 11.9909 - val_loss: 13.4821\n",
      "Epoch 91/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 11.9181 - val_loss: 13.4789\n",
      "Epoch 92/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 12.1638 - val_loss: 13.3420\n",
      "Epoch 93/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 12.1417 - val_loss: 12.9816\n",
      "Epoch 94/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 12.1346 - val_loss: 14.1009\n",
      "Epoch 95/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 11.8221 - val_loss: 14.2425\n",
      "Epoch 96/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 12.8852 - val_loss: 14.4715\n",
      "Epoch 97/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 11.9233 - val_loss: 13.3137\n",
      "Epoch 98/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 11.9396 - val_loss: 14.4102\n",
      "Epoch 99/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 12.3060 - val_loss: 13.2392\n",
      "Epoch 100/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 11.4656 - val_loss: 14.2220\n",
      "Epoch 101/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 11.5913 - val_loss: 14.2279\n",
      "Epoch 102/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 12.0013 - val_loss: 16.4897\n",
      "Epoch 103/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 11.7836 - val_loss: 12.8160\n",
      "Epoch 104/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 12.0336 - val_loss: 15.6509\n",
      "Epoch 105/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 11.5134 - val_loss: 13.0164\n",
      "Epoch 106/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 11.6894 - val_loss: 13.4467\n",
      "Epoch 107/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 11.6683 - val_loss: 16.9441\n",
      "Epoch 108/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 11.5360 - val_loss: 12.9794\n",
      "Epoch 109/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 12.4876 - val_loss: 13.9191\n",
      "Epoch 110/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 11.2442 - val_loss: 12.9730\n",
      "Epoch 111/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 11.7693 - val_loss: 13.1969\n",
      "Epoch 112/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 11.4190 - val_loss: 13.6957\n",
      "Epoch 113/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 11.2705 - val_loss: 13.6070\n",
      "Epoch 114/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 11.2694 - val_loss: 13.4469\n",
      "Epoch 115/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 12.3977 - val_loss: 15.8662\n",
      "Epoch 116/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 11.7665 - val_loss: 12.9097\n",
      "Epoch 117/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 11.0353 - val_loss: 12.7649\n",
      "Epoch 118/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 11.4171 - val_loss: 13.0328\n",
      "Epoch 119/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 11.2063 - val_loss: 12.8775\n",
      "Epoch 120/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 11.4442 - val_loss: 13.3114\n",
      "Epoch 121/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 11.4810 - val_loss: 13.6432\n",
      "Epoch 122/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 11.3252 - val_loss: 13.1330\n",
      "Epoch 123/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 12.1135 - val_loss: 14.0422\n",
      "Epoch 124/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 11.9264 - val_loss: 14.4520\n",
      "Epoch 125/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 12.3921 - val_loss: 13.0320\n",
      "Epoch 126/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 11.3763 - val_loss: 13.4456\n",
      "Epoch 127/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 11.6536 - val_loss: 14.3683\n",
      "Epoch 128/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 11.8737 - val_loss: 13.7215\n",
      "Epoch 129/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 11.0214 - val_loss: 13.3613\n",
      "Epoch 130/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 11.3924 - val_loss: 12.9722\n",
      "Epoch 131/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 11.0391 - val_loss: 13.0450\n",
      "Epoch 132/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 11.8565 - val_loss: 13.1771\n",
      "Epoch 133/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 11.1258 - val_loss: 13.8401\n",
      "Epoch 134/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 11.8760 - val_loss: 12.7552\n",
      "Epoch 135/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 11.4986 - val_loss: 12.5466\n",
      "Epoch 136/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 11.5377 - val_loss: 13.0772\n",
      "Epoch 137/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 11.8690 - val_loss: 13.7319\n",
      "Epoch 138/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 11.1621 - val_loss: 12.8797\n",
      "Epoch 139/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 12.1415 - val_loss: 13.8444\n",
      "Epoch 140/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 11.1061 - val_loss: 15.9190\n",
      "Epoch 141/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 11.2560 - val_loss: 13.1646\n",
      "Epoch 142/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 11.4053 - val_loss: 12.4363\n",
      "Epoch 143/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 10.6843 - val_loss: 13.2800\n",
      "Epoch 144/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 11.3916 - val_loss: 11.9343\n",
      "Epoch 145/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 10.7095 - val_loss: 12.5962\n",
      "Epoch 146/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 11.1955 - val_loss: 12.8124\n",
      "Epoch 147/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 11.1641 - val_loss: 12.3309\n",
      "Epoch 148/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 10.7959 - val_loss: 13.7996\n",
      "Epoch 149/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 11.4796 - val_loss: 14.1810\n",
      "Epoch 150/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 11.2905 - val_loss: 13.2887\n",
      "Epoch 151/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 11.2326 - val_loss: 13.5619\n",
      "Epoch 152/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4297/4297 [==============================] - 47s 11ms/step - loss: 10.8928 - val_loss: 13.4894\n",
      "Epoch 153/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 11.1345 - val_loss: 14.5736\n",
      "Epoch 154/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 10.9850 - val_loss: 13.1713\n",
      "Epoch 155/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 10.9420 - val_loss: 12.8553\n",
      "Epoch 156/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 10.7290 - val_loss: 13.9231\n",
      "Epoch 157/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 10.9419 - val_loss: 12.7377\n",
      "Epoch 158/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 10.7534 - val_loss: 13.1787\n",
      "Epoch 159/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 10.7084 - val_loss: 14.3604\n",
      "Epoch 160/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 11.1297 - val_loss: 13.4566\n",
      "Epoch 161/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 10.5292 - val_loss: 12.9581\n",
      "Epoch 162/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 10.9243 - val_loss: 13.0360\n",
      "Epoch 163/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 10.6036 - val_loss: 12.6770\n",
      "Epoch 164/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 10.5930 - val_loss: 13.3200\n",
      "Epoch 165/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 10.6515 - val_loss: 12.9082\n",
      "Epoch 166/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 11.2199 - val_loss: 13.4058\n",
      "Epoch 167/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 11.3210 - val_loss: 12.6947\n",
      "Epoch 168/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 10.5179 - val_loss: 13.5355\n",
      "Epoch 169/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 10.7810 - val_loss: 13.1142\n",
      "Epoch 170/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 10.4937 - val_loss: 12.9265\n",
      "Epoch 171/200\n",
      "4297/4297 [==============================] - 47s 11ms/step - loss: 11.2231 - val_loss: 13.1419\n",
      "Epoch 172/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 10.2160 - val_loss: 13.0358\n",
      "Epoch 173/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 10.4026 - val_loss: 12.9879\n",
      "Epoch 174/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 10.9762 - val_loss: 14.1997\n",
      "Epoch 175/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 11.0787 - val_loss: 13.6510\n",
      "Epoch 176/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 10.8092 - val_loss: 13.8136\n",
      "Epoch 177/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 10.3176 - val_loss: 13.0792\n",
      "Epoch 178/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 10.6998 - val_loss: 13.0195\n",
      "Epoch 179/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 10.4697 - val_loss: 12.7702\n",
      "Epoch 180/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 10.2625 - val_loss: 13.6880\n",
      "Epoch 181/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 10.6386 - val_loss: 13.2640\n",
      "Epoch 182/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 10.4706 - val_loss: 12.9045\n",
      "Epoch 183/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 10.5098 - val_loss: 13.6442\n",
      "Epoch 184/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 10.8732 - val_loss: 13.6269\n",
      "Epoch 185/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 10.6430 - val_loss: 13.2395\n",
      "Epoch 186/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 10.4680 - val_loss: 13.3956\n",
      "Epoch 187/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 10.9721 - val_loss: 13.2136\n",
      "Epoch 188/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 10.7768 - val_loss: 13.4518\n",
      "Epoch 189/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 10.3064 - val_loss: 13.0973\n",
      "Epoch 190/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 10.4434 - val_loss: 12.9545\n",
      "Epoch 191/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 10.5799 - val_loss: 12.8016\n",
      "Epoch 192/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 10.7291 - val_loss: 12.9646\n",
      "Epoch 193/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 10.6376 - val_loss: 16.9597\n",
      "Epoch 194/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 10.6958 - val_loss: 13.9443\n",
      "Epoch 195/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 10.3149 - val_loss: 12.6335\n",
      "Epoch 196/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 10.5721 - val_loss: 13.7499\n",
      "Epoch 197/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 10.2945 - val_loss: 12.7836\n",
      "Epoch 198/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 10.9811 - val_loss: 12.9295\n",
      "Epoch 199/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 10.3433 - val_loss: 12.2560\n",
      "Epoch 200/200\n",
      "4297/4297 [==============================] - 48s 11ms/step - loss: 10.5619 - val_loss: 13.4189\n"
     ]
    }
   ],
   "source": [
    "hist = lstm_autoencoder.fit_generator(train_generator(x_train), epochs=200, steps_per_epoch=steps_per_epoch, verbose=1, validation_steps=len(x_val), validation_data=val_generator(x_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "model_json = lstm_autoencoder.to_json()\n",
    "filename = 'cce_model'  #input('filename: ') #\n",
    "with open('models/' + filename + '.json', 'w') as file:\n",
    "    file.write(model_json)\n",
    "lstm_autoencoder.save_weights('models/weights_' +  filename + '.h5')\n",
    "\n",
    "with open('models/cce_history.json', 'w') as f:\n",
    "    json.dump(hist.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "loss_ax.plot(hist.history['val_loss'], 'b', label='val loss')\n",
    "loss_ax.plot(hist.history['loss'], 'r', label='train loss')\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "loss_ax.legend(loc='upper left')\n",
    "\n",
    "plt.title('cce')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = lstm_autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36404565399193833\n"
     ]
    }
   ],
   "source": [
    "mean= 0\n",
    "for xt in x_test:\n",
    "    xt = xt.reshape(1, xt.shape[0], xt.shape[1])\n",
    "    out = loaded_model.predict(xt)\n",
    "    mean += ((xt-out)**2).mean(axis=None)\n",
    "print(mean/len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"last_mse_lstmae_cce\"\n",
    "loaded_model = model_from_json(open('res/cce/' +filename + '.json').read())\n",
    "loaded_model.load_weights('model_save/mse_cce_models/weights_' + filename + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(loaded_model.input, loaded_model.layers[3].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = x_test[0].reshape(1, x_test[0].shape[0], x_test[0].shape[1])\n",
    "latent_vector = []\n",
    "for x in x_test:\n",
    "    x = x.reshape(1, x.shape[0], x.shape[1])\n",
    "    latent_vector.append(encoder.predict(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'model_save/mse_cce_models/weights' + '{epoch:02d}-{loss:.4f}.h5'\n",
    "early_stopping_callback = EarlyStopping(monitor='loss', patience=200)\n",
    "checkpoint_callback = ModelCheckpoint(model_path, monitor='loss', verbose = 1, save_best_only = True, save_weights_only = True, mode='min')#, period=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob, os\n",
    "from keras import layers as ly\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import LSTM, Dense, RepeatVector, TimeDistributed\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Lambda\n",
    "from keras import losses\n",
    "from keras.models import model_from_json\n",
    "import keras.backend.tensorflow_backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir = './sequence/*'\n",
    "dir = './datasets/seq/*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./datasets/seq/group1/*\n",
      "./datasets/seq/group2/*\n",
      "./datasets/seq/group3/*\n",
      "./datasets/seq/group4/*\n",
      "./datasets/seq/group5/*\n",
      "./datasets/seq/group6/*\n",
      "./datasets/seq/group7/*\n",
      "./datasets/seq/group8/*\n",
      "./datasets/seq/group9/*\n",
      "./datasets/seq/group10/*\n",
      "./datasets/seq/group11/*\n",
      "./datasets/seq/group12/*\n",
      "./datasets/seq/group13/*\n",
      "./datasets/seq/group14/*\n",
      "./datasets/seq/group15/*\n",
      "./datasets/seq/group16/*\n"
     ]
    }
   ],
   "source": [
    "# file read\n",
    "all_names = []\n",
    "all_data = []\n",
    "sequence_length = []\n",
    "alpha = list(string.ascii_uppercase)\n",
    "data_length = len(glob.glob(dir))\n",
    "file_predix = './datasets/seq/group'\n",
    "for index in range(1, data_length+1):\n",
    "    filename = file_predix + str(index) + \"/*\"\n",
    "    print(filename)\n",
    "    files = glob.glob(filename)\n",
    "    for file in files:\n",
    "        datasets = []\n",
    "        all_names.append(file.split('/')[-1].replace('.txt', ''))\n",
    "        for rf in open(file, 'r'):\n",
    "            (u, v, w) = rf[1:-2].split(', ')\n",
    "            datasets.append([alpha.index(u[1])+1, alpha.index(v[1]) +1, float(w)])\n",
    "        sequence_length.append(len(datasets))\n",
    "        all_data.append(datasets)\n",
    "all_data = np.array([np.array(arr) for arr in all_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, train_name, test_name = train_test_split(all_data, all_names, test_size=0.3)\n",
    "x_test, x_val, test_name, val_name = train_test_split(x_test, test_name, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_name\n",
    "tr_names= []\n",
    "for name in train_name:\n",
    "    tr_names.append(name.split('graph')[0])\n",
    "    #tr_names.append(name.split('-')[0].replace('graph', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = max(sequence_length)\n",
    "n_features = 3\n",
    "batch_size = 32\n",
    "epochs = 500\n",
    "steps_per_epoch = len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    loss1 = losses.mean_squared_error(y_true, y_pred)\n",
    "    loss2 = losses.categorical_crossentropy(y_true, y_pred)\n",
    "    return loss1 * 0.7 + loss2 * 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/cs405a/anaconda3/envs/graph/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/cs405a/anaconda3/envs/graph/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/cs405a/anaconda3/envs/graph/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/cs405a/anaconda3/envs/graph/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/cs405a/anaconda3/envs/graph/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def repeat_vector(args):\n",
    "    layer_to_repeat = args[0]\n",
    "    sequence_layer = args[1]\n",
    "    return RepeatVector(K.shape(sequence_layer)[1])(layer_to_repeat)\n",
    "    \n",
    "\n",
    "inputs = Input(shape=(None, 3))\n",
    "encoded = LSTM(128, return_sequences=True)(inputs)  #activation 안적으면 tanh\n",
    "encoded = LSTM(64)(encoded)\n",
    "\n",
    "decoded = Lambda(repeat_vector, output_shape=(None, 64)) ([encoded, inputs]) # inputs의 shape[1] 만큼 encoded 를 반복 생성\n",
    "\n",
    "decoded = LSTM(64, return_sequences=True)(decoded)\n",
    "decoded = LSTM(128, return_sequences=True)(decoded)\n",
    "decoded = TimeDistributed(Dense(3))(decoded)\n",
    "encoder = Model(inputs, encoded)\n",
    "\n",
    "lstm_autoencoder = Model(inputs, decoded)\n",
    "lstm_autoencoder.compile(loss=custom_loss, optimizer='adam')#lr=1e-2, decay=0.9))\n",
    "#lstm_autoencoder_500 = lstm_autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_generator(x_val):\n",
    "    idx = 0\n",
    "    while True:\n",
    "        yield np.array([x_val[idx]]), np.array([x_val[idx]])\n",
    "        idx += 1\n",
    "        if idx >= len(x_val):\n",
    "            idx = 0\n",
    "\n",
    "def train_generator(x_train):\n",
    "    idx = 0\n",
    "    while True:\n",
    "        yield np.array([x_train[idx]]), np.array([x_train[idx]])\n",
    "        idx += 1\n",
    "        if idx >= len(x_train):\n",
    "            idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/cs405a/anaconda3/envs/graph/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/cs405a/anaconda3/envs/graph/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Epoch 1/200\n",
      "5770/5770 [==============================] - 83s 14ms/step - loss: 145.0791 - val_loss: 109.1718\n",
      "Epoch 2/200\n",
      "5770/5770 [==============================] - 89s 15ms/step - loss: 106.6896 - val_loss: 102.6890\n",
      "Epoch 3/200\n",
      "5770/5770 [==============================] - 89s 15ms/step - loss: 102.7500 - val_loss: 103.0718\n",
      "Epoch 4/200\n",
      "5770/5770 [==============================] - 89s 15ms/step - loss: 96.7037 - val_loss: 89.3877\n",
      "Epoch 5/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 71.7778 - val_loss: 48.6010\n",
      "Epoch 6/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 45.2307 - val_loss: 37.7671\n",
      "Epoch 7/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 37.6227 - val_loss: 31.0102\n",
      "Epoch 8/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 32.6279 - val_loss: 29.4200\n",
      "Epoch 9/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 28.4190 - val_loss: 35.8116\n",
      "Epoch 10/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 26.5539 - val_loss: 22.6947\n",
      "Epoch 11/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 23.7366 - val_loss: 21.8917\n",
      "Epoch 12/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 21.5065 - val_loss: 20.1415\n",
      "Epoch 13/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 20.8634 - val_loss: 21.0717\n",
      "Epoch 14/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 19.4576 - val_loss: 20.1355\n",
      "Epoch 15/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 18.7559 - val_loss: 18.1348\n",
      "Epoch 16/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 19.0671 - val_loss: 18.3301\n",
      "Epoch 17/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 18.1221 - val_loss: 19.0702\n",
      "Epoch 18/200\n",
      "5770/5770 [==============================] - 91s 16ms/step - loss: 18.2727 - val_loss: 18.2637\n",
      "Epoch 19/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 16.3797 - val_loss: 17.6012\n",
      "Epoch 20/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 16.3115 - val_loss: 19.4570\n",
      "Epoch 21/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 16.9177 - val_loss: 16.6882\n",
      "Epoch 22/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 16.2217 - val_loss: 16.0684\n",
      "Epoch 23/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 15.6240 - val_loss: 26.7388\n",
      "Epoch 24/200\n",
      "5770/5770 [==============================] - 75s 13ms/step - loss: 16.0676 - val_loss: 18.8690\n",
      "Epoch 25/200\n",
      "5770/5770 [==============================] - 72s 12ms/step - loss: 14.9534 - val_loss: 14.2007\n",
      "Epoch 26/200\n",
      "5770/5770 [==============================] - 89s 15ms/step - loss: 14.9373 - val_loss: 15.5547\n",
      "Epoch 27/200\n",
      "5770/5770 [==============================] - 89s 15ms/step - loss: 14.7513 - val_loss: 14.4066\n",
      "Epoch 28/200\n",
      "5770/5770 [==============================] - 89s 15ms/step - loss: 15.3880 - val_loss: 15.5865\n",
      "Epoch 29/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 14.4150 - val_loss: 15.1756\n",
      "Epoch 30/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 14.0814 - val_loss: 14.4891\n",
      "Epoch 31/200\n",
      "5770/5770 [==============================] - 89s 15ms/step - loss: 15.2221 - val_loss: 19.2782\n",
      "Epoch 32/200\n",
      "5770/5770 [==============================] - 89s 16ms/step - loss: 14.4086 - val_loss: 15.7075\n",
      "Epoch 33/200\n",
      "5770/5770 [==============================] - 91s 16ms/step - loss: 15.2477 - val_loss: 13.8057\n",
      "Epoch 34/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 13.4234 - val_loss: 15.3941\n",
      "Epoch 35/200\n",
      "5770/5770 [==============================] - 91s 16ms/step - loss: 14.2691 - val_loss: 14.0830\n",
      "Epoch 36/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 14.1512 - val_loss: 14.1826\n",
      "Epoch 37/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 13.5878 - val_loss: 12.4311\n",
      "Epoch 38/200\n",
      "5770/5770 [==============================] - 91s 16ms/step - loss: 13.0946 - val_loss: 12.8712\n",
      "Epoch 39/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 12.6889 - val_loss: 12.5051\n",
      "Epoch 40/200\n",
      "5770/5770 [==============================] - 91s 16ms/step - loss: 13.1141 - val_loss: 13.6959\n",
      "Epoch 41/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 13.1996 - val_loss: 13.1005\n",
      "Epoch 42/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 12.4499 - val_loss: 14.6208\n",
      "Epoch 43/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 12.6167 - val_loss: 12.4977\n",
      "Epoch 44/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 13.0488 - val_loss: 11.7041\n",
      "Epoch 45/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 12.4260 - val_loss: 14.3192\n",
      "Epoch 46/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 12.2583 - val_loss: 12.6522\n",
      "Epoch 47/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 12.3826 - val_loss: 12.0998\n",
      "Epoch 48/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 12.0179 - val_loss: 13.4494\n",
      "Epoch 49/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 13.1787 - val_loss: 13.1345\n",
      "Epoch 50/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 12.6146 - val_loss: 13.1548\n",
      "Epoch 51/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 12.8826 - val_loss: 11.9846\n",
      "Epoch 52/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 12.2361 - val_loss: 15.0975\n",
      "Epoch 53/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 11.5809 - val_loss: 11.7153\n",
      "Epoch 54/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 12.1095 - val_loss: 11.7106\n",
      "Epoch 55/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 11.7009 - val_loss: 12.1649\n",
      "Epoch 56/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 11.6533 - val_loss: 15.2042\n",
      "Epoch 57/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 11.2026 - val_loss: 11.4736\n",
      "Epoch 58/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 11.5665 - val_loss: 11.6431\n",
      "Epoch 59/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 11.8748 - val_loss: 12.1025\n",
      "Epoch 60/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 12.0506 - val_loss: 13.8804\n",
      "Epoch 61/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 11.1708 - val_loss: 11.7421\n",
      "Epoch 62/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 11.6451 - val_loss: 13.0147\n",
      "Epoch 63/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 11.7844 - val_loss: 11.2655\n",
      "Epoch 64/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 11.9145 - val_loss: 11.2554\n",
      "Epoch 65/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 11.3359 - val_loss: 10.9494\n",
      "Epoch 66/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 12.5618 - val_loss: 12.5042\n",
      "Epoch 67/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 11.7506 - val_loss: 12.9412\n",
      "Epoch 68/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 11.3913 - val_loss: 12.4138\n",
      "Epoch 69/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 11.0940 - val_loss: 12.0205\n",
      "Epoch 70/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 11.8199 - val_loss: 11.6458\n",
      "Epoch 71/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5770/5770 [==============================] - 88s 15ms/step - loss: 11.6602 - val_loss: 11.8535\n",
      "Epoch 72/200\n",
      "5770/5770 [==============================] - 88s 15ms/step - loss: 11.6922 - val_loss: 12.8382\n",
      "Epoch 73/200\n",
      "5770/5770 [==============================] - 88s 15ms/step - loss: 12.0119 - val_loss: 11.4964\n",
      "Epoch 74/200\n",
      "5770/5770 [==============================] - 88s 15ms/step - loss: 11.0466 - val_loss: 11.1959\n",
      "Epoch 75/200\n",
      "5770/5770 [==============================] - 89s 15ms/step - loss: 11.3095 - val_loss: 13.4963\n",
      "Epoch 76/200\n",
      "5770/5770 [==============================] - 88s 15ms/step - loss: 11.2822 - val_loss: 11.0364\n",
      "Epoch 77/200\n",
      "5770/5770 [==============================] - 89s 15ms/step - loss: 11.3713 - val_loss: 11.4914\n",
      "Epoch 78/200\n",
      "5770/5770 [==============================] - 88s 15ms/step - loss: 10.6888 - val_loss: 12.4241\n",
      "Epoch 79/200\n",
      "5770/5770 [==============================] - 89s 15ms/step - loss: 12.6205 - val_loss: 12.6518\n",
      "Epoch 80/200\n",
      "5770/5770 [==============================] - 89s 15ms/step - loss: 11.6727 - val_loss: 12.1073\n",
      "Epoch 81/200\n",
      "5770/5770 [==============================] - 88s 15ms/step - loss: 11.3421 - val_loss: 13.0572\n",
      "Epoch 82/200\n",
      "5770/5770 [==============================] - 88s 15ms/step - loss: 11.5934 - val_loss: 14.2932\n",
      "Epoch 83/200\n",
      "5770/5770 [==============================] - 88s 15ms/step - loss: 12.2241 - val_loss: 11.5366\n",
      "Epoch 84/200\n",
      "5770/5770 [==============================] - 89s 15ms/step - loss: 10.8230 - val_loss: 11.4452\n",
      "Epoch 85/200\n",
      "5770/5770 [==============================] - 89s 15ms/step - loss: 10.8390 - val_loss: 11.4562\n",
      "Epoch 86/200\n",
      "5770/5770 [==============================] - 88s 15ms/step - loss: 11.5872 - val_loss: 12.9015\n",
      "Epoch 87/200\n",
      "5770/5770 [==============================] - 88s 15ms/step - loss: 10.6369 - val_loss: 11.3817\n",
      "Epoch 88/200\n",
      "5770/5770 [==============================] - 89s 15ms/step - loss: 10.9862 - val_loss: 12.3468\n",
      "Epoch 89/200\n",
      "5770/5770 [==============================] - 89s 15ms/step - loss: 10.8422 - val_loss: 14.5386\n",
      "Epoch 90/200\n",
      "5770/5770 [==============================] - 89s 15ms/step - loss: 11.0076 - val_loss: 10.9349\n",
      "Epoch 91/200\n",
      "5770/5770 [==============================] - 89s 15ms/step - loss: 10.4984 - val_loss: 11.2009\n",
      "Epoch 92/200\n",
      "5770/5770 [==============================] - 89s 15ms/step - loss: 11.1285 - val_loss: 11.0417\n",
      "Epoch 93/200\n",
      "5770/5770 [==============================] - 89s 15ms/step - loss: 11.2779 - val_loss: 14.5042\n",
      "Epoch 94/200\n",
      "5770/5770 [==============================] - 88s 15ms/step - loss: 11.1273 - val_loss: 10.6787\n",
      "Epoch 95/200\n",
      "5770/5770 [==============================] - 89s 15ms/step - loss: 10.7584 - val_loss: 11.3496\n",
      "Epoch 96/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 10.6110 - val_loss: 10.7594\n",
      "Epoch 97/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 10.5040 - val_loss: 11.0715\n",
      "Epoch 98/200\n",
      "5770/5770 [==============================] - 88s 15ms/step - loss: 10.4975 - val_loss: 11.9161\n",
      "Epoch 99/200\n",
      "5770/5770 [==============================] - 88s 15ms/step - loss: 10.9174 - val_loss: 15.1865\n",
      "Epoch 100/200\n",
      "5770/5770 [==============================] - 88s 15ms/step - loss: 10.5307 - val_loss: 11.2487\n",
      "Epoch 101/200\n",
      "5770/5770 [==============================] - 88s 15ms/step - loss: 10.0249 - val_loss: 13.3029\n",
      "Epoch 102/200\n",
      "5770/5770 [==============================] - 88s 15ms/step - loss: 11.0679 - val_loss: 11.2231\n",
      "Epoch 103/200\n",
      "5770/5770 [==============================] - 88s 15ms/step - loss: 10.4972 - val_loss: 11.1090\n",
      "Epoch 104/200\n",
      "5770/5770 [==============================] - 88s 15ms/step - loss: 10.3379 - val_loss: 11.9805\n",
      "Epoch 105/200\n",
      "5770/5770 [==============================] - 89s 15ms/step - loss: 10.3218 - val_loss: 11.7519\n",
      "Epoch 106/200\n",
      "5770/5770 [==============================] - 89s 15ms/step - loss: 10.6811 - val_loss: 11.1571\n",
      "Epoch 107/200\n",
      "5770/5770 [==============================] - 88s 15ms/step - loss: 10.2606 - val_loss: 11.2108\n",
      "Epoch 108/200\n",
      "5770/5770 [==============================] - 89s 15ms/step - loss: 10.3268 - val_loss: 11.3931\n",
      "Epoch 109/200\n",
      "5770/5770 [==============================] - 88s 15ms/step - loss: 10.5895 - val_loss: 11.2662\n",
      "Epoch 110/200\n",
      "5770/5770 [==============================] - 88s 15ms/step - loss: 11.2396 - val_loss: 11.5222\n",
      "Epoch 111/200\n",
      "5770/5770 [==============================] - 88s 15ms/step - loss: 10.6269 - val_loss: 11.4062\n",
      "Epoch 112/200\n",
      "5770/5770 [==============================] - 89s 15ms/step - loss: 10.3595 - val_loss: 12.5697\n",
      "Epoch 113/200\n",
      "5770/5770 [==============================] - 89s 15ms/step - loss: 10.5920 - val_loss: 10.6313\n",
      "Epoch 114/200\n",
      "5770/5770 [==============================] - 88s 15ms/step - loss: 10.5784 - val_loss: 12.6313\n",
      "Epoch 115/200\n",
      "5770/5770 [==============================] - 89s 15ms/step - loss: 10.5523 - val_loss: 10.6564\n",
      "Epoch 116/200\n",
      "5770/5770 [==============================] - 88s 15ms/step - loss: 10.2928 - val_loss: 11.0531\n",
      "Epoch 117/200\n",
      "5770/5770 [==============================] - 89s 15ms/step - loss: 10.3616 - val_loss: 11.3186\n",
      "Epoch 118/200\n",
      "5770/5770 [==============================] - 88s 15ms/step - loss: 10.1176 - val_loss: 10.3495\n",
      "Epoch 119/200\n",
      "5770/5770 [==============================] - 89s 15ms/step - loss: 10.4219 - val_loss: 10.7943\n",
      "Epoch 120/200\n",
      "5770/5770 [==============================] - 88s 15ms/step - loss: 10.2991 - val_loss: 12.0864\n",
      "Epoch 121/200\n",
      "5770/5770 [==============================] - 89s 15ms/step - loss: 10.1320 - val_loss: 11.0763\n",
      "Epoch 122/200\n",
      "5770/5770 [==============================] - 89s 15ms/step - loss: 10.1080 - val_loss: 10.3112\n",
      "Epoch 123/200\n",
      "5770/5770 [==============================] - 89s 15ms/step - loss: 10.9437 - val_loss: 11.0054\n",
      "Epoch 124/200\n",
      "5770/5770 [==============================] - 89s 15ms/step - loss: 10.9357 - val_loss: 11.3745\n",
      "Epoch 125/200\n",
      "5770/5770 [==============================] - 89s 15ms/step - loss: 10.0302 - val_loss: 10.7010\n",
      "Epoch 126/200\n",
      "5770/5770 [==============================] - 89s 15ms/step - loss: 10.1602 - val_loss: 12.0545\n",
      "Epoch 127/200\n",
      "5770/5770 [==============================] - 89s 15ms/step - loss: 10.0217 - val_loss: 12.5732\n",
      "Epoch 128/200\n",
      "5770/5770 [==============================] - 89s 15ms/step - loss: 10.5999 - val_loss: 11.2977\n",
      "Epoch 129/200\n",
      "5770/5770 [==============================] - 89s 15ms/step - loss: 10.1445 - val_loss: 11.0925\n",
      "Epoch 130/200\n",
      "5770/5770 [==============================] - 89s 15ms/step - loss: 11.1183 - val_loss: 10.2998\n",
      "Epoch 131/200\n",
      "5770/5770 [==============================] - 89s 15ms/step - loss: 9.9351 - val_loss: 10.4676\n",
      "Epoch 132/200\n",
      "5770/5770 [==============================] - 89s 15ms/step - loss: 9.9636 - val_loss: 12.4563\n",
      "Epoch 133/200\n",
      "5770/5770 [==============================] - 89s 15ms/step - loss: 11.0396 - val_loss: 10.9660\n",
      "Epoch 134/200\n",
      "5770/5770 [==============================] - 89s 15ms/step - loss: 10.2160 - val_loss: 10.5089\n",
      "Epoch 135/200\n",
      "5770/5770 [==============================] - 89s 15ms/step - loss: 10.5779 - val_loss: 11.1870\n",
      "Epoch 136/200\n",
      "5770/5770 [==============================] - 89s 15ms/step - loss: 10.7129 - val_loss: 12.6109\n",
      "Epoch 137/200\n",
      "5770/5770 [==============================] - 89s 15ms/step - loss: 10.7697 - val_loss: 10.5065\n",
      "Epoch 138/200\n",
      "5770/5770 [==============================] - 89s 15ms/step - loss: 10.0348 - val_loss: 11.7766\n",
      "Epoch 139/200\n",
      "5770/5770 [==============================] - 89s 15ms/step - loss: 10.3310 - val_loss: 11.6075\n",
      "Epoch 140/200\n",
      "5770/5770 [==============================] - 89s 15ms/step - loss: 10.3038 - val_loss: 10.6961\n",
      "Epoch 141/200\n",
      "5770/5770 [==============================] - 89s 15ms/step - loss: 10.4706 - val_loss: 12.9510\n",
      "Epoch 142/200\n",
      "5770/5770 [==============================] - 89s 15ms/step - loss: 10.4875 - val_loss: 12.0159\n",
      "Epoch 143/200\n",
      "5770/5770 [==============================] - 89s 15ms/step - loss: 10.3138 - val_loss: 10.6703\n",
      "Epoch 144/200\n",
      "5770/5770 [==============================] - 89s 15ms/step - loss: 10.0884 - val_loss: 10.4155\n",
      "Epoch 145/200\n",
      "5770/5770 [==============================] - 89s 15ms/step - loss: 10.7989 - val_loss: 13.5769\n",
      "Epoch 146/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5770/5770 [==============================] - 87s 15ms/step - loss: 10.1656 - val_loss: 10.9126\n",
      "Epoch 147/200\n",
      "5770/5770 [==============================] - 87s 15ms/step - loss: 9.9512 - val_loss: 10.9258\n",
      "Epoch 148/200\n",
      "5770/5770 [==============================] - 87s 15ms/step - loss: 10.3550 - val_loss: 11.3760\n",
      "Epoch 149/200\n",
      "5770/5770 [==============================] - 87s 15ms/step - loss: 10.9736 - val_loss: 11.2014\n",
      "Epoch 150/200\n",
      "5770/5770 [==============================] - 87s 15ms/step - loss: 10.2439 - val_loss: 10.4909\n",
      "Epoch 151/200\n",
      "5770/5770 [==============================] - 87s 15ms/step - loss: 10.6206 - val_loss: 11.6086\n",
      "Epoch 152/200\n",
      "5770/5770 [==============================] - 87s 15ms/step - loss: 10.2268 - val_loss: 12.1870\n",
      "Epoch 153/200\n",
      "5770/5770 [==============================] - 87s 15ms/step - loss: 10.7692 - val_loss: 10.9276\n",
      "Epoch 154/200\n",
      "5770/5770 [==============================] - 87s 15ms/step - loss: 11.3082 - val_loss: 10.5915\n",
      "Epoch 155/200\n",
      "5770/5770 [==============================] - 87s 15ms/step - loss: 10.3436 - val_loss: 10.5655\n",
      "Epoch 156/200\n",
      "5770/5770 [==============================] - 87s 15ms/step - loss: 10.2561 - val_loss: 10.3753\n",
      "Epoch 157/200\n",
      "5770/5770 [==============================] - 88s 15ms/step - loss: 10.1166 - val_loss: 10.4286\n",
      "Epoch 158/200\n",
      "5770/5770 [==============================] - 87s 15ms/step - loss: 10.3149 - val_loss: 10.8726\n",
      "Epoch 159/200\n",
      "5770/5770 [==============================] - 87s 15ms/step - loss: 10.3085 - val_loss: 10.6614\n",
      "Epoch 160/200\n",
      "5770/5770 [==============================] - 87s 15ms/step - loss: 10.7238 - val_loss: 10.9574\n",
      "Epoch 161/200\n",
      "5770/5770 [==============================] - 87s 15ms/step - loss: 10.7678 - val_loss: 11.4677\n",
      "Epoch 162/200\n",
      "5770/5770 [==============================] - 87s 15ms/step - loss: 11.2027 - val_loss: 11.1563\n",
      "Epoch 163/200\n",
      "5770/5770 [==============================] - 87s 15ms/step - loss: 10.0993 - val_loss: 11.6339\n",
      "Epoch 164/200\n",
      "5770/5770 [==============================] - 87s 15ms/step - loss: 10.7850 - val_loss: 11.4815\n",
      "Epoch 165/200\n",
      "5770/5770 [==============================] - 87s 15ms/step - loss: 10.4355 - val_loss: 13.2166\n",
      "Epoch 166/200\n",
      "5770/5770 [==============================] - 88s 15ms/step - loss: 10.5257 - val_loss: 11.2650\n",
      "Epoch 167/200\n",
      "5770/5770 [==============================] - 87s 15ms/step - loss: 10.2747 - val_loss: 11.5336\n",
      "Epoch 168/200\n",
      "5770/5770 [==============================] - 87s 15ms/step - loss: 10.3321 - val_loss: 10.8757\n",
      "Epoch 169/200\n",
      "5770/5770 [==============================] - 87s 15ms/step - loss: 10.6076 - val_loss: 10.3990\n",
      "Epoch 170/200\n",
      "5770/5770 [==============================] - 87s 15ms/step - loss: 10.1575 - val_loss: 11.3154\n",
      "Epoch 171/200\n",
      "5770/5770 [==============================] - 88s 15ms/step - loss: 10.9003 - val_loss: 11.0780\n",
      "Epoch 172/200\n",
      "5770/5770 [==============================] - 88s 15ms/step - loss: 9.9989 - val_loss: 10.5136\n",
      "Epoch 173/200\n",
      "5770/5770 [==============================] - 87s 15ms/step - loss: 10.5056 - val_loss: 10.7518\n",
      "Epoch 174/200\n",
      "5770/5770 [==============================] - 87s 15ms/step - loss: 10.4051 - val_loss: 10.7588\n",
      "Epoch 175/200\n",
      "5770/5770 [==============================] - 88s 15ms/step - loss: 10.4499 - val_loss: 16.4095\n",
      "Epoch 176/200\n",
      "5770/5770 [==============================] - 87s 15ms/step - loss: 10.4031 - val_loss: 11.2838\n",
      "Epoch 177/200\n",
      "5770/5770 [==============================] - 87s 15ms/step - loss: 11.0892 - val_loss: 11.7161\n",
      "Epoch 178/200\n",
      "5770/5770 [==============================] - 87s 15ms/step - loss: 10.9966 - val_loss: 10.9450\n",
      "Epoch 179/200\n",
      "5770/5770 [==============================] - 87s 15ms/step - loss: 10.2230 - val_loss: 10.7604\n",
      "Epoch 180/200\n",
      "5770/5770 [==============================] - 93s 16ms/step - loss: 10.1664 - val_loss: 11.0411\n",
      "Epoch 181/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 10.5964 - val_loss: 11.1105\n",
      "Epoch 182/200\n",
      "5770/5770 [==============================] - 89s 16ms/step - loss: 11.8183 - val_loss: 11.2475\n",
      "Epoch 183/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 11.1612 - val_loss: 10.9973\n",
      "Epoch 184/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 10.4228 - val_loss: 12.8384\n",
      "Epoch 185/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 10.5493 - val_loss: 11.4152\n",
      "Epoch 186/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 11.5208 - val_loss: 12.4473\n",
      "Epoch 187/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 11.0083 - val_loss: 10.8636\n",
      "Epoch 188/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 10.4495 - val_loss: 10.5007\n",
      "Epoch 189/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 10.6454 - val_loss: 11.7225\n",
      "Epoch 190/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 10.7639 - val_loss: 11.2300\n",
      "Epoch 191/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 10.2792 - val_loss: 11.2209\n",
      "Epoch 192/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 11.0282 - val_loss: 12.2868\n",
      "Epoch 193/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 11.4722 - val_loss: 12.7586\n",
      "Epoch 194/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 10.7227 - val_loss: 10.8871\n",
      "Epoch 195/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 11.0029 - val_loss: 11.5154\n",
      "Epoch 196/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 10.9770 - val_loss: 12.3686\n",
      "Epoch 197/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 10.7233 - val_loss: 11.0449\n",
      "Epoch 198/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 10.7224 - val_loss: 12.2282\n",
      "Epoch 199/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 11.3412 - val_loss: 13.3363\n",
      "Epoch 200/200\n",
      "5770/5770 [==============================] - 90s 16ms/step - loss: 11.7425 - val_loss: 14.4441\n"
     ]
    }
   ],
   "source": [
    "hist = lstm_autoencoder.fit_generator(train_generator(x_train), epochs=200, steps_per_epoch=steps_per_epoch, verbose=1, validation_steps=len(x_val), validation_data=val_generator(x_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "model_json = lstm_autoencoder.to_json()\n",
    "filename = 'new_cce_lstmae'  #input('filename: ') #\n",
    "with open('res/cce/' + filename + '.json', 'w') as file:\n",
    "    file.write(model_json)\n",
    "lstm_autoencoder.save_weights('res/cce/weights_' +  filename + '.h5')\n",
    "\n",
    "with open('res/last_cce_histroy.json', 'w') as f:\n",
    "    json.dump(hist.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "loss_ax.plot(hist.history['val_loss'], 'b', label='val loss')\n",
    "loss_ax.plot(hist.history['loss'], 'r', label='train loss')\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "loss_ax.legend(loc='upper left')\n",
    "\n",
    "plt.title('mse_cce')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = lstm_autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36404565399193833\n"
     ]
    }
   ],
   "source": [
    "mean= 0\n",
    "for xt in x_test:\n",
    "    xt = xt.reshape(1, xt.shape[0], xt.shape[1])\n",
    "    out = loaded_model.predict(xt)\n",
    "    mean += ((xt-out)**2).mean(axis=None)\n",
    "print(mean/len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"last_mse_lstmae_cce\"\n",
    "loaded_model = model_from_json(open('res/cce/' +filename + '.json').read())\n",
    "loaded_model.load_weights('model_save/mse_cce_models/weights_' + filename + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(loaded_model.input, loaded_model.layers[3].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = x_test[0].reshape(1, x_test[0].shape[0], x_test[0].shape[1])\n",
    "latent_vector = []\n",
    "for x in x_test:\n",
    "    x = x.reshape(1, x.shape[0], x.shape[1])\n",
    "    latent_vector.append(encoder.predict(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'model_save/mse_cce_models/weights' + '{epoch:02d}-{loss:.4f}.h5'\n",
    "early_stopping_callback = EarlyStopping(monitor='loss', patience=200)\n",
    "checkpoint_callback = ModelCheckpoint(model_path, monitor='loss', verbose = 1, save_best_only = True, save_weights_only = True, mode='min')#, period=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

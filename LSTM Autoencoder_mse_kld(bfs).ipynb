{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob, os\n",
    "from keras import layers as ly\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import LSTM, Dense, RepeatVector, TimeDistributed\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Lambda\n",
    "from keras import losses\n",
    "from keras.models import model_from_json\n",
    "import keras.backend.tensorflow_backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir = './sequence/*'\n",
    "dir = './datasets/latest_seq/bfs-character/*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file read\n",
    "all_names = []\n",
    "all_data = []\n",
    "sequence_length = []\n",
    "alpha = list(string.ascii_uppercase)\n",
    "data_length = len(glob.glob(dir))\n",
    "file_predix = './datasets/latest_seq/bfs-character/graph'\n",
    "for index in range(data_length):\n",
    "    filename = file_predix + str(index) + \"-*\"\n",
    "    files = glob.glob(filename)\n",
    "    for file in files:\n",
    "        datasets = []\n",
    "        all_names.append(file.split('/')[-1].replace('.txt', ''))\n",
    "        for rf in open(file, 'r'):\n",
    "            (u, v, w) = rf[1:-2].split(', ')\n",
    "            datasets.append([alpha.index(u[1])+1, alpha.index(v[1]) +1, float(w)])\n",
    "        sequence_length.append(len(datasets))\n",
    "        all_data.append(datasets)\n",
    "all_data = np.array([np.array(arr) for arr in all_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, train_name, test_name = train_test_split(all_data, all_names, test_size=0.3)\n",
    "x_test, x_val, test_name, val_name = train_test_split(x_test, test_name, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_name\n",
    "tr_names= []\n",
    "for name in train_name:\n",
    "    tr_names.append(name.split('-')[0].replace('graph', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = max(sequence_length)\n",
    "n_features = 3\n",
    "batch_size = 32\n",
    "epochs = 200\n",
    "steps_per_epoch = len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    loss1 = losses.mean_squared_error(y_true, y_pred)\n",
    "    loss2 = losses.kld(y_true, y_pred) # categorical_crossentropy\n",
    "    return loss1 * 0.7 + loss2 * 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1018 14:49:21.282840 140188255131392 deprecation_wrapper.py:119] From /home/minji/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1018 14:49:21.298787 140188255131392 deprecation_wrapper.py:119] From /home/minji/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1018 14:49:21.301094 140188255131392 deprecation_wrapper.py:119] From /home/minji/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1018 14:49:21.792530 140188255131392 deprecation_wrapper.py:119] From /home/minji/.local/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W1018 14:49:21.798334 140188255131392 deprecation_wrapper.py:119] From /home/minji/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1702: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def repeat_vector(args):\n",
    "    layer_to_repeat = args[0]\n",
    "    sequence_layer = args[1]\n",
    "    return RepeatVector(K.shape(sequence_layer)[1])(layer_to_repeat)\n",
    "    \n",
    "\n",
    "inputs = Input(shape=(None, 3))\n",
    "encoded = LSTM(128, return_sequences=True)(inputs)  #activation 안적으면 tanh\n",
    "encoded = LSTM(64)(encoded)\n",
    "\n",
    "decoded = Lambda(repeat_vector, output_shape=(None, 64)) ([encoded, inputs]) # inputs의 shape[1] 만큼 encoded 를 반복 생성\n",
    "\n",
    "decoded = LSTM(64, return_sequences=True)(decoded)\n",
    "decoded = LSTM(128, return_sequences=True)(decoded)\n",
    "decoded = TimeDistributed(Dense(3))(decoded)\n",
    "encoder = Model(inputs, encoded)\n",
    "\n",
    "lstm_autoencoder = Model(inputs, decoded)\n",
    "lstm_autoencoder.compile(loss=custom_loss, optimizer=Adam())#decay=0.9\n",
    "#lstm_autoencoder_500 = lstm_autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_generator(x_val):\n",
    "    idx = 0\n",
    "    while True:\n",
    "        yield np.array([x_val[idx]]), np.array([x_val[idx]])\n",
    "        idx += 1\n",
    "        if idx >= len(x_val):\n",
    "            idx = 0\n",
    "\n",
    "def train_generator(x_train):\n",
    "    idx = 0\n",
    "    while True:\n",
    "        yield np.array([x_train[idx]]), np.array([x_train[idx]])\n",
    "        idx += 1\n",
    "        if idx >= len(x_train):\n",
    "            idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1018 14:49:21.972902 140188255131392 deprecation.py:323] From /home/minji/anaconda3/envs/graph/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W1018 14:49:23.293820 140188255131392 deprecation_wrapper.py:119] From /home/minji/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "4914/4914 [==============================] - 48s 10ms/step - loss: 32.6924 - val_loss: 13.9576\n",
      "Epoch 2/300\n",
      "4914/4914 [==============================] - 46s 9ms/step - loss: 12.4890 - val_loss: 10.6279\n",
      "Epoch 3/300\n",
      "4914/4914 [==============================] - 46s 9ms/step - loss: 10.1235 - val_loss: 8.7866\n",
      "Epoch 4/300\n",
      "4914/4914 [==============================] - 46s 9ms/step - loss: 8.6645 - val_loss: 7.5640\n",
      "Epoch 5/300\n",
      "4914/4914 [==============================] - 46s 9ms/step - loss: 7.7979 - val_loss: 7.1754\n",
      "Epoch 6/300\n",
      "4914/4914 [==============================] - 46s 9ms/step - loss: 6.8902 - val_loss: 6.0163\n",
      "Epoch 7/300\n",
      "4914/4914 [==============================] - 46s 9ms/step - loss: 5.8457 - val_loss: 4.9175\n",
      "Epoch 8/300\n",
      "4914/4914 [==============================] - 46s 9ms/step - loss: 4.6545 - val_loss: 4.9037\n",
      "Epoch 9/300\n",
      "4914/4914 [==============================] - 46s 9ms/step - loss: 4.0800 - val_loss: 4.2875\n",
      "Epoch 10/300\n",
      "4914/4914 [==============================] - 46s 9ms/step - loss: 3.6424 - val_loss: 3.2741\n",
      "Epoch 11/300\n",
      "4914/4914 [==============================] - 46s 9ms/step - loss: 3.2574 - val_loss: 2.8779\n",
      "Epoch 12/300\n",
      "4914/4914 [==============================] - 46s 9ms/step - loss: 2.9274 - val_loss: 2.7177\n",
      "Epoch 13/300\n",
      "4914/4914 [==============================] - 47s 9ms/step - loss: 2.6063 - val_loss: 2.8938\n",
      "Epoch 14/300\n",
      "4914/4914 [==============================] - 42s 9ms/step - loss: 2.3613 - val_loss: 2.2342\n",
      "Epoch 15/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 2.1477 - val_loss: 2.3377\n",
      "Epoch 16/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 1.9758 - val_loss: 1.9407\n",
      "Epoch 17/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 1.8395 - val_loss: 2.2828\n",
      "Epoch 18/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 1.7186 - val_loss: 1.7970\n",
      "Epoch 19/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 1.5225 - val_loss: 1.7362\n",
      "Epoch 20/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 1.4477 - val_loss: 1.5174\n",
      "Epoch 21/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 1.2899 - val_loss: 1.3370\n",
      "Epoch 22/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 1.1876 - val_loss: 1.3779\n",
      "Epoch 23/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 1.1146 - val_loss: 1.2553\n",
      "Epoch 24/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 1.0220 - val_loss: 1.3426\n",
      "Epoch 25/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.9491 - val_loss: 1.2846\n",
      "Epoch 26/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.9071 - val_loss: 1.0683\n",
      "Epoch 27/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.8146 - val_loss: 1.1237\n",
      "Epoch 28/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.7655 - val_loss: 0.8800\n",
      "Epoch 29/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.7344 - val_loss: 0.8541\n",
      "Epoch 30/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.6740 - val_loss: 0.9728\n",
      "Epoch 31/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.6540 - val_loss: 0.7949\n",
      "Epoch 32/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.6140 - val_loss: 0.7341\n",
      "Epoch 33/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.5978 - val_loss: 0.8617\n",
      "Epoch 34/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.5612 - val_loss: 0.7663\n",
      "Epoch 35/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.5409 - val_loss: 0.6546\n",
      "Epoch 36/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.5151 - val_loss: 0.7673\n",
      "Epoch 37/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.5039 - val_loss: 0.6341\n",
      "Epoch 38/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.4862 - val_loss: 0.6901\n",
      "Epoch 39/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.4817 - val_loss: 0.6433\n",
      "Epoch 40/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.4623 - val_loss: 0.6466\n",
      "Epoch 41/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.4524 - val_loss: 0.6214\n",
      "Epoch 42/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.4249 - val_loss: 0.5988\n",
      "Epoch 43/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.4153 - val_loss: 0.6796\n",
      "Epoch 44/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.4115 - val_loss: 0.6784\n",
      "Epoch 45/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.3961 - val_loss: 0.6372\n",
      "Epoch 46/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.4023 - val_loss: 0.5566\n",
      "Epoch 47/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.3814 - val_loss: 0.5834\n",
      "Epoch 48/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.3775 - val_loss: 0.7465\n",
      "Epoch 49/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.3691 - val_loss: 0.6224\n",
      "Epoch 50/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.3630 - val_loss: 0.5702\n",
      "Epoch 51/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.3564 - val_loss: 0.5272\n",
      "Epoch 52/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.3381 - val_loss: 0.5551\n",
      "Epoch 53/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.3534 - val_loss: 0.5902\n",
      "Epoch 54/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.3342 - val_loss: 0.5038\n",
      "Epoch 55/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.3272 - val_loss: 0.6086\n",
      "Epoch 56/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.3377 - val_loss: 0.4847\n",
      "Epoch 57/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.3164 - val_loss: 0.5417\n",
      "Epoch 58/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.3248 - val_loss: 0.5053\n",
      "Epoch 59/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.3126 - val_loss: 0.5120\n",
      "Epoch 60/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.3134 - val_loss: 0.5344\n",
      "Epoch 61/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.2960 - val_loss: 0.4869\n",
      "Epoch 62/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.2857 - val_loss: 0.4431\n",
      "Epoch 63/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.2969 - val_loss: 0.4710\n",
      "Epoch 64/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.2901 - val_loss: 0.5368\n",
      "Epoch 65/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.2706 - val_loss: 0.4301\n",
      "Epoch 66/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.2802 - val_loss: 0.4267\n",
      "Epoch 67/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.2784 - val_loss: 0.4701\n",
      "Epoch 68/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.2796 - val_loss: 0.4262\n",
      "Epoch 69/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.2687 - val_loss: 0.4019\n",
      "Epoch 70/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.2635 - val_loss: 0.4078\n",
      "Epoch 71/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.2597 - val_loss: 0.4232\n",
      "Epoch 72/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.2505 - val_loss: 0.3968\n",
      "Epoch 73/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.2459 - val_loss: 0.3981\n",
      "Epoch 74/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.2525 - val_loss: 0.4589\n",
      "Epoch 75/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.2602 - val_loss: 0.4265\n",
      "Epoch 76/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.2490 - val_loss: 0.5825\n",
      "Epoch 77/300\n",
      "4914/4914 [==============================] - 42s 9ms/step - loss: 0.2460 - val_loss: 0.4124\n",
      "Epoch 78/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.2381 - val_loss: 0.4780\n",
      "Epoch 79/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.2463 - val_loss: 0.4109\n",
      "Epoch 80/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.2336 - val_loss: 0.3963\n",
      "Epoch 81/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.2326 - val_loss: 0.3944\n",
      "Epoch 82/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.2468 - val_loss: 0.4430\n",
      "Epoch 83/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.2203 - val_loss: 0.5161\n",
      "Epoch 84/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.2308 - val_loss: 0.4073\n",
      "Epoch 85/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.2189 - val_loss: 0.3940\n",
      "Epoch 86/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.2178 - val_loss: 0.4250\n",
      "Epoch 87/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.2204 - val_loss: 0.3908\n",
      "Epoch 88/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.2190 - val_loss: 0.4846\n",
      "Epoch 89/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.2118 - val_loss: 0.3965\n",
      "Epoch 90/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.2136 - val_loss: 0.4844\n",
      "Epoch 91/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.2104 - val_loss: 0.4140\n",
      "Epoch 92/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.2084 - val_loss: 0.3900\n",
      "Epoch 93/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.2096 - val_loss: 0.4115\n",
      "Epoch 94/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.2015 - val_loss: 0.4129\n",
      "Epoch 95/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1984 - val_loss: 0.4317\n",
      "Epoch 96/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.2001 - val_loss: 0.3844\n",
      "Epoch 97/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.2034 - val_loss: 0.3620\n",
      "Epoch 98/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1959 - val_loss: 0.3944\n",
      "Epoch 99/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1941 - val_loss: 0.3602\n",
      "Epoch 100/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.2183 - val_loss: 0.3960\n",
      "Epoch 101/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1899 - val_loss: 0.3810\n",
      "Epoch 102/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1924 - val_loss: 0.4261\n",
      "Epoch 103/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1889 - val_loss: 0.3688\n",
      "Epoch 104/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1858 - val_loss: 0.3712\n",
      "Epoch 105/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1887 - val_loss: 0.3804\n",
      "Epoch 106/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1931 - val_loss: 0.3413\n",
      "Epoch 107/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1755 - val_loss: 0.3868\n",
      "Epoch 108/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1848 - val_loss: 0.3794\n",
      "Epoch 109/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1845 - val_loss: 0.3307\n",
      "Epoch 110/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1856 - val_loss: 0.3570\n",
      "Epoch 111/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1845 - val_loss: 0.3732\n",
      "Epoch 112/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1807 - val_loss: 0.3366\n",
      "Epoch 113/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1764 - val_loss: 0.3197\n",
      "Epoch 114/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1846 - val_loss: 0.3199\n",
      "Epoch 115/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1738 - val_loss: 0.4060\n",
      "Epoch 116/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1783 - val_loss: 0.3267\n",
      "Epoch 117/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1797 - val_loss: 0.3539\n",
      "Epoch 118/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1736 - val_loss: 0.3641\n",
      "Epoch 119/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1755 - val_loss: 0.3794\n",
      "Epoch 120/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1668 - val_loss: 0.3736\n",
      "Epoch 121/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1800 - val_loss: 0.3582\n",
      "Epoch 122/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1563 - val_loss: 0.3589\n",
      "Epoch 123/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1734 - val_loss: 0.3848\n",
      "Epoch 124/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1639 - val_loss: 0.3765\n",
      "Epoch 125/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1694 - val_loss: 0.3601\n",
      "Epoch 126/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1645 - val_loss: 0.3438\n",
      "Epoch 127/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1618 - val_loss: 0.3584\n",
      "Epoch 128/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1649 - val_loss: 0.3636\n",
      "Epoch 129/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1551 - val_loss: 0.3615\n",
      "Epoch 130/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1756 - val_loss: 0.4205\n",
      "Epoch 131/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1559 - val_loss: 0.4082\n",
      "Epoch 132/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1555 - val_loss: 0.3638\n",
      "Epoch 133/300\n",
      "4914/4914 [==============================] - 44s 9ms/step - loss: 0.1617 - val_loss: 0.3708\n",
      "Epoch 134/300\n",
      "4914/4914 [==============================] - 42s 9ms/step - loss: 0.1540 - val_loss: 0.3480\n",
      "Epoch 135/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1654 - val_loss: 0.3653\n",
      "Epoch 136/300\n",
      "4914/4914 [==============================] - 42s 9ms/step - loss: 0.1543 - val_loss: 0.3542\n",
      "Epoch 137/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1582 - val_loss: 0.3386\n",
      "Epoch 138/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1738 - val_loss: 0.3373\n",
      "Epoch 139/300\n",
      "4914/4914 [==============================] - 42s 9ms/step - loss: 0.1506 - val_loss: 0.3058\n",
      "Epoch 140/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1498 - val_loss: 0.3139\n",
      "Epoch 141/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1568 - val_loss: 0.3501\n",
      "Epoch 142/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1481 - val_loss: 0.3509\n",
      "Epoch 143/300\n",
      "4914/4914 [==============================] - 42s 9ms/step - loss: 0.1534 - val_loss: 0.3588\n",
      "Epoch 144/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1698 - val_loss: 0.3181\n",
      "Epoch 145/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1496 - val_loss: 0.3656\n",
      "Epoch 146/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1458 - val_loss: 0.3165\n",
      "Epoch 147/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1716 - val_loss: 0.3112\n",
      "Epoch 148/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1379 - val_loss: 0.3435\n",
      "Epoch 149/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1472 - val_loss: 0.2973\n",
      "Epoch 150/300\n",
      "4914/4914 [==============================] - 42s 9ms/step - loss: 0.1482 - val_loss: 0.3606\n",
      "Epoch 151/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1460 - val_loss: 0.3285\n",
      "Epoch 152/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1405 - val_loss: 0.3268\n",
      "Epoch 153/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1521 - val_loss: 0.3507\n",
      "Epoch 154/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1454 - val_loss: 0.3284\n",
      "Epoch 155/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1390 - val_loss: 0.3229\n",
      "Epoch 156/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1418 - val_loss: 0.3137\n",
      "Epoch 157/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1529 - val_loss: 0.3076\n",
      "Epoch 158/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1359 - val_loss: 0.3229\n",
      "Epoch 159/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1432 - val_loss: 0.3663\n",
      "Epoch 160/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1446 - val_loss: 0.3247\n",
      "Epoch 161/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1472 - val_loss: 0.3457\n",
      "Epoch 162/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1362 - val_loss: 0.3756\n",
      "Epoch 163/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1319 - val_loss: 0.3289\n",
      "Epoch 164/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1451 - val_loss: 0.3293\n",
      "Epoch 165/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1390 - val_loss: 0.3027\n",
      "Epoch 166/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1401 - val_loss: 0.2981\n",
      "Epoch 167/300\n",
      "4914/4914 [==============================] - 44s 9ms/step - loss: 0.1401 - val_loss: 0.3373\n",
      "Epoch 168/300\n",
      "4914/4914 [==============================] - 44s 9ms/step - loss: 0.1330 - val_loss: 0.3304\n",
      "Epoch 169/300\n",
      "4914/4914 [==============================] - 43s 9ms/step - loss: 0.1345 - val_loss: 0.3225\n",
      "Epoch 170/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1408 - val_loss: 0.3105\n",
      "Epoch 171/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1485 - val_loss: 0.2978\n",
      "Epoch 172/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1282 - val_loss: 0.3263\n",
      "Epoch 173/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1331 - val_loss: 0.3220\n",
      "Epoch 174/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1290 - val_loss: 0.2921\n",
      "Epoch 175/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1336 - val_loss: 0.2854\n",
      "Epoch 176/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1267 - val_loss: 0.3267\n",
      "Epoch 177/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1305 - val_loss: 0.3291\n",
      "Epoch 178/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1311 - val_loss: 0.3723\n",
      "Epoch 179/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1347 - val_loss: 0.3094\n",
      "Epoch 180/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1284 - val_loss: 0.3228\n",
      "Epoch 181/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1289 - val_loss: 0.3350\n",
      "Epoch 182/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1319 - val_loss: 0.2929\n",
      "Epoch 183/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1310 - val_loss: 0.2873\n",
      "Epoch 184/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1271 - val_loss: 0.3151\n",
      "Epoch 185/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1328 - val_loss: 0.3542\n",
      "Epoch 186/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1280 - val_loss: 0.2946\n",
      "Epoch 187/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1214 - val_loss: 0.3300\n",
      "Epoch 188/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1274 - val_loss: 0.3014\n",
      "Epoch 189/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1276 - val_loss: 0.3801\n",
      "Epoch 190/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1228 - val_loss: 0.3780\n",
      "Epoch 191/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1305 - val_loss: 0.2914\n",
      "Epoch 192/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1213 - val_loss: 0.3193\n",
      "Epoch 193/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1276 - val_loss: 0.3158\n",
      "Epoch 194/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1212 - val_loss: 0.3226\n",
      "Epoch 195/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1221 - val_loss: 0.3602\n",
      "Epoch 196/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1189 - val_loss: 0.2695\n",
      "Epoch 197/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1399 - val_loss: 0.2690\n",
      "Epoch 198/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1153 - val_loss: 0.3065\n",
      "Epoch 199/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1205 - val_loss: 0.2901\n",
      "Epoch 200/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1178 - val_loss: 0.3138\n",
      "Epoch 201/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1220 - val_loss: 0.3023\n",
      "Epoch 202/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1189 - val_loss: 0.2927\n",
      "Epoch 203/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1233 - val_loss: 0.2934\n",
      "Epoch 204/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1159 - val_loss: 0.3355\n",
      "Epoch 205/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1156 - val_loss: 0.3003\n",
      "Epoch 206/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1191 - val_loss: 0.2928\n",
      "Epoch 207/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1190 - val_loss: 0.2852\n",
      "Epoch 208/300\n",
      "4914/4914 [==============================] - 42s 9ms/step - loss: 0.1219 - val_loss: 0.3067\n",
      "Epoch 209/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1156 - val_loss: 0.2986\n",
      "Epoch 210/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1841 - val_loss: 0.3122\n",
      "Epoch 211/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1186 - val_loss: 0.2877\n",
      "Epoch 212/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1093 - val_loss: 0.2974\n",
      "Epoch 213/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1186 - val_loss: 0.3339\n",
      "Epoch 214/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1678 - val_loss: 0.2864\n",
      "Epoch 215/300\n",
      "4914/4914 [==============================] - 42s 9ms/step - loss: 0.1644 - val_loss: 0.2782\n",
      "Epoch 216/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1046 - val_loss: 0.3002\n",
      "Epoch 217/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1106 - val_loss: 0.3280\n",
      "Epoch 218/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1187 - val_loss: 0.2967\n",
      "Epoch 219/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1161 - val_loss: 0.3283\n",
      "Epoch 220/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1292 - val_loss: 0.3197\n",
      "Epoch 221/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1126 - val_loss: 0.2825\n",
      "Epoch 222/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1107 - val_loss: 0.2686\n",
      "Epoch 223/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1134 - val_loss: 0.2990\n",
      "Epoch 224/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1097 - val_loss: 0.2975\n",
      "Epoch 225/300\n",
      "4914/4914 [==============================] - 42s 9ms/step - loss: 0.1227 - val_loss: 0.2681\n",
      "Epoch 226/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1091 - val_loss: 0.3011\n",
      "Epoch 227/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1330 - val_loss: 0.2935\n",
      "Epoch 228/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1234 - val_loss: 0.3298\n",
      "Epoch 229/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1087 - val_loss: 0.2739\n",
      "Epoch 230/300\n",
      "4914/4914 [==============================] - 42s 9ms/step - loss: 0.1152 - val_loss: 0.3046\n",
      "Epoch 231/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1145 - val_loss: 0.3313\n",
      "Epoch 232/300\n",
      "4914/4914 [==============================] - 42s 8ms/step - loss: 0.1111 - val_loss: 0.2825\n",
      "Epoch 233/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1068 - val_loss: 0.3470\n",
      "Epoch 234/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1081 - val_loss: 0.3088\n",
      "Epoch 235/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1104 - val_loss: 0.2904\n",
      "Epoch 236/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1059 - val_loss: 0.2753\n",
      "Epoch 237/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1121 - val_loss: 0.3104\n",
      "Epoch 238/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1067 - val_loss: 0.2922\n",
      "Epoch 239/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1033 - val_loss: 0.3557\n",
      "Epoch 240/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1102 - val_loss: 0.3274\n",
      "Epoch 241/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1055 - val_loss: 0.3028\n",
      "Epoch 242/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1051 - val_loss: 0.2880\n",
      "Epoch 243/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1025 - val_loss: 0.3001\n",
      "Epoch 244/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1058 - val_loss: 0.3776\n",
      "Epoch 245/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1097 - val_loss: 0.3173\n",
      "Epoch 246/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1058 - val_loss: 0.2948\n",
      "Epoch 247/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.0993 - val_loss: 0.3430\n",
      "Epoch 248/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1049 - val_loss: 0.2865\n",
      "Epoch 249/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1081 - val_loss: 0.3034\n",
      "Epoch 250/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1058 - val_loss: 0.3003\n",
      "Epoch 251/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1060 - val_loss: 0.2895\n",
      "Epoch 252/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1096 - val_loss: 0.3241\n",
      "Epoch 253/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1166 - val_loss: 0.2759\n",
      "Epoch 254/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.0958 - val_loss: 0.2846\n",
      "Epoch 255/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1010 - val_loss: 0.3525\n",
      "Epoch 256/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1077 - val_loss: 0.2752\n",
      "Epoch 257/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1033 - val_loss: 0.3011\n",
      "Epoch 258/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.0975 - val_loss: 0.3128\n",
      "Epoch 259/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1055 - val_loss: 0.3198\n",
      "Epoch 260/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1046 - val_loss: 0.2850\n",
      "Epoch 261/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1012 - val_loss: 0.3080\n",
      "Epoch 262/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.0996 - val_loss: 0.2820\n",
      "Epoch 263/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1020 - val_loss: 0.3294\n",
      "Epoch 264/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.0989 - val_loss: 0.3271\n",
      "Epoch 265/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.0961 - val_loss: 0.2839\n",
      "Epoch 266/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1063 - val_loss: 0.3158\n",
      "Epoch 267/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.0980 - val_loss: 0.3333\n",
      "Epoch 268/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.0995 - val_loss: 0.2765\n",
      "Epoch 269/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1026 - val_loss: 0.3495\n",
      "Epoch 270/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1035 - val_loss: 0.2912\n",
      "Epoch 271/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.0958 - val_loss: 0.2868\n",
      "Epoch 272/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1044 - val_loss: 0.2861\n",
      "Epoch 273/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.0963 - val_loss: 0.3172\n",
      "Epoch 274/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.0985 - val_loss: 0.2933\n",
      "Epoch 275/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1044 - val_loss: 0.2996\n",
      "Epoch 276/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.0949 - val_loss: 0.3187\n",
      "Epoch 277/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1031 - val_loss: 0.2886\n",
      "Epoch 278/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.0987 - val_loss: 0.2978\n",
      "Epoch 279/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.0987 - val_loss: 0.3194\n",
      "Epoch 280/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.0975 - val_loss: 0.3107\n",
      "Epoch 281/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.0984 - val_loss: 0.2925\n",
      "Epoch 282/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.0940 - val_loss: 0.3198\n",
      "Epoch 283/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.0978 - val_loss: 0.2806\n",
      "Epoch 284/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.0917 - val_loss: 0.3005\n",
      "Epoch 285/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.0962 - val_loss: 0.2840\n",
      "Epoch 286/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.0962 - val_loss: 0.2964\n",
      "Epoch 287/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.0954 - val_loss: 0.2836\n",
      "Epoch 288/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.0975 - val_loss: 0.3104\n",
      "Epoch 289/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.0960 - val_loss: 0.3132\n",
      "Epoch 290/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.0947 - val_loss: 0.3122\n",
      "Epoch 291/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.0965 - val_loss: 0.3312\n",
      "Epoch 292/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.1155 - val_loss: 0.2852\n",
      "Epoch 293/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.0951 - val_loss: 0.3727\n",
      "Epoch 294/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.0929 - val_loss: 0.3460\n",
      "Epoch 295/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.0931 - val_loss: 0.3148\n",
      "Epoch 296/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.0904 - val_loss: 0.2887\n",
      "Epoch 297/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.0913 - val_loss: 0.3086\n",
      "Epoch 298/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.0919 - val_loss: 0.3642\n",
      "Epoch 299/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.0937 - val_loss: 0.2973\n",
      "Epoch 300/300\n",
      "4914/4914 [==============================] - 41s 8ms/step - loss: 0.0958 - val_loss: 0.2746\n"
     ]
    }
   ],
   "source": [
    "hist = lstm_autoencoder.fit_generator(train_generator(x_train), epochs=300, steps_per_epoch=steps_per_epoch, verbose=1, validation_steps=len(x_val), validation_data=val_generator(x_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL SAVE\n",
    "model_json = lstm_autoencoder.to_json()\n",
    "filename = 'new_mse_kld_lstmae' #input('filename: ') \n",
    "with open('model_save/mse_kld_models/' + filename + '.json', 'w') as file:\n",
    "    file.write(model_json)\n",
    "lstm_autoencoder.save_weights('model_save/mse_kld_models/weights_' +  filename + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('model_save/mse_kld_models/new_kld_history.json', 'w') as f:\n",
    "    json.dump(hist.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "loss_ax.plot(hist.history['val_loss'], 'b', label='val loss')\n",
    "loss_ax.plot(hist.history['loss'], 'r', label='train loss')\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "loss_ax.legend(loc='upper left')\n",
    "\n",
    "plt.title('kld mse')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = lstm_autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38918926875524856\n"
     ]
    }
   ],
   "source": [
    "mean= 0\n",
    "for xt in x_test:\n",
    "    xt = xt.reshape(1, xt.shape[0], xt.shape[1])\n",
    "    out = loaded_model.predict(xt)\n",
    "    mean += ((xt-out)**2).mean(axis=None)\n",
    "print(mean/len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1017 20:05:39.672357 139985310242560 deprecation_wrapper.py:119] From /home/minji/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1017 20:05:39.683409 139985310242560 deprecation_wrapper.py:119] From /home/minji/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1017 20:05:39.685610 139985310242560 deprecation_wrapper.py:119] From /home/minji/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1017 20:05:40.291379 139985310242560 deprecation_wrapper.py:119] From /home/minji/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W1017 20:05:40.292181 139985310242560 deprecation_wrapper.py:119] From /home/minji/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filename = \"latest_mse_kld_lstmae1\"\n",
    "# MODEL LOAD\n",
    "loaded_model = model_from_json(open('model_save/mse_kld_models/' +filename + '.json').read())\n",
    "loaded_model.load_weights('model_save/mse_kld_models/weights_' + filename + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(loaded_model.input, loaded_model.layers[3].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = x_test[0].reshape(1, x_test[0].shape[0], x_test[0].shape[1])\n",
    "latent_vector = []\n",
    "for x in x_test:\n",
    "    x = x.reshape(1, x.shape[0], x.shape[1])\n",
    "    latent_vector.append(encoder.predict(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'model_save/mse_kld_models/weights' + '{epoch:02d}-{loss:.4f}.h5'\n",
    "early_stopping_callback = EarlyStopping(monitor='loss', patience=200)\n",
    "checkpoint_callback = ModelCheckpoint(model_path, monitor='loss', verbose = 1, save_best_only = True, save_weights_only = True, mode='min', period=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob, os\n",
    "from keras import layers as ly\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import LSTM, Dense, RepeatVector, TimeDistributed\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Lambda\n",
    "from keras import losses\n",
    "from keras.models import model_from_json\n",
    "import keras.backend.tensorflow_backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir = './sequence/*'\n",
    "dir = './latest_sequence/bfs/*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file read\n",
    "all_data = []\n",
    "sequence_length = []\n",
    "name = []\n",
    "for file in sorted(glob.glob(dir)):\n",
    "    name.append(file.split('/')[-1].replace('.txt', ''))\n",
    "    datasets = []\n",
    "    for f in open(file, 'r'):\n",
    "        f = f.replace(']', '').replace('[', '').replace('\\n','')\n",
    "        (u, v, w) = f.split(',')\n",
    "        datasets.append([int(u), int(v), float(w)])\n",
    "    sequence_length.append(len(datasets))\n",
    "    all_data.append(datasets)\n",
    "#all_data = np.array(all_data)\n",
    "all_data = np.array([np.array(arr) for arr in all_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, train_name, test_name = train_test_split(all_data, name, test_size=0.3)\n",
    "x_test, x_val, test_name, val_name = train_test_split(x_test, test_name, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_name\n",
    "tr_names= []\n",
    "for name in train_name:\n",
    "    tr_names.append(name.split('-')[0].replace('graph', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = max(sequence_length)\n",
    "n_features = 3\n",
    "batch_size = 32\n",
    "epochs = 500\n",
    "steps_per_epoch = len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    loss1 = losses.mean_squared_error(y_true, y_pred)\n",
    "    loss2 = losses.kld(y_true, y_pred) # categorical_crossentropy\n",
    "    return loss1 * 0.7 + loss2 * 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1004 17:02:47.855664 140183748400896 deprecation_wrapper.py:119] From /home/minji/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1004 17:02:47.864664 140183748400896 deprecation_wrapper.py:119] From /home/minji/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1004 17:02:47.867667 140183748400896 deprecation_wrapper.py:119] From /home/minji/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1004 17:02:48.361598 140183748400896 deprecation_wrapper.py:119] From /home/minji/.local/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def repeat_vector(args):\n",
    "    layer_to_repeat = args[0]\n",
    "    sequence_layer = args[1]\n",
    "    return RepeatVector(K.shape(sequence_layer)[1])(layer_to_repeat)\n",
    "    \n",
    "\n",
    "inputs = Input(shape=(None, 3))\n",
    "encoded = LSTM(128, return_sequences=True)(inputs)  #activation 안적으면 tanh\n",
    "encoded = LSTM(64)(encoded)\n",
    "\n",
    "decoded = Lambda(repeat_vector, output_shape=(None, 64)) ([encoded, inputs]) # inputs의 shape[1] 만큼 encoded 를 반복 생성\n",
    "\n",
    "decoded = LSTM(64, return_sequences=True)(decoded)\n",
    "decoded = LSTM(128, return_sequences=True)(decoded)\n",
    "decoded = TimeDistributed(Dense(3))(decoded)\n",
    "encoder = Model(inputs, encoded)\n",
    "\n",
    "lstm_autoencoder = Model(inputs, decoded)\n",
    "lstm_autoencoder.compile(loss=losses.mean_squared_error, optimizer='adam')\n",
    "#lstm_autoencoder_500 = lstm_autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(x_train):\n",
    "    idx = 0\n",
    "    while True:\n",
    "        yield np.array([x_train[idx]]), np.array([x_train[idx]])\n",
    "        idx +=1\n",
    "        if idx >= len(x_train):\n",
    "            idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'models/weights' + '{epoch:02d}-{loss:.4f}.h5'\n",
    "early_stopping_callback = EarlyStopping(monitor='loss', patience=200)\n",
    "checkpoint_callback = ModelCheckpoint(model_path, monitor='loss', verbose = 1, save_best_only = True, save_weights_only = True, mode='min')#, period=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1004 17:02:53.719572 140183748400896 deprecation.py:323] From /home/minji/anaconda3/envs/graph/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W1004 17:02:55.021816 140183748400896 deprecation_wrapper.py:119] From /home/minji/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W1004 17:02:55.124544 140183748400896 deprecation_wrapper.py:119] From /home/minji/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/350\n",
      "2360/2360 [==============================] - 21s 9ms/step - loss: 38.3778\n",
      "\n",
      "Epoch 00001: loss improved from inf to 38.37784, saving model to models/weights01-38.3778.h5\n",
      "Epoch 2/350\n",
      "2360/2360 [==============================] - 20s 8ms/step - loss: 37.4693\n",
      "\n",
      "Epoch 00002: loss improved from 38.37784 to 37.46928, saving model to models/weights02-37.4693.h5\n",
      "Epoch 3/350\n",
      "2360/2360 [==============================] - 20s 8ms/step - loss: 20.1401\n",
      "\n",
      "Epoch 00003: loss improved from 37.46928 to 20.14012, saving model to models/weights03-20.1401.h5\n",
      "Epoch 4/350\n",
      "2360/2360 [==============================] - 20s 9ms/step - loss: 13.4845\n",
      "\n",
      "Epoch 00004: loss improved from 20.14012 to 13.48451, saving model to models/weights04-13.4845.h5\n",
      "Epoch 5/350\n",
      "2360/2360 [==============================] - 20s 8ms/step - loss: 11.7295\n",
      "\n",
      "Epoch 00005: loss improved from 13.48451 to 11.72947, saving model to models/weights05-11.7295.h5\n",
      "Epoch 6/350\n",
      "2360/2360 [==============================] - 20s 9ms/step - loss: 11.0322\n",
      "\n",
      "Epoch 00006: loss improved from 11.72947 to 11.03216, saving model to models/weights06-11.0322.h5\n",
      "Epoch 7/350\n",
      "2360/2360 [==============================] - 20s 9ms/step - loss: 10.6759\n",
      "\n",
      "Epoch 00007: loss improved from 11.03216 to 10.67590, saving model to models/weights07-10.6759.h5\n",
      "Epoch 8/350\n",
      "2360/2360 [==============================] - 20s 8ms/step - loss: 10.3362\n",
      "\n",
      "Epoch 00008: loss improved from 10.67590 to 10.33617, saving model to models/weights08-10.3362.h5\n",
      "Epoch 9/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 9.2114\n",
      "\n",
      "Epoch 00009: loss improved from 10.33617 to 9.21136, saving model to models/weights09-9.2114.h5\n",
      "Epoch 10/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 8.8903\n",
      "\n",
      "Epoch 00010: loss improved from 9.21136 to 8.89031, saving model to models/weights10-8.8903.h5\n",
      "Epoch 11/350\n",
      "2360/2360 [==============================] - 23s 10ms/step - loss: 8.5510\n",
      "\n",
      "Epoch 00011: loss improved from 8.89031 to 8.55100, saving model to models/weights11-8.5510.h5\n",
      "Epoch 12/350\n",
      "2360/2360 [==============================] - 23s 10ms/step - loss: 8.5668\n",
      "\n",
      "Epoch 00012: loss did not improve from 8.55100\n",
      "Epoch 13/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 7.6825\n",
      "\n",
      "Epoch 00013: loss improved from 8.55100 to 7.68252, saving model to models/weights13-7.6825.h5\n",
      "Epoch 14/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 6.8573\n",
      "\n",
      "Epoch 00014: loss improved from 7.68252 to 6.85732, saving model to models/weights14-6.8573.h5\n",
      "Epoch 15/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 6.3789\n",
      "\n",
      "Epoch 00015: loss improved from 6.85732 to 6.37895, saving model to models/weights15-6.3789.h5\n",
      "Epoch 16/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 5.8980\n",
      "\n",
      "Epoch 00016: loss improved from 6.37895 to 5.89801, saving model to models/weights16-5.8980.h5\n",
      "Epoch 17/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 5.5569\n",
      "\n",
      "Epoch 00017: loss improved from 5.89801 to 5.55691, saving model to models/weights17-5.5569.h5\n",
      "Epoch 18/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 4.9311\n",
      "\n",
      "Epoch 00018: loss improved from 5.55691 to 4.93111, saving model to models/weights18-4.9311.h5\n",
      "Epoch 19/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 4.8410\n",
      "\n",
      "Epoch 00019: loss improved from 4.93111 to 4.84105, saving model to models/weights19-4.8410.h5\n",
      "Epoch 20/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 4.3819\n",
      "\n",
      "Epoch 00020: loss improved from 4.84105 to 4.38191, saving model to models/weights20-4.3819.h5\n",
      "Epoch 21/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 4.3944\n",
      "\n",
      "Epoch 00021: loss did not improve from 4.38191\n",
      "Epoch 22/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 3.7658\n",
      "\n",
      "Epoch 00022: loss improved from 4.38191 to 3.76582, saving model to models/weights22-3.7658.h5\n",
      "Epoch 23/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 3.7113\n",
      "\n",
      "Epoch 00023: loss improved from 3.76582 to 3.71132, saving model to models/weights23-3.7113.h5\n",
      "Epoch 24/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 3.7291\n",
      "\n",
      "Epoch 00024: loss did not improve from 3.71132\n",
      "Epoch 25/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 3.8383\n",
      "\n",
      "Epoch 00025: loss did not improve from 3.71132\n",
      "Epoch 26/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 3.6492\n",
      "\n",
      "Epoch 00026: loss improved from 3.71132 to 3.64918, saving model to models/weights26-3.6492.h5\n",
      "Epoch 27/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 3.1219\n",
      "\n",
      "Epoch 00027: loss improved from 3.64918 to 3.12195, saving model to models/weights27-3.1219.h5\n",
      "Epoch 28/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 3.3083\n",
      "\n",
      "Epoch 00028: loss did not improve from 3.12195\n",
      "Epoch 29/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 3.5455\n",
      "\n",
      "Epoch 00029: loss did not improve from 3.12195\n",
      "Epoch 30/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 2.9634\n",
      "\n",
      "Epoch 00030: loss improved from 3.12195 to 2.96345, saving model to models/weights30-2.9634.h5\n",
      "Epoch 31/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 2.9239\n",
      "\n",
      "Epoch 00031: loss improved from 2.96345 to 2.92389, saving model to models/weights31-2.9239.h5\n",
      "Epoch 32/350\n",
      "2360/2360 [==============================] - 30s 13ms/step - loss: 3.0197\n",
      "\n",
      "Epoch 00032: loss did not improve from 2.92389\n",
      "Epoch 33/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 3.1279\n",
      "\n",
      "Epoch 00033: loss did not improve from 2.92389\n",
      "Epoch 34/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 2.8869\n",
      "\n",
      "Epoch 00034: loss improved from 2.92389 to 2.88687, saving model to models/weights34-2.8869.h5\n",
      "Epoch 35/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 2.7797\n",
      "\n",
      "Epoch 00035: loss improved from 2.88687 to 2.77972, saving model to models/weights35-2.7797.h5\n",
      "Epoch 36/350\n",
      "2360/2360 [==============================] - 31s 13ms/step - loss: 2.7126\n",
      "\n",
      "Epoch 00036: loss improved from 2.77972 to 2.71257, saving model to models/weights36-2.7126.h5\n",
      "Epoch 37/350\n",
      "2360/2360 [==============================] - 30s 13ms/step - loss: 2.7959\n",
      "\n",
      "Epoch 00037: loss did not improve from 2.71257\n",
      "Epoch 38/350\n",
      "2360/2360 [==============================] - 30s 13ms/step - loss: 2.3975\n",
      "\n",
      "Epoch 00038: loss improved from 2.71257 to 2.39745, saving model to models/weights38-2.3975.h5\n",
      "Epoch 39/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 2.4717\n",
      "\n",
      "Epoch 00039: loss did not improve from 2.39745\n",
      "Epoch 40/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 2.3921\n",
      "\n",
      "Epoch 00040: loss improved from 2.39745 to 2.39210, saving model to models/weights40-2.3921.h5\n",
      "Epoch 41/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 2.4868\n",
      "\n",
      "Epoch 00041: loss did not improve from 2.39210\n",
      "Epoch 42/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 2.2252\n",
      "\n",
      "Epoch 00042: loss improved from 2.39210 to 2.22516, saving model to models/weights42-2.2252.h5\n",
      "Epoch 43/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 2.0617\n",
      "\n",
      "Epoch 00043: loss improved from 2.22516 to 2.06173, saving model to models/weights43-2.0617.h5\n",
      "Epoch 44/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 2.5169\n",
      "\n",
      "Epoch 00044: loss did not improve from 2.06173\n",
      "Epoch 45/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 1.9343\n",
      "\n",
      "Epoch 00045: loss improved from 2.06173 to 1.93430, saving model to models/weights45-1.9343.h5\n",
      "Epoch 46/350\n",
      "2360/2360 [==============================] - 30s 13ms/step - loss: 2.0065\n",
      "\n",
      "Epoch 00046: loss did not improve from 1.93430\n",
      "Epoch 47/350\n",
      "2360/2360 [==============================] - 30s 13ms/step - loss: 1.9642\n",
      "\n",
      "Epoch 00047: loss did not improve from 1.93430\n",
      "Epoch 48/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 2.0353\n",
      "\n",
      "Epoch 00048: loss did not improve from 1.93430\n",
      "Epoch 49/350\n",
      "2360/2360 [==============================] - 30s 13ms/step - loss: 2.0041\n",
      "\n",
      "Epoch 00049: loss did not improve from 1.93430\n",
      "Epoch 50/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2360/2360 [==============================] - 29s 12ms/step - loss: 1.6914\n",
      "\n",
      "Epoch 00050: loss improved from 1.93430 to 1.69137, saving model to models/weights50-1.6914.h5\n",
      "Epoch 51/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 1.9980\n",
      "\n",
      "Epoch 00051: loss did not improve from 1.69137\n",
      "Epoch 52/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 2.0341\n",
      "\n",
      "Epoch 00052: loss did not improve from 1.69137\n",
      "Epoch 53/350\n",
      "2360/2360 [==============================] - 30s 13ms/step - loss: 1.6088\n",
      "\n",
      "Epoch 00053: loss improved from 1.69137 to 1.60879, saving model to models/weights53-1.6088.h5\n",
      "Epoch 54/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 1.5620\n",
      "\n",
      "Epoch 00054: loss improved from 1.60879 to 1.56198, saving model to models/weights54-1.5620.h5\n",
      "Epoch 55/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 1.8510\n",
      "\n",
      "Epoch 00055: loss did not improve from 1.56198\n",
      "Epoch 56/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 1.4714\n",
      "\n",
      "Epoch 00056: loss improved from 1.56198 to 1.47143, saving model to models/weights56-1.4714.h5\n",
      "Epoch 57/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 1.8746\n",
      "\n",
      "Epoch 00057: loss did not improve from 1.47143\n",
      "Epoch 58/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 1.6763\n",
      "\n",
      "Epoch 00058: loss did not improve from 1.47143\n",
      "Epoch 59/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 1.6744\n",
      "\n",
      "Epoch 00059: loss did not improve from 1.47143\n",
      "Epoch 60/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 1.5385\n",
      "\n",
      "Epoch 00060: loss did not improve from 1.47143\n",
      "Epoch 61/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 1.6308\n",
      "\n",
      "Epoch 00061: loss did not improve from 1.47143\n",
      "Epoch 62/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 1.4342\n",
      "\n",
      "Epoch 00062: loss improved from 1.47143 to 1.43418, saving model to models/weights62-1.4342.h5\n",
      "Epoch 63/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 1.2912\n",
      "\n",
      "Epoch 00063: loss improved from 1.43418 to 1.29120, saving model to models/weights63-1.2912.h5\n",
      "Epoch 64/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 1.2670\n",
      "\n",
      "Epoch 00064: loss improved from 1.29120 to 1.26701, saving model to models/weights64-1.2670.h5\n",
      "Epoch 65/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 1.2417\n",
      "\n",
      "Epoch 00065: loss improved from 1.26701 to 1.24167, saving model to models/weights65-1.2417.h5\n",
      "Epoch 66/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 1.3619\n",
      "\n",
      "Epoch 00066: loss did not improve from 1.24167\n",
      "Epoch 67/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 1.1349\n",
      "\n",
      "Epoch 00067: loss improved from 1.24167 to 1.13488, saving model to models/weights67-1.1349.h5\n",
      "Epoch 68/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 1.3537\n",
      "\n",
      "Epoch 00068: loss did not improve from 1.13488\n",
      "Epoch 69/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 1.3643\n",
      "\n",
      "Epoch 00069: loss did not improve from 1.13488\n",
      "Epoch 70/350\n",
      "2360/2360 [==============================] - 27s 12ms/step - loss: 1.3334\n",
      "\n",
      "Epoch 00070: loss did not improve from 1.13488\n",
      "Epoch 71/350\n",
      "2360/2360 [==============================] - 30s 13ms/step - loss: 1.2542\n",
      "\n",
      "Epoch 00071: loss did not improve from 1.13488\n",
      "Epoch 72/350\n",
      "2360/2360 [==============================] - 27s 12ms/step - loss: 1.1274\n",
      "\n",
      "Epoch 00072: loss improved from 1.13488 to 1.12742, saving model to models/weights72-1.1274.h5\n",
      "Epoch 73/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 1.1824\n",
      "\n",
      "Epoch 00073: loss did not improve from 1.12742\n",
      "Epoch 74/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.9704\n",
      "\n",
      "Epoch 00074: loss improved from 1.12742 to 0.97043, saving model to models/weights74-0.9704.h5\n",
      "Epoch 75/350\n",
      "2360/2360 [==============================] - 27s 12ms/step - loss: 0.8773\n",
      "\n",
      "Epoch 00075: loss improved from 0.97043 to 0.87727, saving model to models/weights75-0.8773.h5\n",
      "Epoch 76/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 1.2333\n",
      "\n",
      "Epoch 00076: loss did not improve from 0.87727\n",
      "Epoch 77/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.8324\n",
      "\n",
      "Epoch 00077: loss improved from 0.87727 to 0.83242, saving model to models/weights77-0.8324.h5\n",
      "Epoch 78/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 1.0231\n",
      "\n",
      "Epoch 00078: loss did not improve from 0.83242\n",
      "Epoch 79/350\n",
      "2360/2360 [==============================] - 27s 12ms/step - loss: 0.9823\n",
      "\n",
      "Epoch 00079: loss did not improve from 0.83242\n",
      "Epoch 80/350\n",
      "2360/2360 [==============================] - 27s 12ms/step - loss: 1.1780\n",
      "\n",
      "Epoch 00080: loss did not improve from 0.83242\n",
      "Epoch 81/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.9910\n",
      "\n",
      "Epoch 00081: loss did not improve from 0.83242\n",
      "Epoch 82/350\n",
      "2360/2360 [==============================] - 27s 12ms/step - loss: 1.3861\n",
      "\n",
      "Epoch 00082: loss did not improve from 0.83242\n",
      "Epoch 83/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 1.0881\n",
      "\n",
      "Epoch 00083: loss did not improve from 0.83242\n",
      "Epoch 84/350\n",
      "2360/2360 [==============================] - 31s 13ms/step - loss: 0.9377\n",
      "\n",
      "Epoch 00084: loss did not improve from 0.83242\n",
      "Epoch 85/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.8617\n",
      "\n",
      "Epoch 00085: loss did not improve from 0.83242\n",
      "Epoch 86/350\n",
      "2360/2360 [==============================] - 32s 13ms/step - loss: 1.2228\n",
      "\n",
      "Epoch 00086: loss did not improve from 0.83242\n",
      "Epoch 87/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.8384\n",
      "\n",
      "Epoch 00087: loss did not improve from 0.83242\n",
      "Epoch 88/350\n",
      "2360/2360 [==============================] - 27s 11ms/step - loss: 0.9214\n",
      "\n",
      "Epoch 00088: loss did not improve from 0.83242\n",
      "Epoch 89/350\n",
      "2360/2360 [==============================] - 27s 11ms/step - loss: 0.9020\n",
      "\n",
      "Epoch 00089: loss did not improve from 0.83242\n",
      "Epoch 90/350\n",
      "2360/2360 [==============================] - 27s 11ms/step - loss: 0.8915\n",
      "\n",
      "Epoch 00090: loss did not improve from 0.83242\n",
      "Epoch 91/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 1.3699\n",
      "\n",
      "Epoch 00091: loss did not improve from 0.83242\n",
      "Epoch 92/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.8060\n",
      "\n",
      "Epoch 00092: loss improved from 0.83242 to 0.80603, saving model to models/weights92-0.8060.h5\n",
      "Epoch 93/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.9034\n",
      "\n",
      "Epoch 00093: loss did not improve from 0.80603\n",
      "Epoch 94/350\n",
      "2360/2360 [==============================] - 27s 12ms/step - loss: 0.8476\n",
      "\n",
      "Epoch 00094: loss did not improve from 0.80603\n",
      "Epoch 95/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.6852\n",
      "\n",
      "Epoch 00095: loss improved from 0.80603 to 0.68518, saving model to models/weights95-0.6852.h5\n",
      "Epoch 96/350\n",
      "2360/2360 [==============================] - 27s 11ms/step - loss: 0.9461\n",
      "\n",
      "Epoch 00096: loss did not improve from 0.68518\n",
      "Epoch 97/350\n",
      "2360/2360 [==============================] - 27s 11ms/step - loss: 0.6251\n",
      "\n",
      "Epoch 00097: loss improved from 0.68518 to 0.62506, saving model to models/weights97-0.6251.h5\n",
      "Epoch 98/350\n",
      "2360/2360 [==============================] - 30s 13ms/step - loss: 0.9694\n",
      "\n",
      "Epoch 00098: loss did not improve from 0.62506\n",
      "Epoch 99/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.8713\n",
      "\n",
      "Epoch 00099: loss did not improve from 0.62506\n",
      "Epoch 100/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.6489\n",
      "\n",
      "Epoch 00100: loss did not improve from 0.62506\n",
      "Epoch 101/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.7944\n",
      "\n",
      "Epoch 00101: loss did not improve from 0.62506\n",
      "Epoch 102/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.9691\n",
      "\n",
      "Epoch 00102: loss did not improve from 0.62506\n",
      "Epoch 103/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.5506\n",
      "\n",
      "Epoch 00103: loss improved from 0.62506 to 0.55058, saving model to models/weights103-0.5506.h5\n",
      "Epoch 104/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.5459\n",
      "\n",
      "Epoch 00104: loss improved from 0.55058 to 0.54588, saving model to models/weights104-0.5459.h5\n",
      "Epoch 105/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.5803\n",
      "\n",
      "Epoch 00105: loss did not improve from 0.54588\n",
      "Epoch 106/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.9095\n",
      "\n",
      "Epoch 00106: loss did not improve from 0.54588\n",
      "Epoch 107/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.7965\n",
      "\n",
      "Epoch 00107: loss did not improve from 0.54588\n",
      "Epoch 108/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.5807\n",
      "\n",
      "Epoch 00108: loss did not improve from 0.54588\n",
      "Epoch 109/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.7437\n",
      "\n",
      "Epoch 00109: loss did not improve from 0.54588\n",
      "Epoch 110/350\n",
      "2360/2360 [==============================] - 31s 13ms/step - loss: 0.6975\n",
      "\n",
      "Epoch 00110: loss did not improve from 0.54588\n",
      "Epoch 111/350\n",
      "2360/2360 [==============================] - 27s 12ms/step - loss: 0.6215\n",
      "\n",
      "Epoch 00111: loss did not improve from 0.54588\n",
      "Epoch 112/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.9310\n",
      "\n",
      "Epoch 00112: loss did not improve from 0.54588\n",
      "Epoch 113/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.6151\n",
      "\n",
      "Epoch 00113: loss did not improve from 0.54588\n",
      "Epoch 114/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.6740\n",
      "\n",
      "Epoch 00114: loss did not improve from 0.54588\n",
      "Epoch 115/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.4907\n",
      "\n",
      "Epoch 00115: loss improved from 0.54588 to 0.49066, saving model to models/weights115-0.4907.h5\n",
      "Epoch 116/350\n",
      "2360/2360 [==============================] - 27s 12ms/step - loss: 0.4667\n",
      "\n",
      "Epoch 00116: loss improved from 0.49066 to 0.46674, saving model to models/weights116-0.4667.h5\n",
      "Epoch 117/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.5234\n",
      "\n",
      "Epoch 00117: loss did not improve from 0.46674\n",
      "Epoch 118/350\n",
      "2360/2360 [==============================] - 27s 12ms/step - loss: 0.4612\n",
      "\n",
      "Epoch 00118: loss improved from 0.46674 to 0.46119, saving model to models/weights118-0.4612.h5\n",
      "Epoch 119/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.4817\n",
      "\n",
      "Epoch 00119: loss did not improve from 0.46119\n",
      "Epoch 120/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.4203\n",
      "\n",
      "Epoch 00120: loss improved from 0.46119 to 0.42030, saving model to models/weights120-0.4203.h5\n",
      "Epoch 121/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.5749\n",
      "\n",
      "Epoch 00121: loss did not improve from 0.42030\n",
      "Epoch 122/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.4264\n",
      "\n",
      "Epoch 00122: loss did not improve from 0.42030\n",
      "Epoch 123/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.5408\n",
      "\n",
      "Epoch 00123: loss did not improve from 0.42030\n",
      "Epoch 124/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.8264\n",
      "\n",
      "Epoch 00124: loss did not improve from 0.42030\n",
      "Epoch 125/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.5106\n",
      "\n",
      "Epoch 00125: loss did not improve from 0.42030\n",
      "Epoch 126/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.3909\n",
      "\n",
      "Epoch 00126: loss improved from 0.42030 to 0.39088, saving model to models/weights126-0.3909.h5\n",
      "Epoch 127/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.4642\n",
      "\n",
      "Epoch 00127: loss did not improve from 0.39088\n",
      "Epoch 128/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.3933\n",
      "\n",
      "Epoch 00128: loss did not improve from 0.39088\n",
      "Epoch 129/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.3858\n",
      "\n",
      "Epoch 00129: loss improved from 0.39088 to 0.38577, saving model to models/weights129-0.3858.h5\n",
      "Epoch 130/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.3861\n",
      "\n",
      "Epoch 00130: loss did not improve from 0.38577\n",
      "Epoch 131/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.4015\n",
      "\n",
      "Epoch 00131: loss did not improve from 0.38577\n",
      "Epoch 132/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.4061\n",
      "\n",
      "Epoch 00132: loss did not improve from 0.38577\n",
      "Epoch 133/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.3458\n",
      "\n",
      "Epoch 00133: loss improved from 0.38577 to 0.34578, saving model to models/weights133-0.3458.h5\n",
      "Epoch 134/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.3797\n",
      "\n",
      "Epoch 00134: loss did not improve from 0.34578\n",
      "Epoch 135/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.4044\n",
      "\n",
      "Epoch 00135: loss did not improve from 0.34578\n",
      "Epoch 136/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.6133\n",
      "\n",
      "Epoch 00136: loss did not improve from 0.34578\n",
      "Epoch 137/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.5319\n",
      "\n",
      "Epoch 00137: loss did not improve from 0.34578\n",
      "Epoch 138/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.7014\n",
      "\n",
      "Epoch 00138: loss did not improve from 0.34578\n",
      "Epoch 139/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.4698\n",
      "\n",
      "Epoch 00139: loss did not improve from 0.34578\n",
      "Epoch 140/350\n",
      "2360/2360 [==============================] - 31s 13ms/step - loss: 0.3013\n",
      "\n",
      "Epoch 00140: loss improved from 0.34578 to 0.30129, saving model to models/weights140-0.3013.h5\n",
      "Epoch 141/350\n",
      "2360/2360 [==============================] - 27s 12ms/step - loss: 0.3379\n",
      "\n",
      "Epoch 00141: loss did not improve from 0.30129\n",
      "Epoch 142/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.3655\n",
      "\n",
      "Epoch 00142: loss did not improve from 0.30129\n",
      "Epoch 143/350\n",
      "2360/2360 [==============================] - 27s 11ms/step - loss: 0.3259\n",
      "\n",
      "Epoch 00143: loss did not improve from 0.30129\n",
      "Epoch 144/350\n",
      "2360/2360 [==============================] - 27s 11ms/step - loss: 0.3322\n",
      "\n",
      "Epoch 00144: loss did not improve from 0.30129\n",
      "Epoch 145/350\n",
      "2360/2360 [==============================] - 27s 11ms/step - loss: 0.4264\n",
      "\n",
      "Epoch 00145: loss did not improve from 0.30129\n",
      "Epoch 146/350\n",
      "2360/2360 [==============================] - 27s 12ms/step - loss: 0.4021\n",
      "\n",
      "Epoch 00146: loss did not improve from 0.30129\n",
      "Epoch 147/350\n",
      "2360/2360 [==============================] - 27s 12ms/step - loss: 0.3220\n",
      "\n",
      "Epoch 00147: loss did not improve from 0.30129\n",
      "Epoch 148/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.3588\n",
      "\n",
      "Epoch 00148: loss did not improve from 0.30129\n",
      "Epoch 149/350\n",
      "2360/2360 [==============================] - 30s 13ms/step - loss: 0.3692\n",
      "\n",
      "Epoch 00149: loss did not improve from 0.30129\n",
      "Epoch 150/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.2646\n",
      "\n",
      "Epoch 00150: loss improved from 0.30129 to 0.26459, saving model to models/weights150-0.2646.h5\n",
      "Epoch 151/350\n",
      "2360/2360 [==============================] - 30s 13ms/step - loss: 0.3249\n",
      "\n",
      "Epoch 00151: loss did not improve from 0.26459\n",
      "Epoch 152/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.3354\n",
      "\n",
      "Epoch 00152: loss did not improve from 0.26459\n",
      "Epoch 153/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.4038\n",
      "\n",
      "Epoch 00153: loss did not improve from 0.26459\n",
      "Epoch 154/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.3681\n",
      "\n",
      "Epoch 00154: loss did not improve from 0.26459\n",
      "Epoch 155/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.3392\n",
      "\n",
      "Epoch 00155: loss did not improve from 0.26459\n",
      "Epoch 156/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.2795\n",
      "\n",
      "Epoch 00156: loss did not improve from 0.26459\n",
      "Epoch 157/350\n",
      "2360/2360 [==============================] - 30s 13ms/step - loss: 0.3140\n",
      "\n",
      "Epoch 00157: loss did not improve from 0.26459\n",
      "Epoch 158/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.4696\n",
      "\n",
      "Epoch 00158: loss did not improve from 0.26459\n",
      "Epoch 159/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.2204\n",
      "\n",
      "Epoch 00159: loss improved from 0.26459 to 0.22042, saving model to models/weights159-0.2204.h5\n",
      "Epoch 160/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.2646\n",
      "\n",
      "Epoch 00160: loss did not improve from 0.22042\n",
      "Epoch 161/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.2612\n",
      "\n",
      "Epoch 00161: loss did not improve from 0.22042\n",
      "Epoch 162/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.5173\n",
      "\n",
      "Epoch 00162: loss did not improve from 0.22042\n",
      "Epoch 163/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.3351\n",
      "\n",
      "Epoch 00163: loss did not improve from 0.22042\n",
      "Epoch 164/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.4883\n",
      "\n",
      "Epoch 00164: loss did not improve from 0.22042\n",
      "Epoch 165/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.3194\n",
      "\n",
      "Epoch 00165: loss did not improve from 0.22042\n",
      "Epoch 166/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.4231\n",
      "\n",
      "Epoch 00166: loss did not improve from 0.22042\n",
      "Epoch 167/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.3425\n",
      "\n",
      "Epoch 00167: loss did not improve from 0.22042\n",
      "Epoch 168/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.4977\n",
      "\n",
      "Epoch 00168: loss did not improve from 0.22042\n",
      "Epoch 169/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.3489\n",
      "\n",
      "Epoch 00169: loss did not improve from 0.22042\n",
      "Epoch 170/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.2280\n",
      "\n",
      "Epoch 00170: loss did not improve from 0.22042\n",
      "Epoch 171/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.2715\n",
      "\n",
      "Epoch 00171: loss did not improve from 0.22042\n",
      "Epoch 172/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.2653\n",
      "\n",
      "Epoch 00172: loss did not improve from 0.22042\n",
      "Epoch 173/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.4900\n",
      "\n",
      "Epoch 00173: loss did not improve from 0.22042\n",
      "Epoch 174/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.2164\n",
      "\n",
      "Epoch 00174: loss improved from 0.22042 to 0.21642, saving model to models/weights174-0.2164.h5\n",
      "Epoch 175/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.2529\n",
      "\n",
      "Epoch 00175: loss did not improve from 0.21642\n",
      "Epoch 176/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.2831\n",
      "\n",
      "Epoch 00176: loss did not improve from 0.21642\n",
      "Epoch 177/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.2275\n",
      "\n",
      "Epoch 00177: loss did not improve from 0.21642\n",
      "Epoch 178/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.5122\n",
      "\n",
      "Epoch 00178: loss did not improve from 0.21642\n",
      "Epoch 179/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.6409\n",
      "\n",
      "Epoch 00179: loss did not improve from 0.21642\n",
      "Epoch 180/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.6424\n",
      "\n",
      "Epoch 00180: loss did not improve from 0.21642\n",
      "Epoch 181/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.2658\n",
      "\n",
      "Epoch 00181: loss did not improve from 0.21642\n",
      "Epoch 182/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.1861\n",
      "\n",
      "Epoch 00182: loss improved from 0.21642 to 0.18614, saving model to models/weights182-0.1861.h5\n",
      "Epoch 183/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.2494\n",
      "\n",
      "Epoch 00183: loss did not improve from 0.18614\n",
      "Epoch 184/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.2402\n",
      "\n",
      "Epoch 00184: loss did not improve from 0.18614\n",
      "Epoch 185/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.3434\n",
      "\n",
      "Epoch 00185: loss did not improve from 0.18614\n",
      "Epoch 186/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.1986\n",
      "\n",
      "Epoch 00186: loss did not improve from 0.18614\n",
      "Epoch 187/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.2334\n",
      "\n",
      "Epoch 00187: loss did not improve from 0.18614\n",
      "Epoch 188/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.3283\n",
      "\n",
      "Epoch 00188: loss did not improve from 0.18614\n",
      "Epoch 189/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.1733\n",
      "\n",
      "Epoch 00189: loss improved from 0.18614 to 0.17335, saving model to models/weights189-0.1733.h5\n",
      "Epoch 190/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.2369\n",
      "\n",
      "Epoch 00190: loss did not improve from 0.17335\n",
      "Epoch 191/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.2340\n",
      "\n",
      "Epoch 00191: loss did not improve from 0.17335\n",
      "Epoch 192/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.4671\n",
      "\n",
      "Epoch 00192: loss did not improve from 0.17335\n",
      "Epoch 193/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.5053\n",
      "\n",
      "Epoch 00193: loss did not improve from 0.17335\n",
      "Epoch 194/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.6818\n",
      "\n",
      "Epoch 00194: loss did not improve from 0.17335\n",
      "Epoch 195/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.4635\n",
      "\n",
      "Epoch 00195: loss did not improve from 0.17335\n",
      "Epoch 196/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.4143\n",
      "\n",
      "Epoch 00196: loss did not improve from 0.17335\n",
      "Epoch 197/350\n",
      "2360/2360 [==============================] - 32s 13ms/step - loss: 0.2787\n",
      "\n",
      "Epoch 00197: loss did not improve from 0.17335\n",
      "Epoch 198/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.2548\n",
      "\n",
      "Epoch 00198: loss did not improve from 0.17335\n",
      "Epoch 199/350\n",
      "2360/2360 [==============================] - 27s 11ms/step - loss: 0.5581\n",
      "\n",
      "Epoch 00199: loss did not improve from 0.17335\n",
      "Epoch 200/350\n",
      "2360/2360 [==============================] - 27s 11ms/step - loss: 0.3385\n",
      "\n",
      "Epoch 00200: loss did not improve from 0.17335\n",
      "Epoch 201/350\n",
      "2360/2360 [==============================] - 27s 12ms/step - loss: 0.2062\n",
      "\n",
      "Epoch 00201: loss did not improve from 0.17335\n",
      "Epoch 202/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.2479\n",
      "\n",
      "Epoch 00202: loss did not improve from 0.17335\n",
      "Epoch 203/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.2307\n",
      "\n",
      "Epoch 00203: loss did not improve from 0.17335\n",
      "Epoch 204/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.2386\n",
      "\n",
      "Epoch 00204: loss did not improve from 0.17335\n",
      "Epoch 205/350\n",
      "2360/2360 [==============================] - 32s 13ms/step - loss: 0.2059\n",
      "\n",
      "Epoch 00205: loss did not improve from 0.17335\n",
      "Epoch 206/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.2577\n",
      "\n",
      "Epoch 00206: loss did not improve from 0.17335\n",
      "Epoch 207/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.2080\n",
      "\n",
      "Epoch 00207: loss did not improve from 0.17335\n",
      "Epoch 208/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.2173\n",
      "\n",
      "Epoch 00208: loss did not improve from 0.17335\n",
      "Epoch 209/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.2309\n",
      "\n",
      "Epoch 00209: loss did not improve from 0.17335\n",
      "Epoch 210/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1945\n",
      "\n",
      "Epoch 00210: loss did not improve from 0.17335\n",
      "Epoch 211/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.2004\n",
      "\n",
      "Epoch 00211: loss did not improve from 0.17335\n",
      "Epoch 212/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.3363\n",
      "\n",
      "Epoch 00212: loss did not improve from 0.17335\n",
      "Epoch 213/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.4076\n",
      "\n",
      "Epoch 00213: loss did not improve from 0.17335\n",
      "Epoch 214/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.2769\n",
      "\n",
      "Epoch 00214: loss did not improve from 0.17335\n",
      "Epoch 215/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.2693\n",
      "\n",
      "Epoch 00215: loss did not improve from 0.17335\n",
      "Epoch 216/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.1897\n",
      "\n",
      "Epoch 00216: loss did not improve from 0.17335\n",
      "Epoch 217/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.1784\n",
      "\n",
      "Epoch 00217: loss did not improve from 0.17335\n",
      "Epoch 218/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.2289\n",
      "\n",
      "Epoch 00218: loss did not improve from 0.17335\n",
      "Epoch 219/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1888\n",
      "\n",
      "Epoch 00219: loss did not improve from 0.17335\n",
      "Epoch 220/350\n",
      "2360/2360 [==============================] - 30s 13ms/step - loss: 0.2356\n",
      "\n",
      "Epoch 00220: loss did not improve from 0.17335\n",
      "Epoch 221/350\n",
      "2360/2360 [==============================] - 34s 14ms/step - loss: 0.1849\n",
      "\n",
      "Epoch 00221: loss did not improve from 0.17335\n",
      "Epoch 222/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.2050\n",
      "\n",
      "Epoch 00222: loss did not improve from 0.17335\n",
      "Epoch 223/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1809\n",
      "\n",
      "Epoch 00223: loss did not improve from 0.17335\n",
      "Epoch 224/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.1937\n",
      "\n",
      "Epoch 00224: loss did not improve from 0.17335\n",
      "Epoch 225/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.1940\n",
      "\n",
      "Epoch 00225: loss did not improve from 0.17335\n",
      "Epoch 226/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1733\n",
      "\n",
      "Epoch 00226: loss improved from 0.17335 to 0.17325, saving model to models/weights226-0.1733.h5\n",
      "Epoch 227/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1960\n",
      "\n",
      "Epoch 00227: loss did not improve from 0.17325\n",
      "Epoch 228/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.2020\n",
      "\n",
      "Epoch 00228: loss did not improve from 0.17325\n",
      "Epoch 229/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.2009\n",
      "\n",
      "Epoch 00229: loss did not improve from 0.17325\n",
      "Epoch 230/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1958\n",
      "\n",
      "Epoch 00230: loss did not improve from 0.17325\n",
      "Epoch 231/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.2452\n",
      "\n",
      "Epoch 00231: loss did not improve from 0.17325\n",
      "Epoch 232/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1471\n",
      "\n",
      "Epoch 00232: loss improved from 0.17325 to 0.14706, saving model to models/weights232-0.1471.h5\n",
      "Epoch 233/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.1799\n",
      "\n",
      "Epoch 00233: loss did not improve from 0.14706\n",
      "Epoch 234/350\n",
      "2360/2360 [==============================] - 30s 13ms/step - loss: 0.1695\n",
      "\n",
      "Epoch 00234: loss did not improve from 0.14706\n",
      "Epoch 235/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1794\n",
      "\n",
      "Epoch 00235: loss did not improve from 0.14706\n",
      "Epoch 236/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1712\n",
      "\n",
      "Epoch 00236: loss did not improve from 0.14706\n",
      "Epoch 237/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.1803\n",
      "\n",
      "Epoch 00237: loss did not improve from 0.14706\n",
      "Epoch 238/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.1791\n",
      "\n",
      "Epoch 00238: loss did not improve from 0.14706\n",
      "Epoch 239/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.2718\n",
      "\n",
      "Epoch 00239: loss did not improve from 0.14706\n",
      "Epoch 240/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.1379\n",
      "\n",
      "Epoch 00240: loss improved from 0.14706 to 0.13792, saving model to models/weights240-0.1379.h5\n",
      "Epoch 241/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.2229\n",
      "\n",
      "Epoch 00241: loss did not improve from 0.13792\n",
      "Epoch 242/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.1429\n",
      "\n",
      "Epoch 00242: loss did not improve from 0.13792\n",
      "Epoch 243/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1582\n",
      "\n",
      "Epoch 00243: loss did not improve from 0.13792\n",
      "Epoch 244/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.5085\n",
      "\n",
      "Epoch 00244: loss did not improve from 0.13792\n",
      "Epoch 245/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.1693\n",
      "\n",
      "Epoch 00245: loss did not improve from 0.13792\n",
      "Epoch 246/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1200\n",
      "\n",
      "Epoch 00246: loss improved from 0.13792 to 0.12002, saving model to models/weights246-0.1200.h5\n",
      "Epoch 247/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1666\n",
      "\n",
      "Epoch 00247: loss did not improve from 0.12002\n",
      "Epoch 248/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.1517\n",
      "\n",
      "Epoch 00248: loss did not improve from 0.12002\n",
      "Epoch 249/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.2023\n",
      "\n",
      "Epoch 00249: loss did not improve from 0.12002\n",
      "Epoch 250/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.1475\n",
      "\n",
      "Epoch 00250: loss did not improve from 0.12002\n",
      "Epoch 251/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.2377\n",
      "\n",
      "Epoch 00251: loss did not improve from 0.12002\n",
      "Epoch 252/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.1584\n",
      "\n",
      "Epoch 00252: loss did not improve from 0.12002\n",
      "Epoch 253/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1425\n",
      "\n",
      "Epoch 00253: loss did not improve from 0.12002\n",
      "Epoch 254/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1658\n",
      "\n",
      "Epoch 00254: loss did not improve from 0.12002\n",
      "Epoch 255/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.1535\n",
      "\n",
      "Epoch 00255: loss did not improve from 0.12002\n",
      "Epoch 256/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.1498\n",
      "\n",
      "Epoch 00256: loss did not improve from 0.12002\n",
      "Epoch 257/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.1504\n",
      "\n",
      "Epoch 00257: loss did not improve from 0.12002\n",
      "Epoch 258/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1502\n",
      "\n",
      "Epoch 00258: loss did not improve from 0.12002\n",
      "Epoch 259/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1505\n",
      "\n",
      "Epoch 00259: loss did not improve from 0.12002\n",
      "Epoch 260/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1548\n",
      "\n",
      "Epoch 00260: loss did not improve from 0.12002\n",
      "Epoch 261/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.1498\n",
      "\n",
      "Epoch 00261: loss did not improve from 0.12002\n",
      "Epoch 262/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.1392\n",
      "\n",
      "Epoch 00262: loss did not improve from 0.12002\n",
      "Epoch 263/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1363\n",
      "\n",
      "Epoch 00263: loss did not improve from 0.12002\n",
      "Epoch 264/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.2001\n",
      "\n",
      "Epoch 00264: loss did not improve from 0.12002\n",
      "Epoch 265/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.1337\n",
      "\n",
      "Epoch 00265: loss did not improve from 0.12002\n",
      "Epoch 266/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.1544\n",
      "\n",
      "Epoch 00266: loss did not improve from 0.12002\n",
      "Epoch 267/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.1674\n",
      "\n",
      "Epoch 00267: loss did not improve from 0.12002\n",
      "Epoch 268/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1243\n",
      "\n",
      "Epoch 00268: loss did not improve from 0.12002\n",
      "Epoch 269/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1543\n",
      "\n",
      "Epoch 00269: loss did not improve from 0.12002\n",
      "Epoch 270/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1305\n",
      "\n",
      "Epoch 00270: loss did not improve from 0.12002\n",
      "Epoch 271/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.1314\n",
      "\n",
      "Epoch 00271: loss did not improve from 0.12002\n",
      "Epoch 272/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1512\n",
      "\n",
      "Epoch 00272: loss did not improve from 0.12002\n",
      "Epoch 273/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.1656\n",
      "\n",
      "Epoch 00273: loss did not improve from 0.12002\n",
      "Epoch 274/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1203\n",
      "\n",
      "Epoch 00274: loss did not improve from 0.12002\n",
      "Epoch 275/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1868\n",
      "\n",
      "Epoch 00275: loss did not improve from 0.12002\n",
      "Epoch 276/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.2884\n",
      "\n",
      "Epoch 00276: loss did not improve from 0.12002\n",
      "Epoch 277/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1184\n",
      "\n",
      "Epoch 00277: loss improved from 0.12002 to 0.11836, saving model to models/weights277-0.1184.h5\n",
      "Epoch 278/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1072\n",
      "\n",
      "Epoch 00278: loss improved from 0.11836 to 0.10716, saving model to models/weights278-0.1072.h5\n",
      "Epoch 279/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1328\n",
      "\n",
      "Epoch 00279: loss did not improve from 0.10716\n",
      "Epoch 280/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1929\n",
      "\n",
      "Epoch 00280: loss did not improve from 0.10716\n",
      "Epoch 281/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.2501\n",
      "\n",
      "Epoch 00281: loss did not improve from 0.10716\n",
      "Epoch 282/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1900\n",
      "\n",
      "Epoch 00282: loss did not improve from 0.10716\n",
      "Epoch 283/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1367\n",
      "\n",
      "Epoch 00283: loss did not improve from 0.10716\n",
      "Epoch 284/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1179\n",
      "\n",
      "Epoch 00284: loss did not improve from 0.10716\n",
      "Epoch 285/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1256\n",
      "\n",
      "Epoch 00285: loss did not improve from 0.10716\n",
      "Epoch 286/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1428\n",
      "\n",
      "Epoch 00286: loss did not improve from 0.10716\n",
      "Epoch 287/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1329\n",
      "\n",
      "Epoch 00287: loss did not improve from 0.10716\n",
      "Epoch 288/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1615\n",
      "\n",
      "Epoch 00288: loss did not improve from 0.10716\n",
      "Epoch 289/350\n",
      "2360/2360 [==============================] - 20s 8ms/step - loss: 0.1270\n",
      "\n",
      "Epoch 00289: loss did not improve from 0.10716\n",
      "Epoch 290/350\n",
      "2360/2360 [==============================] - 21s 9ms/step - loss: 0.1907\n",
      "\n",
      "Epoch 00290: loss did not improve from 0.10716\n",
      "Epoch 291/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.1306\n",
      "\n",
      "Epoch 00291: loss did not improve from 0.10716\n",
      "Epoch 292/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.1268\n",
      "\n",
      "Epoch 00292: loss did not improve from 0.10716\n",
      "Epoch 293/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.1627\n",
      "\n",
      "Epoch 00293: loss did not improve from 0.10716\n",
      "Epoch 294/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.1268\n",
      "\n",
      "Epoch 00294: loss did not improve from 0.10716\n",
      "Epoch 295/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1933\n",
      "\n",
      "Epoch 00295: loss did not improve from 0.10716\n",
      "Epoch 296/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.1145\n",
      "\n",
      "Epoch 00296: loss did not improve from 0.10716\n",
      "Epoch 297/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1352\n",
      "\n",
      "Epoch 00297: loss did not improve from 0.10716\n",
      "Epoch 298/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.1339\n",
      "\n",
      "Epoch 00298: loss did not improve from 0.10716\n",
      "Epoch 299/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.2006\n",
      "\n",
      "Epoch 00299: loss did not improve from 0.10716\n",
      "Epoch 300/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.1121\n",
      "\n",
      "Epoch 00300: loss did not improve from 0.10716\n",
      "Epoch 301/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.1207\n",
      "\n",
      "Epoch 00301: loss did not improve from 0.10716\n",
      "Epoch 302/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.1282\n",
      "\n",
      "Epoch 00302: loss did not improve from 0.10716\n",
      "Epoch 303/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.1452\n",
      "\n",
      "Epoch 00303: loss did not improve from 0.10716\n",
      "Epoch 304/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1238\n",
      "\n",
      "Epoch 00304: loss did not improve from 0.10716\n",
      "Epoch 305/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1254\n",
      "\n",
      "Epoch 00305: loss did not improve from 0.10716\n",
      "Epoch 306/350\n",
      "2360/2360 [==============================] - 33s 14ms/step - loss: 0.1459\n",
      "\n",
      "Epoch 00306: loss did not improve from 0.10716\n",
      "Epoch 307/350\n",
      "2360/2360 [==============================] - 30s 13ms/step - loss: 0.1348\n",
      "\n",
      "Epoch 00307: loss did not improve from 0.10716\n",
      "Epoch 308/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1066\n",
      "\n",
      "Epoch 00308: loss improved from 0.10716 to 0.10662, saving model to models/weights308-0.1066.h5\n",
      "Epoch 309/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1475\n",
      "\n",
      "Epoch 00309: loss did not improve from 0.10662\n",
      "Epoch 310/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1030\n",
      "\n",
      "Epoch 00310: loss improved from 0.10662 to 0.10300, saving model to models/weights310-0.1030.h5\n",
      "Epoch 311/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1169\n",
      "\n",
      "Epoch 00311: loss did not improve from 0.10300\n",
      "Epoch 312/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1262\n",
      "\n",
      "Epoch 00312: loss did not improve from 0.10300\n",
      "Epoch 313/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1229\n",
      "\n",
      "Epoch 00313: loss did not improve from 0.10300\n",
      "Epoch 314/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.5070\n",
      "\n",
      "Epoch 00314: loss did not improve from 0.10300\n",
      "Epoch 315/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.3236\n",
      "\n",
      "Epoch 00315: loss did not improve from 0.10300\n",
      "Epoch 316/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.2580\n",
      "\n",
      "Epoch 00316: loss did not improve from 0.10300\n",
      "Epoch 317/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1109\n",
      "\n",
      "Epoch 00317: loss did not improve from 0.10300\n",
      "Epoch 318/350\n",
      "2360/2360 [==============================] - 30s 13ms/step - loss: 0.0894\n",
      "\n",
      "Epoch 00318: loss improved from 0.10300 to 0.08943, saving model to models/weights318-0.0894.h5\n",
      "Epoch 319/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1753\n",
      "\n",
      "Epoch 00319: loss did not improve from 0.08943\n",
      "Epoch 320/350\n",
      "2360/2360 [==============================] - 30s 13ms/step - loss: 0.1085\n",
      "\n",
      "Epoch 00320: loss did not improve from 0.08943\n",
      "Epoch 321/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1759\n",
      "\n",
      "Epoch 00321: loss did not improve from 0.08943\n",
      "Epoch 322/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1302\n",
      "\n",
      "Epoch 00322: loss did not improve from 0.08943\n",
      "Epoch 323/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1170\n",
      "\n",
      "Epoch 00323: loss did not improve from 0.08943\n",
      "Epoch 324/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1285\n",
      "\n",
      "Epoch 00324: loss did not improve from 0.08943\n",
      "Epoch 325/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1547\n",
      "\n",
      "Epoch 00325: loss did not improve from 0.08943\n",
      "Epoch 326/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1075\n",
      "\n",
      "Epoch 00326: loss did not improve from 0.08943\n",
      "Epoch 327/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1211\n",
      "\n",
      "Epoch 00327: loss did not improve from 0.08943\n",
      "Epoch 328/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1388\n",
      "\n",
      "Epoch 00328: loss did not improve from 0.08943\n",
      "Epoch 329/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1608\n",
      "\n",
      "Epoch 00329: loss did not improve from 0.08943\n",
      "Epoch 330/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.0935\n",
      "\n",
      "Epoch 00330: loss did not improve from 0.08943\n",
      "Epoch 331/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1252\n",
      "\n",
      "Epoch 00331: loss did not improve from 0.08943\n",
      "Epoch 332/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1159\n",
      "\n",
      "Epoch 00332: loss did not improve from 0.08943\n",
      "Epoch 333/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.1234\n",
      "\n",
      "Epoch 00333: loss did not improve from 0.08943\n",
      "Epoch 334/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1095\n",
      "\n",
      "Epoch 00334: loss did not improve from 0.08943\n",
      "Epoch 335/350\n",
      "2360/2360 [==============================] - 34s 15ms/step - loss: 0.1106\n",
      "\n",
      "Epoch 00335: loss did not improve from 0.08943\n",
      "Epoch 336/350\n",
      "2360/2360 [==============================] - 30s 13ms/step - loss: 0.1310\n",
      "\n",
      "Epoch 00336: loss did not improve from 0.08943\n",
      "Epoch 337/350\n",
      "2360/2360 [==============================] - 31s 13ms/step - loss: 0.1609\n",
      "\n",
      "Epoch 00337: loss did not improve from 0.08943\n",
      "Epoch 338/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.3206\n",
      "\n",
      "Epoch 00338: loss did not improve from 0.08943\n",
      "Epoch 339/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.1074\n",
      "\n",
      "Epoch 00339: loss did not improve from 0.08943\n",
      "Epoch 340/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1006\n",
      "\n",
      "Epoch 00340: loss did not improve from 0.08943\n",
      "Epoch 341/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1124\n",
      "\n",
      "Epoch 00341: loss did not improve from 0.08943\n",
      "Epoch 342/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.1312\n",
      "\n",
      "Epoch 00342: loss did not improve from 0.08943\n",
      "Epoch 343/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1039\n",
      "\n",
      "Epoch 00343: loss did not improve from 0.08943\n",
      "Epoch 344/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1188\n",
      "\n",
      "Epoch 00344: loss did not improve from 0.08943\n",
      "Epoch 345/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.1101\n",
      "\n",
      "Epoch 00345: loss did not improve from 0.08943\n",
      "Epoch 346/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.1107\n",
      "\n",
      "Epoch 00346: loss did not improve from 0.08943\n",
      "Epoch 347/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.1156\n",
      "\n",
      "Epoch 00347: loss did not improve from 0.08943\n",
      "Epoch 348/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.1336\n",
      "\n",
      "Epoch 00348: loss did not improve from 0.08943\n",
      "Epoch 349/350\n",
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.0964\n",
      "\n",
      "Epoch 00349: loss did not improve from 0.08943\n",
      "Epoch 350/350\n",
      "2360/2360 [==============================] - 28s 12ms/step - loss: 0.1303\n",
      "\n",
      "Epoch 00350: loss did not improve from 0.08943\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7ea15a5a10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_autoencoder.fit_generator(train_generator(x_train), epochs=500, steps_per_epoch=steps_per_epoch, verbose=1, callbacks=[early_stopping_callback, checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL SAVE\n",
    "model_json = lstm_autoencoder.to_json()\n",
    "filename = input('filename: ') #'latest_lstmae' #\n",
    "with open('model/' + filename + '.json', 'w') as file:\n",
    "    file.write(model_json)\n",
    "lstm_autoencoder.save_weights('model/weights_' +  filename + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL LOAD\n",
    "loaded_model = model_from_json(open('model/' +filename + '.json').read())\n",
    "loaded_model.load_weights('model/weights_' + filename + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = lstm_autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.04192586293003\n"
     ]
    }
   ],
   "source": [
    "mean= 0\n",
    "for xt in x_test:\n",
    "    xt = xt.reshape(1, xt.shape[0], xt.shape[1])\n",
    "    out = loaded_model.predict(xt)\n",
    "    mean += ((xt-out)**2).mean(axis=None)\n",
    "print(mean/len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(loaded_model.input, loaded_model.layers[3].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = x_test[0].reshape(1, x_test[0].shape[0], x_test[0].shape[1])\n",
    "latent_vector = []\n",
    "for x in x_test:\n",
    "    x = x.reshape(1, x.shape[0], x.shape[1])\n",
    "    latent_vector.append(encoder.predict(x)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

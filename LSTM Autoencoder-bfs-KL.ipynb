{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob, os\n",
    "from keras import layers as ly\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras.backend.tensorflow_backend as K\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Lambda\n",
    "from keras.models import Model\n",
    "from keras import losses\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir = './sequence/*'\n",
    "dir = './latest_sequence/bfs/*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file read\n",
    "all_data = []\n",
    "sequence_length = []\n",
    "name = []\n",
    "for file in sorted(glob.glob(dir)):\n",
    "    name.append(file.split('/')[-1].replace('.txt', ''))\n",
    "    datasets = []\n",
    "    for f in open(file, 'r'):\n",
    "        f = f.replace(']', '').replace('[', '').replace('\\n','')\n",
    "        (u, v, w) = f.split(',')\n",
    "        datasets.append([int(u), int(v), float(w)])\n",
    "    sequence_length.append(len(datasets))\n",
    "    all_data.append(datasets)\n",
    "#all_data = np.array(all_data)\n",
    "all_data = np.array([np.array(arr) for arr in all_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, train_name, test_name = train_test_split(all_data, name, test_size=0.3)\n",
    "x_test, x_val, test_name, val_name = train_test_split(x_test, test_name, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_name\n",
    "tr_names= []\n",
    "for name in train_name:\n",
    "    tr_names.append(name.split('-')[0].replace('graph', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = max(sequence_length)\n",
    "n_features = 3\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "steps_per_epoch = len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    loss1 = losses.mean_squared_error(y_true, y_pred)\n",
    "    loss2 = losses.KLD(y_true, y_pred) # KLD 왜 써야할까,,,\n",
    "    return loss1 + loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def repeat_vector(args):\n",
    "    layer_to_repeat = args[0]\n",
    "    sequence_layer = args[1]\n",
    "    return RepeatVector(K.shape(sequence_layer)[1])(layer_to_repeat)\n",
    "    \n",
    "\n",
    "inputs = Input(shape=(None, 3))\n",
    "encoded = LSTM(128, return_sequences=True)(inputs)  #activation 안적으면 tanh\n",
    "#encoded = LSTM(128, return_sequences=True)(encoded)\n",
    "encoded = LSTM(64, activation='relu')(encoded)\n",
    "\n",
    "decoded = Lambda(repeat_vector, output_shape=(None, 64)) ([encoded, inputs]) # inputs의 shape[1] 만큼 encoded 를 반복 생성\n",
    "\n",
    "decoded = LSTM(64, return_sequences=True)(decoded)\n",
    "decoded = LSTM(128, return_sequences=True)(decoded)\n",
    "#decoded = LSTM(256, return_sequences=True)(decoded)\n",
    "decoded = TimeDistributed(Dense(3))(decoded)\n",
    "encoder = Model(inputs, encoded)\n",
    "\n",
    "lstm_autoencoder = Model(inputs, decoded)\n",
    "lstm_autoencoder.compile(loss=custom_loss, optimizer=Adam(lr=0.001))\n",
    "#lstm_autoencoder_500 = lstm_autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(x_train):\n",
    "    idx = 0\n",
    "    while True:\n",
    "        yield np.array([x_train[idx]]), np.array([x_train[idx]])\n",
    "        idx +=1\n",
    "        if idx >= len(x_train):\n",
    "            idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2352/2352 [==============================] - 23s 10ms/step - loss: 21.7425\n",
      "Epoch 2/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 13.3673\n",
      "Epoch 3/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 11.9341\n",
      "Epoch 4/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 10.8175\n",
      "Epoch 5/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 9.6804\n",
      "Epoch 6/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 8.3871\n",
      "Epoch 7/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 7.6728\n",
      "Epoch 8/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 6.9347\n",
      "Epoch 9/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 5.8199\n",
      "Epoch 10/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 4.9524\n",
      "Epoch 11/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 4.4042\n",
      "Epoch 12/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 3.8541\n",
      "Epoch 13/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 3.6902\n",
      "Epoch 14/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 3.2014\n",
      "Epoch 15/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 2.8816\n",
      "Epoch 16/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 2.8799\n",
      "Epoch 17/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 2.5421\n",
      "Epoch 18/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 2.3133\n",
      "Epoch 19/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 2.5866\n",
      "Epoch 20/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 2.1031\n",
      "Epoch 21/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 1.9832\n",
      "Epoch 22/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 2.4337\n",
      "Epoch 23/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 1.6884\n",
      "Epoch 24/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 2.1257\n",
      "Epoch 25/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 1.9730\n",
      "Epoch 26/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 1.7031\n",
      "Epoch 27/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 1.6604\n",
      "Epoch 28/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 1.8241\n",
      "Epoch 29/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 1.8014\n",
      "Epoch 30/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 1.3747\n",
      "Epoch 31/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 1.6987\n",
      "Epoch 32/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 1.1901\n",
      "Epoch 33/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 1.3073\n",
      "Epoch 34/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 1.1490\n",
      "Epoch 35/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 1.1671\n",
      "Epoch 36/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 1.2882\n",
      "Epoch 37/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 0.9948\n",
      "Epoch 38/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 0.9733\n",
      "Epoch 39/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 0.9723\n",
      "Epoch 40/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 0.9287\n",
      "Epoch 41/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 1.2381\n",
      "Epoch 42/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 1.2216\n",
      "Epoch 43/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 1.1002\n",
      "Epoch 44/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 0.7414\n",
      "Epoch 45/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 0.8053\n",
      "Epoch 46/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 0.8799\n",
      "Epoch 47/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 0.7769\n",
      "Epoch 48/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 0.7144\n",
      "Epoch 49/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 0.6888\n",
      "Epoch 50/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 0.7166\n",
      "Epoch 51/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 0.7311\n",
      "Epoch 52/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 0.5946\n",
      "Epoch 53/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 0.6219\n",
      "Epoch 54/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 0.6870\n",
      "Epoch 55/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 0.8119\n",
      "Epoch 56/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 0.5393\n",
      "Epoch 57/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 0.5378\n",
      "Epoch 58/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 0.7124\n",
      "Epoch 59/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 0.7255\n",
      "Epoch 60/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 0.5344\n",
      "Epoch 61/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 0.5039\n",
      "Epoch 62/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 0.4794\n",
      "Epoch 63/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 0.7826\n",
      "Epoch 64/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 0.5835\n",
      "Epoch 65/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 0.4920\n",
      "Epoch 66/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 0.4605\n",
      "Epoch 67/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 0.4356\n",
      "Epoch 68/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 0.4902\n",
      "Epoch 69/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 0.6988\n",
      "Epoch 70/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 0.4493\n",
      "Epoch 71/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 0.4784\n",
      "Epoch 72/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 0.4377\n",
      "Epoch 73/100\n",
      "2352/2352 [==============================] - 21s 9ms/step - loss: 0.4122\n",
      "Epoch 74/100\n",
      "2352/2352 [==============================] - 20s 8ms/step - loss: 0.3881\n",
      "Epoch 75/100\n",
      "2352/2352 [==============================] - 20s 8ms/step - loss: 0.4189\n",
      "Epoch 76/100\n",
      "2352/2352 [==============================] - 20s 8ms/step - loss: 0.6332\n",
      "Epoch 77/100\n",
      "2352/2352 [==============================] - 20s 8ms/step - loss: 0.3411\n",
      "Epoch 78/100\n",
      "2352/2352 [==============================] - 20s 8ms/step - loss: 0.3421\n",
      "Epoch 79/100\n",
      "2352/2352 [==============================] - 20s 8ms/step - loss: 0.4235\n",
      "Epoch 80/100\n",
      "2352/2352 [==============================] - 20s 8ms/step - loss: 0.4502\n",
      "Epoch 81/100\n",
      "2352/2352 [==============================] - 20s 8ms/step - loss: 0.4326\n",
      "Epoch 82/100\n",
      "2352/2352 [==============================] - 20s 8ms/step - loss: 0.3331\n",
      "Epoch 83/100\n",
      "2352/2352 [==============================] - 20s 8ms/step - loss: 0.4017\n",
      "Epoch 84/100\n",
      "2352/2352 [==============================] - 20s 8ms/step - loss: 0.3414\n",
      "Epoch 85/100\n",
      "2352/2352 [==============================] - 20s 8ms/step - loss: 0.3654\n",
      "Epoch 86/100\n",
      "2352/2352 [==============================] - 20s 8ms/step - loss: 0.4964\n",
      "Epoch 87/100\n",
      "2352/2352 [==============================] - 20s 8ms/step - loss: 0.3818\n",
      "Epoch 88/100\n",
      "2352/2352 [==============================] - 20s 8ms/step - loss: 0.2927\n",
      "Epoch 89/100\n",
      "2352/2352 [==============================] - 20s 8ms/step - loss: 0.3009\n",
      "Epoch 90/100\n",
      "2352/2352 [==============================] - 20s 8ms/step - loss: 0.3116\n",
      "Epoch 91/100\n",
      "2352/2352 [==============================] - 20s 8ms/step - loss: 0.3221\n",
      "Epoch 92/100\n",
      "2352/2352 [==============================] - 20s 8ms/step - loss: 0.2864\n",
      "Epoch 93/100\n",
      "2352/2352 [==============================] - 20s 8ms/step - loss: 0.3078\n",
      "Epoch 94/100\n",
      "2352/2352 [==============================] - 20s 8ms/step - loss: 0.2905\n",
      "Epoch 95/100\n",
      "2352/2352 [==============================] - 20s 8ms/step - loss: 0.2885\n",
      "Epoch 96/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2352/2352 [==============================] - 19s 8ms/step - loss: 0.2727\n",
      "Epoch 97/100\n",
      "2352/2352 [==============================] - 19s 8ms/step - loss: 0.3385\n",
      "Epoch 98/100\n",
      "2352/2352 [==============================] - 19s 8ms/step - loss: 0.3751\n",
      "Epoch 99/100\n",
      "2352/2352 [==============================] - 19s 8ms/step - loss: 0.3059\n",
      "Epoch 100/100\n",
      "2352/2352 [==============================] - 19s 8ms/step - loss: 0.2247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f35381c83d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_autoencoder.fit_generator(train_generator(x_train), epochs=epochs, steps_per_epoch=steps_per_epoch, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model save\n",
    "#lstm_autoencoder.save('model/lstm_autoencoder_custom_loss_bfs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = lstm_autoencoder.to_json()\n",
    "with open('model/lstm_autoencoder_bfs.json', 'w') as file:\n",
    "    file.write(model_json)\n",
    "lstm_autoencoder.save_weights('model/weights_lstm_autoencoder_bfs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_from_json(open('model/lstm_autoencoder_custom_loss_bfs.json').read())\n",
    "model.load_weights('model/weights_lstm_autoencoder_custom_loss_bfs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_14\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, None, 3)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_33 (LSTM)                  (None, None, 128)    67584       input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_34 (LSTM)                  (None, 64)           49408       lstm_33[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, None, 64)     0           lstm_34[0][0]                    \n",
      "                                                                 input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_35 (LSTM)                  (None, None, 64)     33024       lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_36 (LSTM)                  (None, None, 128)    98816       lstm_35[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_7 (TimeDistrib (None, None, 3)      387         lstm_36[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 249,219\n",
      "Trainable params: 249,219\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = lstm_autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5025875809588274\n"
     ]
    }
   ],
   "source": [
    "mean= 0\n",
    "for xt in x_test:\n",
    "    xt = xt.reshape(1, xt.shape[0], xt.shape[1])\n",
    "    out = loaded_model.predict(xt)\n",
    "    mean += ((xt-out)**2).mean(axis=None)\n",
    "print(mean/len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(loaded_model.input, loaded_model.layers[3].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = x_test[0].reshape(1, x_test[0].shape[0], x_test[0].shape[1])\n",
    "latent_vector = []\n",
    "for x in x_test:\n",
    "    x = x.reshape(1, x.shape[0], x.shape[1])\n",
    "    latent_vector.append(encoder.predict(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "latent_vector = np.array(latent_vector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

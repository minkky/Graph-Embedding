{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob, os\n",
    "import random as rd\n",
    "from keras import layers as ly\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import LSTM, Dense, RepeatVector, TimeDistributed\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.layers import Input, Lambda\n",
    "from keras import losses\n",
    "from keras.models import model_from_json\n",
    "import keras.backend.tensorflow_backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import string, json\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir = './sequence/*'\n",
    "dir = './REAL/seq/*'\n",
    "alpha = list(string.ascii_uppercase)\n",
    "chr2index = {'C':1, 'N':2, 'O':3, 'F':4, 'P':5, 'S':6, 'Cl':7, 'As': 8, 'I':9, 'Hg':10}\n",
    "#chr2index = {'H':1, 'B':2,  'C':3, 'N':4, 'O':5, 'F':6, 'P':7, 'S':8, 'Cl':9, 'Fe':10,  'Co':11, 'Cu':12, 'Zn':13, 'Br':14}\n",
    "#chr2index = {'H':1, 'B':5,  'C':6, 'N':7, 'O':8, 'F':9, 'P':15, 'S':16, 'Cl':17, 'Fe':26,  'Co':27, 'Cu':29, 'Zn':30, 'Br':35}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1,\n",
       " 'N': 2,\n",
       " 'O': 3,\n",
       " 'F': 4,\n",
       " 'P': 5,\n",
       " 'S': 6,\n",
       " 'Cl': 7,\n",
       " 'As': 8,\n",
       " 'I': 9,\n",
       " 'Hg': 10}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chr2OH(node):\n",
    "    oh = [0 for i in range(len(chr2index))]\n",
    "    index = chr2index[node]-1\n",
    "    oh[index] = 1\n",
    "    return oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = [[8, 9, 10, 5, 7, 6, 16, 2, 1, 15, 13, 14, 20, 4], [16, 13, 3, 15, 2, 17, 19, 8, 11, 6, 12, 9, 18, 20], [16, 4, 2, 14, 19, 1, 17, 3, 20, 12, 9, 8, 10, 7], [2, 5, 1, 4, 7, 18, 8, 11, 20, 17, 15, 12, 16, 19], [15, 14, 13, 5, 4, 3, 9, 18, 7, 20, 8, 11, 16, 1]]\n",
    "val_label = [[11, 17], [7, 1], [13, 5], [14, 3], [6, 12]]\n",
    "test_label = [[12, 3, 19, 18], [10, 4, 14, 5], [6, 11, 15, 18], [13, 10, 6, 9], [17, 10, 19, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./REAL/seq/0/\n",
      "x_train [8, 9, 10, 5, 7, 6, 16, 2, 1, 15, 13, 14, 20, 4] x_val [11, 17] x_test [12, 3, 19, 18]\n",
      "./REAL/seq/1/\n",
      "x_train [16, 13, 3, 15, 2, 17, 19, 8, 11, 6, 12, 9, 18, 20] x_val [7, 1] x_test [10, 4, 14, 5]\n",
      "./REAL/seq/2/\n",
      "x_train [16, 4, 2, 14, 19, 1, 17, 3, 20, 12, 9, 8, 10, 7] x_val [13, 5] x_test [6, 11, 15, 18]\n",
      "./REAL/seq/3/\n",
      "x_train [2, 5, 1, 4, 7, 18, 8, 11, 20, 17, 15, 12, 16, 19] x_val [14, 3] x_test [13, 10, 6, 9]\n",
      "./REAL/seq/4/\n",
      "x_train [15, 14, 13, 5, 4, 3, 9, 18, 7, 20, 8, 11, 16, 1] x_val [6, 12] x_test [17, 10, 19, 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'# file read\\nall_names = []\\nall_data = []\\n#test = set()\\nsequence_length = []\\nalpha = list(string.ascii_uppercase)\\ndata_length = len(glob.glob(dir))\\nfile_predix = \\'./REAL/seq/\\'\\nfor index in range(data_length):\\n    filename = file_predix + str(index) + \"/\"\\n    prefix = str(index) + \\'REAL\\'\\n    print(filename)\\n    nums = [i for i in range(1, 21)]\\n    rd.shuffle(nums)\\n    print(\\'x_train\\', nums[:14], \\'x_val\\', nums[14:16], \\'x_test\\', nums[16:])\\n    x_train = []\\n    x_val = []\\n    for num in nums[:14]:\\n        f = filename + prefix + str(num) + \\'-*\\'\\n        files = glob.glob(f)\\n        for file in files:\\n            dts = []\\n            for rf in open(file, \\'r\\'):\\n                (u, v, w) = rf[1:-2].split(\\', \\')\\n                dts.append(chr2OH(u[1:-1]) + chr2OH(v[1:-1]) +[float(w)])\\n            sequence_length.append(len(dts))\\n            x_train.append(dts)\\n            \\n    for num in nums[14:16]:\\n        f = filename + prefix + str(num) + \\'-*\\'\\n        files = glob.glob(f)\\n        for file in files:\\n            dts = []\\n            for rf in open(file, \\'r\\'):\\n                (u, v, w) = rf[1:-2].split(\\', \\')\\n                dts.append(chr2OH(u[1:-1]) + chr2OH(v[1:-1]) +[float(w)])\\n            sequence_length.append(len(dts))\\n            x_val.append(dts)\\n            \\nx_train = np.array([np.array(arr) for arr in x_train])\\nx_val = np.array([np.array(arr) for arr in x_val])\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# file read\n",
    "all_names = []\n",
    "all_data = []\n",
    "#test = set()\n",
    "sequence_length = []\n",
    "alpha = list(string.ascii_uppercase)\n",
    "data_length = len(glob.glob(dir))\n",
    "file_predix = './REAL/seq/'\n",
    "for index in range(data_length):\n",
    "    filename = file_predix + str(index) + \"/\"\n",
    "    prefix = str(index) + 'REAL'\n",
    "    print(filename)\n",
    "    nums = [i for i in range(1, 21)]\n",
    "    rd.shuffle(nums)\n",
    "    print('x_train', train_label[index], 'x_val', val_label[index], 'x_test', test_label[index])\n",
    "    x_train = []\n",
    "    x_val = []\n",
    "    for num in train_label[index]:\n",
    "        f = filename + prefix + str(num) + '-*'\n",
    "        files = glob.glob(f)\n",
    "        for file in files:\n",
    "            dts = []\n",
    "            for rf in open(file, 'r'):\n",
    "                (u, v, w) = rf[1:-2].split(', ')\n",
    "                #dts.append([chr2index[u[1:-1]], chr2index[v[1:-1]], float(w)])\n",
    "                dts.append(chr2OH(u[1:-1]) + chr2OH(v[1:-1]) +[float(w)])\n",
    "            sequence_length.append(len(dts))\n",
    "            x_train.append(dts)\n",
    "            \n",
    "    for num in val_label[index]:\n",
    "        f = filename + prefix + str(num) + '-*'\n",
    "        files = glob.glob(f)\n",
    "        for file in files:\n",
    "            dts = []\n",
    "            for rf in open(file, 'r'):\n",
    "                (u, v, w) = rf[1:-2].split(', ')\n",
    "                dts.append(chr2OH(u[1:-1]) + chr2OH(v[1:-1]) +[float(w)])\n",
    "                #dts.append([chr2index[u[1:-1]], chr2index[v[1:-1]], float(w)])\n",
    "            sequence_length.append(len(dts))\n",
    "            x_val.append(dts)\n",
    "            \n",
    "x_train = np.array([np.array(arr) for arr in x_train])\n",
    "x_val = np.array([np.array(arr) for arr in x_val])\n",
    "'''# file read\n",
    "all_names = []\n",
    "all_data = []\n",
    "#test = set()\n",
    "sequence_length = []\n",
    "alpha = list(string.ascii_uppercase)\n",
    "data_length = len(glob.glob(dir))\n",
    "file_predix = './REAL/seq/'\n",
    "for index in range(data_length):\n",
    "    filename = file_predix + str(index) + \"/\"\n",
    "    prefix = str(index) + 'REAL'\n",
    "    print(filename)\n",
    "    nums = [i for i in range(1, 21)]\n",
    "    rd.shuffle(nums)\n",
    "    print('x_train', nums[:14], 'x_val', nums[14:16], 'x_test', nums[16:])\n",
    "    x_train = []\n",
    "    x_val = []\n",
    "    for num in nums[:14]:\n",
    "        f = filename + prefix + str(num) + '-*'\n",
    "        files = glob.glob(f)\n",
    "        for file in files:\n",
    "            dts = []\n",
    "            for rf in open(file, 'r'):\n",
    "                (u, v, w) = rf[1:-2].split(', ')\n",
    "                dts.append(chr2OH(u[1:-1]) + chr2OH(v[1:-1]) +[float(w)])\n",
    "            sequence_length.append(len(dts))\n",
    "            x_train.append(dts)\n",
    "            \n",
    "    for num in nums[14:16]:\n",
    "        f = filename + prefix + str(num) + '-*'\n",
    "        files = glob.glob(f)\n",
    "        for file in files:\n",
    "            dts = []\n",
    "            for rf in open(file, 'r'):\n",
    "                (u, v, w) = rf[1:-2].split(', ')\n",
    "                dts.append(chr2OH(u[1:-1]) + chr2OH(v[1:-1]) +[float(w)])\n",
    "            sequence_length.append(len(dts))\n",
    "            x_val.append(dts)\n",
    "            \n",
    "x_train = np.array([np.array(arr) for arr in x_train])\n",
    "x_val = np.array([np.array(arr) for arr in x_val])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "659 98\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train), len(x_val))\n",
    "#max_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train, x_test, train_name, test_name = train_test_split(all_data, all_names, test_size=0.3)\n",
    "#x_test, x_val, test_name, val_name = train_test_split(x_test, test_name, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = max(sequence_length)\n",
    "n_features = 3\n",
    "batch_size = 32\n",
    "epochs = 200\n",
    "steps_per_epoch = len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    loss1 = losses.mean_squared_error(y_true, y_pred)\n",
    "    loss2 = losses.kld(y_true, y_pred) # categorical_crossentropy\n",
    "    return loss1 * 0.7 + loss2 * 0.3\n",
    "\n",
    "def custom_loss1(y_true, y_pred):\n",
    "    loss1 = losses.mean_squared_error(y_true, y_pred)\n",
    "    loss2 = losses.categorical_crossentropy(y_true, y_pred) # categorical_crossentropy\n",
    "    return loss1 * 0.7 + loss2 * 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0528 02:40:25.568122 139759467755264 deprecation_wrapper.py:119] From /home/minji/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0528 02:40:25.576388 139759467755264 deprecation_wrapper.py:119] From /home/minji/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0528 02:40:25.578418 139759467755264 deprecation_wrapper.py:119] From /home/minji/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def repeat_vector(args):\n",
    "    layer_to_repeat = args[0]\n",
    "    sequence_layer = args[1]\n",
    "    return RepeatVector(K.shape(sequence_layer)[1])(layer_to_repeat)\n",
    "    \n",
    "\n",
    "inputs = Input(shape=(None, 21))\n",
    "encoded = LSTM(128, return_sequences=True)(inputs)  #activation 안적으면 tanh\n",
    "encoded = LSTM(64)(encoded)\n",
    "\n",
    "decoded = Lambda(repeat_vector, output_shape=(None, 64)) ([encoded, inputs]) # inputs의 shape[1] 만큼 encoded 를 반복 생성\n",
    "\n",
    "decoded = LSTM(64, return_sequences=True)(decoded)\n",
    "decoded = LSTM(128, return_sequences=True)(decoded)\n",
    "decoded = TimeDistributed(Dense(21))(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_generator(x_val):\n",
    "    idx = 0\n",
    "    while True:\n",
    "        yield np.array([x_val[idx]]), np.array([x_val[idx]])\n",
    "        idx += 1\n",
    "        if idx >= len(x_val):\n",
    "            idx = 0\n",
    "\n",
    "def train_generator(x_train):\n",
    "    idx = 0\n",
    "    while True:\n",
    "        yield np.array([x_train[idx]]), np.array([x_train[idx]])\n",
    "        idx += 1\n",
    "        if idx >= len(x_train):\n",
    "            idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd.shuffle(x_train)\n",
    "rd.shuffle(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "659/659 [==============================] - 36s 55ms/step - loss: 0.1348 - val_loss: 0.1451\n",
      "Epoch 2/200\n",
      "659/659 [==============================] - 33s 50ms/step - loss: 0.1306 - val_loss: 0.1750\n",
      "Epoch 3/200\n",
      "659/659 [==============================] - 32s 48ms/step - loss: 0.1307 - val_loss: 0.1396\n",
      "Epoch 4/200\n",
      "659/659 [==============================] - 32s 49ms/step - loss: 0.1282 - val_loss: 0.1376\n",
      "Epoch 5/200\n",
      "659/659 [==============================] - 32s 49ms/step - loss: 0.1278 - val_loss: 0.1351\n",
      "Epoch 6/200\n",
      "659/659 [==============================] - 32s 48ms/step - loss: 0.1268 - val_loss: 0.1360\n",
      "Epoch 7/200\n",
      "659/659 [==============================] - 32s 48ms/step - loss: 0.1267 - val_loss: 0.1332\n",
      "Epoch 8/200\n",
      "659/659 [==============================] - 32s 48ms/step - loss: 0.1239 - val_loss: 0.1275\n",
      "Epoch 9/200\n",
      "659/659 [==============================] - 32s 48ms/step - loss: 0.1221 - val_loss: 0.1271\n",
      "Epoch 10/200\n",
      "659/659 [==============================] - 32s 48ms/step - loss: 0.1193 - val_loss: 0.1283\n",
      "Epoch 11/200\n",
      "659/659 [==============================] - 32s 49ms/step - loss: 0.1173 - val_loss: 0.1234\n",
      "Epoch 12/200\n",
      "659/659 [==============================] - 32s 48ms/step - loss: 0.1150 - val_loss: 0.1211\n",
      "Epoch 13/200\n",
      "659/659 [==============================] - 32s 48ms/step - loss: 0.1126 - val_loss: 0.1178\n",
      "Epoch 14/200\n",
      "659/659 [==============================] - 32s 48ms/step - loss: 0.1147 - val_loss: 0.1222\n",
      "Epoch 15/200\n",
      "659/659 [==============================] - 32s 48ms/step - loss: 0.1083 - val_loss: 0.1172\n",
      "Epoch 16/200\n",
      "659/659 [==============================] - 32s 48ms/step - loss: 0.1076 - val_loss: 0.1220\n",
      "Epoch 17/200\n",
      "659/659 [==============================] - 32s 48ms/step - loss: 0.1049 - val_loss: 0.1154\n",
      "Epoch 18/200\n",
      "659/659 [==============================] - 32s 48ms/step - loss: 0.1037 - val_loss: 0.1093\n",
      "Epoch 19/200\n",
      "659/659 [==============================] - 32s 48ms/step - loss: 0.1048 - val_loss: 0.1262\n",
      "Epoch 20/200\n",
      "659/659 [==============================] - 32s 48ms/step - loss: 0.1045 - val_loss: 0.1085\n",
      "Epoch 21/200\n",
      "659/659 [==============================] - 32s 48ms/step - loss: 0.1000 - val_loss: 0.1077\n",
      "Epoch 22/200\n",
      "659/659 [==============================] - 32s 48ms/step - loss: 0.0980 - val_loss: 0.1071\n",
      "Epoch 23/200\n",
      "659/659 [==============================] - 32s 49ms/step - loss: 0.0966 - val_loss: 0.1082\n",
      "Epoch 24/200\n",
      "659/659 [==============================] - 32s 49ms/step - loss: 0.0957 - val_loss: 0.1053\n",
      "Epoch 25/200\n",
      "659/659 [==============================] - 32s 49ms/step - loss: 0.0943 - val_loss: 0.1045\n",
      "Epoch 26/200\n",
      "659/659 [==============================] - 32s 48ms/step - loss: 0.0930 - val_loss: 0.1030\n",
      "Epoch 27/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0914 - val_loss: 0.1013\n",
      "Epoch 28/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0918 - val_loss: 0.0995\n",
      "Epoch 29/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0899 - val_loss: 0.0958\n",
      "Epoch 30/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0898 - val_loss: 0.0950\n",
      "Epoch 31/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0887 - val_loss: 0.0980\n",
      "Epoch 32/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0855 - val_loss: 0.0933\n",
      "Epoch 33/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.1782 - val_loss: 0.1108\n",
      "Epoch 34/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0947 - val_loss: 0.1010\n",
      "Epoch 35/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0980 - val_loss: 0.1025\n",
      "Epoch 36/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0956 - val_loss: 0.1129\n",
      "Epoch 37/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.1019 - val_loss: 0.1038\n",
      "Epoch 38/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0957 - val_loss: 0.0992\n",
      "Epoch 39/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0951 - val_loss: 0.1069\n",
      "Epoch 40/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0935 - val_loss: 0.0962\n",
      "Epoch 41/200\n",
      "659/659 [==============================] - 32s 48ms/step - loss: 0.0905 - val_loss: 0.0961\n",
      "Epoch 42/200\n",
      "659/659 [==============================] - 32s 49ms/step - loss: 0.0916 - val_loss: 0.0966\n",
      "Epoch 43/200\n",
      "659/659 [==============================] - 32s 48ms/step - loss: 0.0907 - val_loss: 0.0922\n",
      "Epoch 44/200\n",
      "659/659 [==============================] - 32s 48ms/step - loss: 0.0983 - val_loss: 0.1042\n",
      "Epoch 45/200\n",
      "659/659 [==============================] - 32s 49ms/step - loss: 0.0943 - val_loss: 0.0959\n",
      "Epoch 46/200\n",
      "659/659 [==============================] - 32s 48ms/step - loss: 0.0914 - val_loss: 0.0977\n",
      "Epoch 47/200\n",
      "659/659 [==============================] - 32s 48ms/step - loss: 0.0912 - val_loss: 0.0974\n",
      "Epoch 48/200\n",
      "659/659 [==============================] - 32s 48ms/step - loss: 0.0894 - val_loss: 0.0914\n",
      "Epoch 49/200\n",
      "659/659 [==============================] - 32s 48ms/step - loss: 0.0824 - val_loss: 0.0879\n",
      "Epoch 50/200\n",
      "659/659 [==============================] - 32s 48ms/step - loss: 0.0810 - val_loss: 0.0933\n",
      "Epoch 51/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0833 - val_loss: 0.0844\n",
      "Epoch 52/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0810 - val_loss: 0.0844\n",
      "Epoch 53/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0786 - val_loss: 0.0861\n",
      "Epoch 54/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0784 - val_loss: 0.0863\n",
      "Epoch 55/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0763 - val_loss: 0.0832\n",
      "Epoch 56/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0759 - val_loss: 0.0843\n",
      "Epoch 57/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0790 - val_loss: 0.0924\n",
      "Epoch 58/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0976 - val_loss: 0.1217\n",
      "Epoch 59/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0938 - val_loss: 0.0940\n",
      "Epoch 60/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0822 - val_loss: 0.0874\n",
      "Epoch 61/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0858 - val_loss: 0.0870\n",
      "Epoch 62/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0808 - val_loss: 0.0861\n",
      "Epoch 63/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0796 - val_loss: 0.0859\n",
      "Epoch 64/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0781 - val_loss: 0.0836\n",
      "Epoch 65/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0859 - val_loss: 0.0863\n",
      "Epoch 66/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0784 - val_loss: 0.0863\n",
      "Epoch 67/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0816 - val_loss: 0.1249\n",
      "Epoch 68/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0864 - val_loss: 0.0866\n",
      "Epoch 69/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0763 - val_loss: 0.0795\n",
      "Epoch 70/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0765 - val_loss: 0.0956\n",
      "Epoch 71/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0792 - val_loss: 0.0892\n",
      "Epoch 72/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0801 - val_loss: 0.0804\n",
      "Epoch 73/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0735 - val_loss: 0.0761\n",
      "Epoch 74/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0733 - val_loss: 0.0777\n",
      "Epoch 75/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0761 - val_loss: 0.0959\n",
      "Epoch 76/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0789 - val_loss: 0.0764\n",
      "Epoch 77/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0711 - val_loss: 0.0740\n",
      "Epoch 78/200\n",
      "659/659 [==============================] - 32s 48ms/step - loss: 0.0698 - val_loss: 0.0753\n",
      "Epoch 79/200\n",
      "659/659 [==============================] - 32s 48ms/step - loss: 0.0693 - val_loss: 0.0739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0753 - val_loss: 0.0726\n",
      "Epoch 81/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0968 - val_loss: 0.1129\n",
      "Epoch 82/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0955 - val_loss: 0.0939\n",
      "Epoch 83/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0852 - val_loss: 0.0855\n",
      "Epoch 84/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0810 - val_loss: 0.0822\n",
      "Epoch 85/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0771 - val_loss: 0.0768\n",
      "Epoch 86/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0767 - val_loss: 0.0781\n",
      "Epoch 87/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0752 - val_loss: 0.0744\n",
      "Epoch 88/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0726 - val_loss: 0.0742\n",
      "Epoch 89/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0821 - val_loss: 0.0891\n",
      "Epoch 90/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.1239 - val_loss: 0.1219\n",
      "Epoch 91/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.1148 - val_loss: 0.1189\n",
      "Epoch 92/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.1095 - val_loss: 0.1096\n",
      "Epoch 93/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.1096 - val_loss: 0.1093\n",
      "Epoch 94/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.1046 - val_loss: 0.1070\n",
      "Epoch 95/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0999 - val_loss: 0.1044\n",
      "Epoch 96/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0997 - val_loss: 0.1732\n",
      "Epoch 97/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.1297 - val_loss: 0.1303\n",
      "Epoch 98/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.1163 - val_loss: 0.1195\n",
      "Epoch 99/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.1101 - val_loss: 0.1112\n",
      "Epoch 100/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.1053 - val_loss: 0.1165\n",
      "Epoch 101/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.1089 - val_loss: 0.1245\n",
      "Epoch 102/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.1067 - val_loss: 0.1078\n",
      "Epoch 103/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.1004 - val_loss: 0.1013\n",
      "Epoch 104/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0966 - val_loss: 0.0982\n",
      "Epoch 105/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0959 - val_loss: 0.0945\n",
      "Epoch 106/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0917 - val_loss: 0.0915\n",
      "Epoch 107/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0936 - val_loss: 0.0921\n",
      "Epoch 108/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0904 - val_loss: 0.0915\n",
      "Epoch 109/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0909 - val_loss: 0.1755\n",
      "Epoch 110/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.1195 - val_loss: 0.1163\n",
      "Epoch 111/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.1070 - val_loss: 0.1119\n",
      "Epoch 112/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0995 - val_loss: 0.1008\n",
      "Epoch 113/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0947 - val_loss: 0.1045\n",
      "Epoch 114/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0913 - val_loss: 0.0898\n",
      "Epoch 115/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.1016 - val_loss: 0.0949\n",
      "Epoch 116/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0875 - val_loss: 0.0875\n",
      "Epoch 117/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0861 - val_loss: 0.0882\n",
      "Epoch 118/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0843 - val_loss: 0.0842\n",
      "Epoch 119/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0830 - val_loss: 0.0825\n",
      "Epoch 120/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0828 - val_loss: 0.0974\n",
      "Epoch 121/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0830 - val_loss: 0.0829\n",
      "Epoch 122/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0845 - val_loss: 0.0852\n",
      "Epoch 123/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0810 - val_loss: 0.0837\n",
      "Epoch 124/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0853 - val_loss: 0.0892\n",
      "Epoch 125/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0785 - val_loss: 0.0761\n",
      "Epoch 126/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0773 - val_loss: 0.0771\n",
      "Epoch 127/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0770 - val_loss: 0.0762\n",
      "Epoch 128/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.1188 - val_loss: 0.1026\n",
      "Epoch 129/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0931 - val_loss: 0.0881\n",
      "Epoch 130/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0955 - val_loss: 0.0875\n",
      "Epoch 131/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0839 - val_loss: 0.0816\n",
      "Epoch 132/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0799 - val_loss: 0.0800\n",
      "Epoch 133/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0788 - val_loss: 0.0834\n",
      "Epoch 134/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0762 - val_loss: 0.0960\n",
      "Epoch 135/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0771 - val_loss: 0.0741\n",
      "Epoch 136/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0847 - val_loss: 0.0828\n",
      "Epoch 137/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0768 - val_loss: 0.0739\n",
      "Epoch 138/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0746 - val_loss: 0.0712\n",
      "Epoch 139/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0819 - val_loss: 0.0777\n",
      "Epoch 140/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0736 - val_loss: 0.0715\n",
      "Epoch 141/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0715 - val_loss: 0.0770\n",
      "Epoch 142/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0713 - val_loss: 0.0691\n",
      "Epoch 143/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0774 - val_loss: 0.0807\n",
      "Epoch 144/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0745 - val_loss: 0.0692\n",
      "Epoch 145/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0718 - val_loss: 0.0740\n",
      "Epoch 146/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0701 - val_loss: 0.0674\n",
      "Epoch 147/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0715 - val_loss: 0.0697\n",
      "Epoch 148/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0691 - val_loss: 0.0677\n",
      "Epoch 149/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0698 - val_loss: 0.0744\n",
      "Epoch 150/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0673 - val_loss: 0.0674\n",
      "Epoch 151/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0689 - val_loss: 0.0689\n",
      "Epoch 152/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0673 - val_loss: 0.0664\n",
      "Epoch 153/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0656 - val_loss: 0.0736\n",
      "Epoch 154/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0643 - val_loss: 0.0633\n",
      "Epoch 155/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0625 - val_loss: 0.0652\n",
      "Epoch 156/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0621 - val_loss: 0.0739\n",
      "Epoch 157/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0626 - val_loss: 0.0835\n",
      "Epoch 158/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0694 - val_loss: 0.0856\n",
      "Epoch 159/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0698 - val_loss: 0.0631\n",
      "Epoch 160/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0617 - val_loss: 0.0619\n",
      "Epoch 161/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0593 - val_loss: 0.0589\n",
      "Epoch 162/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0576 - val_loss: 0.0642\n",
      "Epoch 163/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0624 - val_loss: 0.0694\n",
      "Epoch 164/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0633 - val_loss: 0.0632\n",
      "Epoch 165/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0627 - val_loss: 0.0744\n",
      "Epoch 166/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0734 - val_loss: 0.0684\n",
      "Epoch 167/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0572 - val_loss: 0.0652\n",
      "Epoch 168/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0553 - val_loss: 0.0616\n",
      "Epoch 169/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0563 - val_loss: 0.0587\n",
      "Epoch 170/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0662 - val_loss: 0.0603\n",
      "Epoch 171/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0571 - val_loss: 0.0620\n",
      "Epoch 172/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0547 - val_loss: 0.0749\n",
      "Epoch 173/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0537 - val_loss: 0.0674\n",
      "Epoch 174/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0557 - val_loss: 0.0674\n",
      "Epoch 175/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0578 - val_loss: 0.0615\n",
      "Epoch 176/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0556 - val_loss: 0.0576\n",
      "Epoch 177/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0577 - val_loss: 0.0614\n",
      "Epoch 178/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0676 - val_loss: 0.1149\n",
      "Epoch 179/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0683 - val_loss: 0.0613\n",
      "Epoch 180/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0547 - val_loss: 0.0569\n",
      "Epoch 181/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0515 - val_loss: 0.0563\n",
      "Epoch 182/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0598 - val_loss: 0.0592\n",
      "Epoch 183/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0593 - val_loss: 0.0707\n",
      "Epoch 184/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0733 - val_loss: 0.0703\n",
      "Epoch 185/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0570 - val_loss: 0.0606\n",
      "Epoch 186/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0518 - val_loss: 0.0637\n",
      "Epoch 187/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0504 - val_loss: 0.0563\n",
      "Epoch 188/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0496 - val_loss: 0.0655\n",
      "Epoch 189/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0511 - val_loss: 0.0549\n",
      "Epoch 190/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0500 - val_loss: 0.0624\n",
      "Epoch 191/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0535 - val_loss: 0.0561\n",
      "Epoch 192/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0630 - val_loss: 0.0560\n",
      "Epoch 193/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0502 - val_loss: 0.0566\n",
      "Epoch 194/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0489 - val_loss: 0.0607\n",
      "Epoch 195/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0503 - val_loss: 0.0606\n",
      "Epoch 196/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0531 - val_loss: 0.0565\n",
      "Epoch 197/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0564 - val_loss: 0.0577\n",
      "Epoch 198/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0543 - val_loss: 0.0657\n",
      "Epoch 199/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0488 - val_loss: 0.0768\n",
      "Epoch 200/200\n",
      "659/659 [==============================] - 31s 47ms/step - loss: 0.0496 - val_loss: 0.0635\n"
     ]
    }
   ],
   "source": [
    "kld_ae = Model(inputs, decoded)\n",
    "kld_ae.compile(loss=custom_loss, optimizer='adam')#decay=0.9\n",
    "kld_hist = kld_ae.fit_generator(train_generator(x_train), epochs=200, steps_per_epoch=steps_per_epoch, verbose=1, validation_steps=len(x_val), validation_data=val_generator(x_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL SAVE\n",
    "model_json = kld_ae.to_json()\n",
    "filename = 'REAL_kld_real_model' #input('filename: ') \n",
    "with open('models/real/2REAL_kld_real_model.json', 'w') as file:\n",
    "    file.write(model_json)\n",
    "kld_ae.save_weights('models/real/2weights_' +  filename + '.h5')\n",
    "\n",
    "\n",
    "with open('models/real/2REAL_kld_real_history.json', 'w') as f:\n",
    "    json.dump(kld_hist.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1377"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "640/659 [============================>.] - ETA: 0s - loss: 0.1342"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-556ede08bac2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mkld_ae1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mkld_ae1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#decay=0.9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mkld_hist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkld_ae1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    213\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    214\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1447\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1449\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1450\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/graph/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kld_ae1 = Model(inputs, decoded)\n",
    "kld_ae1.compile(loss=custom_loss, optimizer='adam')#decay=0.9\n",
    "kld_hist = kld_ae1.fit_generator(train_generator(x_train), epochs=200, steps_per_epoch=steps_per_epoch, verbose=1, validation_steps=len(x_val), validation_data=val_generator(x_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL SAVE\n",
    "model_json = kld_ae1.to_json()\n",
    "filename = '2REAL_kld_real_model' #input('filename: ') \n",
    "with open('models/real/2REAL_kld_real_model.json', 'w') as file:\n",
    "    file.write(model_json)\n",
    "kld_ae.save_weights('models/real/2weights_' +  filename + '.h5')\n",
    "\n",
    "\n",
    "with open('models/real/2REAL_kld_real_history.json', 'w') as f:\n",
    "    json.dump(kld_hist.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'models/real/chr_weights' + '{epoch:02d}-{loss:.4f}.h5'\n",
    "checkpoint_callback = ModelCheckpoint(model_path, monitor='loss', verbose = 1, save_best_only = True, save_weights_only = True, period=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "659/659 [==============================] - 35s 53ms/step - loss: 0.0401 - val_loss: 0.0408\n"
     ]
    }
   ],
   "source": [
    "mse_ae = Model(inputs, decoded)\n",
    "mse_ae.compile(loss='mse', optimizer='adam')#decay=0.9\n",
    "mse_hist = mse_ae.fit_generator(train_generator(x_train), epochs=1, steps_per_epoch=steps_per_epoch, verbose=1, validation_steps=len(x_val), validation_data=val_generator(x_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cce_inputs = Input(shape=(None, 17))\n",
    "cce_encoded = LSTM(128, return_sequences=True)(cce_inputs)  #activation 안적으면 tanh\n",
    "cce_encoded = LSTM(64, activation='relu')(cce_encoded)\n",
    "\n",
    "cce_decoded = Lambda(repeat_vector, output_shape=(None, 64)) ([cce_encoded, cce_inputs]) # inputs의 shape[1] 만큼 encoded 를 반복 생성\n",
    "\n",
    "cce_decoded = LSTM(64, return_sequences=True)(cce_decoded)\n",
    "cce_decoded = LSTM(128, return_sequences=True)(cce_decoded)\n",
    "cce_decoded = TimeDistributed(Dense(17))(cce_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/cs405a/anaconda3/envs/graph/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/cs405a/anaconda3/envs/graph/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/cs405a/anaconda3/envs/graph/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/cs405a/anaconda3/envs/graph/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Epoch 1/200\n",
      "4563/4563 [==============================] - 107s 23ms/step - loss: 4.7056 - val_loss: 1.2168\n",
      "Epoch 2/200\n",
      "4563/4563 [==============================] - 105s 23ms/step - loss: 3.3900 - val_loss: 1.2168\n",
      "Epoch 3/200\n",
      "4563/4563 [==============================] - 105s 23ms/step - loss: 2.9607 - val_loss: 1.2159\n",
      "Epoch 4/200\n",
      "4563/4563 [==============================] - 105s 23ms/step - loss: 2.8799 - val_loss: 15.3959\n",
      "Epoch 5/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: 2.7521 - val_loss: 1.2159\n",
      "Epoch 6/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: 2.4007 - val_loss: 1.8026\n",
      "Epoch 7/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: 2.2163 - val_loss: 1.2154\n",
      "Epoch 8/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: 2.3507 - val_loss: 1.2161\n",
      "Epoch 9/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: 2.0271 - val_loss: 1.2158\n",
      "Epoch 10/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: 1.8115 - val_loss: 1.2160\n",
      "Epoch 11/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: 2.1381 - val_loss: 1.2161\n",
      "Epoch 12/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: 1.9456 - val_loss: 1.2154\n",
      "Epoch 13/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: 1.9183 - val_loss: 1.2157\n",
      "Epoch 14/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: 1.6823 - val_loss: 1.2153\n",
      "Epoch 15/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: 1.6462 - val_loss: 1.2151\n",
      "Epoch 16/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: 2.8709 - val_loss: 1.7478\n",
      "Epoch 17/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: 4.6515 - val_loss: 9.5535\n",
      "Epoch 18/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: 6.6022 - val_loss: 12.8959\n",
      "Epoch 19/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: 8.7244 - val_loss: 0.6303\n",
      "Epoch 20/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: 6.0476 - val_loss: 0.6475\n",
      "Epoch 21/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: 9.0883 - val_loss: 1.2612\n",
      "Epoch 22/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: 9.8905 - val_loss: 15.9817\n",
      "Epoch 23/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: 9.3753 - val_loss: 15.3515\n",
      "Epoch 24/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: 5.5784 - val_loss: 1.2355\n",
      "Epoch 25/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: 6.0554 - val_loss: 1.3495\n",
      "Epoch 26/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: 8.1204 - val_loss: 1.2103\n",
      "Epoch 27/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: 9.7348 - val_loss: 14.9854\n",
      "Epoch 28/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 29/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 30/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 31/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 32/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 33/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 34/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 35/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 36/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 37/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 38/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 39/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 40/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 41/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 42/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 43/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 44/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 45/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 46/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 47/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 48/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 49/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 50/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 51/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 52/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 53/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 54/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 55/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 56/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 57/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 58/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 59/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 60/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 61/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 62/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 63/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 64/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 65/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 66/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 67/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 68/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 69/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 70/200\n",
      "4563/4563 [==============================] - 102s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 71/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 72/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 73/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 74/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 75/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 76/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 77/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 78/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 79/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 80/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 81/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 82/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 83/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 84/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 85/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 86/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 87/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 88/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 89/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 90/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 91/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 92/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 93/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 94/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 95/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 96/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 97/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 98/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 99/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 100/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 101/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 102/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 103/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 104/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 105/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 106/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 107/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 108/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 109/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 110/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 111/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 112/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 113/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 114/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 115/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 116/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 117/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 118/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 119/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 120/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 121/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 122/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 123/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 124/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 125/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 126/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 127/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 128/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 129/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 130/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 131/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 132/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 133/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 134/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 135/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 136/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 137/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 138/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 139/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 140/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 141/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 142/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 143/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 144/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 145/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 146/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 147/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 148/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 149/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 150/200\n",
      "4563/4563 [==============================] - 101s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 151/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 152/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 153/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 154/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 155/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 156/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 157/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 158/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 159/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 160/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 161/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 162/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 163/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 164/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 165/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 166/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 167/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 168/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 169/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 170/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 171/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 172/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 173/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 174/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 175/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 176/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 177/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 178/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 179/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 180/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 181/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 182/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 183/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 184/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 185/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 186/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 187/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 188/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 189/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 190/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 191/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 192/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 193/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 194/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 195/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 196/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 197/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 198/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 199/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 200/200\n",
      "4563/4563 [==============================] - 100s 22ms/step - loss: nan - val_loss: nan\n"
     ]
    }
   ],
   "source": [
    "cce_ae = Model(inputs, decoded)\n",
    "cce_ae.compile(loss=custom_loss1, optimizer='adam')#Adam(lr=0.0002, decay=1e-3))#Adam(lr=0.003))#decay=0.9\n",
    "cce_hist = cce_ae.fit_generator(train_generator(x_train), epochs=200, steps_per_epoch=steps_per_epoch, verbose=1, validation_steps=len(x_val), validation_data=val_generator(x_val))\n",
    "#cce_hist = cce_ae.fit_generator(train_generator(x_train), epochs=200, steps_per_epoch=steps_per_epoch, verbose=1, validation_steps=len(x_val), validation_data=val_generator(x_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob, os\n",
    "from keras import layers as ly\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras.backend.tensorflow_backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir = './sequence/*'\n",
    "dir = './p_sequence/*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file read\n",
    "all_data = []\n",
    "sequence_length = []\n",
    "for file in sorted(glob.glob(dir)):\n",
    "    datasets = []\n",
    "    for f in open(file, 'r'):\n",
    "        f = f.replace(']', '').replace('[', '').replace('\\n','')\n",
    "        (u, v, w) = f.split(',')\n",
    "        datasets.append([int(u), int(v), float(w)])\n",
    "    sequence_length.append(len(datasets))\n",
    "    all_data.append(datasets)\n",
    "#all_data = np.array(all_data)\n",
    "all_data = np.array([np.array(arr) for arr in all_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = train_test_split(all_data, test_size = 0.2)\n",
    "x_test, x_val = train_test_split(x_test, test_size = 0.66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = max(sequence_length)\n",
    "n_features = 3\n",
    "batch_size = 32\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "756"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "756/756 [==============================] - 10s 13ms/step - loss: 52.6284\n",
      "Epoch 2/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 37.0736\n",
      "Epoch 3/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 30.3203\n",
      "Epoch 4/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 27.1553\n",
      "Epoch 5/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 25.5362\n",
      "Epoch 6/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 24.0204\n",
      "Epoch 7/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 22.5114\n",
      "Epoch 8/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 21.2432\n",
      "Epoch 9/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 21.0304\n",
      "Epoch 10/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 18.9596\n",
      "Epoch 11/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 18.8432\n",
      "Epoch 12/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 18.9515\n",
      "Epoch 13/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 18.5652\n",
      "Epoch 14/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 16.5533\n",
      "Epoch 15/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 15.7463\n",
      "Epoch 16/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 15.6005\n",
      "Epoch 17/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 14.6084\n",
      "Epoch 18/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 13.5249\n",
      "Epoch 19/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 13.9462\n",
      "Epoch 20/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 12.0662\n",
      "Epoch 21/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 12.0780\n",
      "Epoch 22/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 11.2539\n",
      "Epoch 23/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 10.7347\n",
      "Epoch 24/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 11.5120\n",
      "Epoch 25/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 11.0094\n",
      "Epoch 26/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 9.1173\n",
      "Epoch 27/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 9.1541\n",
      "Epoch 28/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 9.6228\n",
      "Epoch 29/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 7.6700\n",
      "Epoch 30/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 7.5927\n",
      "Epoch 31/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 8.1135\n",
      "Epoch 32/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 6.8007\n",
      "Epoch 33/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 6.0820\n",
      "Epoch 34/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 6.3422\n",
      "Epoch 35/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 6.1807\n",
      "Epoch 36/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 7.7707\n",
      "Epoch 37/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 6.7455\n",
      "Epoch 38/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 5.2367\n",
      "Epoch 39/500\n",
      "756/756 [==============================] - 7s 10ms/step - loss: 5.2743\n",
      "Epoch 40/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 5.0329\n",
      "Epoch 41/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 4.2066\n",
      "Epoch 42/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 5.3182\n",
      "Epoch 43/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 5.8145\n",
      "Epoch 44/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 4.3514\n",
      "Epoch 45/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 4.0315\n",
      "Epoch 46/500\n",
      "756/756 [==============================] - 7s 10ms/step - loss: 3.8495\n",
      "Epoch 47/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 4.1588\n",
      "Epoch 48/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 3.4476\n",
      "Epoch 49/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 3.0701\n",
      "Epoch 50/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 3.7967\n",
      "Epoch 51/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 3.6371\n",
      "Epoch 52/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 3.6774\n",
      "Epoch 53/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 2.9936\n",
      "Epoch 54/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 2.9710\n",
      "Epoch 55/500\n",
      "756/756 [==============================] - 7s 10ms/step - loss: 3.2703\n",
      "Epoch 56/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 3.1647\n",
      "Epoch 57/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 2.7979\n",
      "Epoch 58/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 2.9542\n",
      "Epoch 59/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 2.5896\n",
      "Epoch 60/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 2.5570\n",
      "Epoch 61/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 2.6094\n",
      "Epoch 62/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 2.0618\n",
      "Epoch 63/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 2.5550\n",
      "Epoch 64/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 2.2716\n",
      "Epoch 65/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 1.9724\n",
      "Epoch 66/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 2.0612\n",
      "Epoch 67/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 2.0425\n",
      "Epoch 68/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 2.0805\n",
      "Epoch 69/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 1.9851\n",
      "Epoch 70/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 2.1884\n",
      "Epoch 71/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 1.7120\n",
      "Epoch 72/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 1.6654\n",
      "Epoch 73/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 1.9816\n",
      "Epoch 74/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 1.7307\n",
      "Epoch 75/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 2.1615\n",
      "Epoch 76/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 1.9191\n",
      "Epoch 77/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 1.7277\n",
      "Epoch 78/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 1.6664\n",
      "Epoch 79/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 1.3618\n",
      "Epoch 80/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 1.1380\n",
      "Epoch 81/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 1.2600\n",
      "Epoch 82/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 2.5638\n",
      "Epoch 83/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 1.9730\n",
      "Epoch 84/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 1.1465\n",
      "Epoch 85/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.8848\n",
      "Epoch 86/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.9042\n",
      "Epoch 87/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 1.5457\n",
      "Epoch 88/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 1.6420\n",
      "Epoch 89/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 1.1340\n",
      "Epoch 90/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.8870\n",
      "Epoch 91/500\n",
      "756/756 [==============================] - 7s 10ms/step - loss: 1.1885\n",
      "Epoch 92/500\n",
      "756/756 [==============================] - 7s 10ms/step - loss: 1.2728\n",
      "Epoch 93/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 1.1911\n",
      "Epoch 94/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 1.2022\n",
      "Epoch 95/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 1.1187\n",
      "Epoch 96/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 1.4390\n",
      "Epoch 97/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 1.0508\n",
      "Epoch 98/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.9104\n",
      "Epoch 99/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "756/756 [==============================] - 7s 9ms/step - loss: 1.0037\n",
      "Epoch 100/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.9118\n",
      "Epoch 101/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.8644\n",
      "Epoch 102/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 1.0232\n",
      "Epoch 103/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.8093\n",
      "Epoch 104/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.7260\n",
      "Epoch 105/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.8848\n",
      "Epoch 106/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.9264\n",
      "Epoch 107/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.7697\n",
      "Epoch 108/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.8606\n",
      "Epoch 109/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.9384\n",
      "Epoch 110/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.7871\n",
      "Epoch 111/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.8996\n",
      "Epoch 112/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.8829\n",
      "Epoch 113/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.6894\n",
      "Epoch 114/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.5544\n",
      "Epoch 115/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.6110\n",
      "Epoch 116/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.8178\n",
      "Epoch 117/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.5948\n",
      "Epoch 118/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.9084\n",
      "Epoch 119/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 1.7582\n",
      "Epoch 120/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.5632\n",
      "Epoch 121/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.3948\n",
      "Epoch 122/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.3284\n",
      "Epoch 123/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.4262\n",
      "Epoch 124/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.6057\n",
      "Epoch 125/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.8321\n",
      "Epoch 126/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.6408\n",
      "Epoch 127/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.4342\n",
      "Epoch 128/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.5264\n",
      "Epoch 129/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.7850\n",
      "Epoch 130/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.6045\n",
      "Epoch 131/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.4768\n",
      "Epoch 132/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.4622\n",
      "Epoch 133/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.4982\n",
      "Epoch 134/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.8878\n",
      "Epoch 135/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.5390\n",
      "Epoch 136/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.3573\n",
      "Epoch 137/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.3392\n",
      "Epoch 138/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.5038\n",
      "Epoch 139/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.7468\n",
      "Epoch 140/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.4513\n",
      "Epoch 141/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.3377\n",
      "Epoch 142/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.3541\n",
      "Epoch 143/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.5599\n",
      "Epoch 144/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.6937\n",
      "Epoch 145/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.4329\n",
      "Epoch 146/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.2957\n",
      "Epoch 147/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.3224\n",
      "Epoch 148/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.6310\n",
      "Epoch 149/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.5404\n",
      "Epoch 150/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.3767\n",
      "Epoch 151/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.2830\n",
      "Epoch 152/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.3561\n",
      "Epoch 153/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.3970\n",
      "Epoch 154/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.4340\n",
      "Epoch 155/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.3722\n",
      "Epoch 156/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.3529\n",
      "Epoch 157/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.3604\n",
      "Epoch 158/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.4039\n",
      "Epoch 159/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.4785\n",
      "Epoch 160/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.3134\n",
      "Epoch 161/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.2609\n",
      "Epoch 162/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.4051\n",
      "Epoch 163/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.3292\n",
      "Epoch 164/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.3990\n",
      "Epoch 165/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.4807\n",
      "Epoch 166/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.3111\n",
      "Epoch 167/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.2474\n",
      "Epoch 168/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.2523\n",
      "Epoch 169/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.3670\n",
      "Epoch 170/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.3793\n",
      "Epoch 171/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.2817\n",
      "Epoch 172/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.2500\n",
      "Epoch 173/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.5639\n",
      "Epoch 174/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.3671\n",
      "Epoch 175/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1899\n",
      "Epoch 176/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1579\n",
      "Epoch 177/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.2268\n",
      "Epoch 178/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.3751\n",
      "Epoch 179/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.3273\n",
      "Epoch 180/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.2787\n",
      "Epoch 181/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.2896\n",
      "Epoch 182/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.3044\n",
      "Epoch 183/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.2987\n",
      "Epoch 184/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.2431\n",
      "Epoch 185/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.2502\n",
      "Epoch 186/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.2679\n",
      "Epoch 187/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.3521\n",
      "Epoch 188/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.2602\n",
      "Epoch 189/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1739\n",
      "Epoch 190/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1981\n",
      "Epoch 191/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.3222\n",
      "Epoch 192/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.2965\n",
      "Epoch 193/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.2147\n",
      "Epoch 194/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.2022\n",
      "Epoch 195/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.2780\n",
      "Epoch 196/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "756/756 [==============================] - 6s 8ms/step - loss: 0.2448\n",
      "Epoch 197/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.2928\n",
      "Epoch 198/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.2309\n",
      "Epoch 199/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.2024\n",
      "Epoch 200/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.2204\n",
      "Epoch 201/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.2388\n",
      "Epoch 202/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.2394\n",
      "Epoch 203/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.2832\n",
      "Epoch 204/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.2242\n",
      "Epoch 205/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1812\n",
      "Epoch 206/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1883\n",
      "Epoch 207/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.3660\n",
      "Epoch 208/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.3460\n",
      "Epoch 209/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.2046\n",
      "Epoch 210/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.1435\n",
      "Epoch 211/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.1329\n",
      "Epoch 212/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.2186\n",
      "Epoch 213/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.3393\n",
      "Epoch 214/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.2122\n",
      "Epoch 215/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1633\n",
      "Epoch 216/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.1773\n",
      "Epoch 217/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.2682\n",
      "Epoch 218/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.2247\n",
      "Epoch 219/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1881\n",
      "Epoch 220/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1511\n",
      "Epoch 221/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1850\n",
      "Epoch 222/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.2215\n",
      "Epoch 223/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.2694\n",
      "Epoch 224/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1555\n",
      "Epoch 225/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.1273\n",
      "Epoch 226/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1648\n",
      "Epoch 227/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.2322\n",
      "Epoch 228/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.2253\n",
      "Epoch 229/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.1616\n",
      "Epoch 230/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.1779\n",
      "Epoch 231/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1713\n",
      "Epoch 232/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.1724\n",
      "Epoch 233/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.1826\n",
      "Epoch 234/500\n",
      "756/756 [==============================] - 7s 10ms/step - loss: 0.1776\n",
      "Epoch 235/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.1790\n",
      "Epoch 236/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.2173\n",
      "Epoch 237/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1685\n",
      "Epoch 238/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.1267\n",
      "Epoch 239/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.1519\n",
      "Epoch 240/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.1921\n",
      "Epoch 241/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.1873\n",
      "Epoch 242/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.1376\n",
      "Epoch 243/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.1343\n",
      "Epoch 244/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.2273\n",
      "Epoch 245/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.2324\n",
      "Epoch 246/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.1318\n",
      "Epoch 247/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0959\n",
      "Epoch 248/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.1252\n",
      "Epoch 249/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.1953\n",
      "Epoch 250/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.1814\n",
      "Epoch 251/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.1608\n",
      "Epoch 252/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.1233\n",
      "Epoch 253/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.1384\n",
      "Epoch 254/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.1727\n",
      "Epoch 255/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.1545\n",
      "Epoch 256/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.1317\n",
      "Epoch 257/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.1491\n",
      "Epoch 258/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1568\n",
      "Epoch 259/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1395\n",
      "Epoch 260/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1336\n",
      "Epoch 261/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1563\n",
      "Epoch 262/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1654\n",
      "Epoch 263/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1473\n",
      "Epoch 264/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1174\n",
      "Epoch 265/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1433\n",
      "Epoch 266/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1605\n",
      "Epoch 267/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1370\n",
      "Epoch 268/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1162\n",
      "Epoch 269/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1316\n",
      "Epoch 270/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1496\n",
      "Epoch 271/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1303\n",
      "Epoch 272/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1202\n",
      "Epoch 273/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1392\n",
      "Epoch 274/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.1426\n",
      "Epoch 275/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.1287\n",
      "Epoch 276/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1140\n",
      "Epoch 277/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1212\n",
      "Epoch 278/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.1491\n",
      "Epoch 279/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.1728\n",
      "Epoch 280/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1224\n",
      "Epoch 281/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1013\n",
      "Epoch 282/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.1069\n",
      "Epoch 283/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1387\n",
      "Epoch 284/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1597\n",
      "Epoch 285/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1357\n",
      "Epoch 286/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1248\n",
      "Epoch 287/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0939\n",
      "Epoch 288/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.1027\n",
      "Epoch 289/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1169\n",
      "Epoch 290/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1339\n",
      "Epoch 291/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1304\n",
      "Epoch 292/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1283\n",
      "Epoch 293/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1377\n",
      "Epoch 294/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.0934\n",
      "Epoch 295/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.0992\n",
      "Epoch 296/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.1201\n",
      "Epoch 297/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.1296\n",
      "Epoch 298/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.1144\n",
      "Epoch 299/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.1038\n",
      "Epoch 300/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.1052\n",
      "Epoch 301/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.1194\n",
      "Epoch 302/500\n",
      "756/756 [==============================] - 7s 10ms/step - loss: 0.1110\n",
      "Epoch 303/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.1359\n",
      "Epoch 304/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.1099\n",
      "Epoch 305/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0996\n",
      "Epoch 306/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.1063\n",
      "Epoch 307/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.1178\n",
      "Epoch 308/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.1810\n",
      "Epoch 309/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1130\n",
      "Epoch 310/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.0622\n",
      "Epoch 311/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.0725\n",
      "Epoch 312/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.1261\n",
      "Epoch 313/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.1449\n",
      "Epoch 314/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.1001\n",
      "Epoch 315/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0759\n",
      "Epoch 316/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0992\n",
      "Epoch 317/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1074\n",
      "Epoch 318/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1101\n",
      "Epoch 319/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1188\n",
      "Epoch 320/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.1023\n",
      "Epoch 321/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.1623\n",
      "Epoch 322/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0941\n",
      "Epoch 323/500\n",
      "756/756 [==============================] - 8s 11ms/step - loss: 0.0811\n",
      "Epoch 324/500\n",
      "756/756 [==============================] - 8s 11ms/step - loss: 0.0839\n",
      "Epoch 325/500\n",
      "756/756 [==============================] - 8s 10ms/step - loss: 0.1197\n",
      "Epoch 326/500\n",
      "756/756 [==============================] - 8s 10ms/step - loss: 0.1242\n",
      "Epoch 327/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0951\n",
      "Epoch 328/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.1097\n",
      "Epoch 329/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0851\n",
      "Epoch 330/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0828\n",
      "Epoch 331/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.1089\n",
      "Epoch 332/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.1111\n",
      "Epoch 333/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.1024\n",
      "Epoch 334/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0992\n",
      "Epoch 335/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0960\n",
      "Epoch 336/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.1046\n",
      "Epoch 337/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1096\n",
      "Epoch 338/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.0833\n",
      "Epoch 339/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.0739\n",
      "Epoch 340/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.0847\n",
      "Epoch 341/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1488\n",
      "Epoch 342/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.2332\n",
      "Epoch 343/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1199\n",
      "Epoch 344/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.0477\n",
      "Epoch 345/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.0321\n",
      "Epoch 346/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.0335\n",
      "Epoch 347/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.0693\n",
      "Epoch 348/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1265\n",
      "Epoch 349/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1316\n",
      "Epoch 350/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.0846\n",
      "Epoch 351/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.0715\n",
      "Epoch 352/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.0910\n",
      "Epoch 353/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.0931\n",
      "Epoch 354/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.1091\n",
      "Epoch 355/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1032\n",
      "Epoch 356/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.0797\n",
      "Epoch 357/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.0680\n",
      "Epoch 358/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.0943\n",
      "Epoch 359/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1101\n",
      "Epoch 360/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.0997\n",
      "Epoch 361/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0821\n",
      "Epoch 362/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.0848\n",
      "Epoch 363/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.0921\n",
      "Epoch 364/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.0904\n",
      "Epoch 365/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.0778\n",
      "Epoch 366/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1480\n",
      "Epoch 367/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.4302\n",
      "Epoch 368/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1290\n",
      "Epoch 369/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.0460\n",
      "Epoch 370/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.0288\n",
      "Epoch 371/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.0238\n",
      "Epoch 372/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.0258\n",
      "Epoch 373/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.0561\n",
      "Epoch 374/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1448\n",
      "Epoch 375/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.1476\n",
      "Epoch 376/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0661\n",
      "Epoch 377/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0536\n",
      "Epoch 378/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.0630\n",
      "Epoch 379/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.1004\n",
      "Epoch 380/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.1280\n",
      "Epoch 381/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.2036\n",
      "Epoch 382/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.1311\n",
      "Epoch 383/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.0515\n",
      "Epoch 384/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.0352\n",
      "Epoch 385/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0449\n",
      "Epoch 386/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0870\n",
      "Epoch 387/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.1162\n",
      "Epoch 388/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.0942\n",
      "Epoch 389/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0717\n",
      "Epoch 390/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "756/756 [==============================] - 6s 8ms/step - loss: 0.0659\n",
      "Epoch 391/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.0732\n",
      "Epoch 392/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.0914\n",
      "Epoch 393/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.0861\n",
      "Epoch 394/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.0729\n",
      "Epoch 395/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.0831\n",
      "Epoch 396/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.0870\n",
      "Epoch 397/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.0858\n",
      "Epoch 398/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.0731\n",
      "Epoch 399/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.0744\n",
      "Epoch 400/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.0925\n",
      "Epoch 401/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.0845\n",
      "Epoch 402/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.0758\n",
      "Epoch 403/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.0638\n",
      "Epoch 404/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.0790\n",
      "Epoch 405/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.0955\n",
      "Epoch 406/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.0822\n",
      "Epoch 407/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.0600\n",
      "Epoch 408/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.0702\n",
      "Epoch 409/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.0759\n",
      "Epoch 410/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.0813\n",
      "Epoch 411/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0783\n",
      "Epoch 412/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.0682\n",
      "Epoch 413/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.0685\n",
      "Epoch 414/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.0684\n",
      "Epoch 415/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.0865\n",
      "Epoch 416/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0830\n",
      "Epoch 417/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0675\n",
      "Epoch 418/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0662\n",
      "Epoch 419/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.0722\n",
      "Epoch 420/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.0682\n",
      "Epoch 421/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.0769\n",
      "Epoch 422/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0838\n",
      "Epoch 423/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.0867\n",
      "Epoch 424/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0626\n",
      "Epoch 425/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0538\n",
      "Epoch 426/500\n",
      "756/756 [==============================] - 9s 12ms/step - loss: 0.0753\n",
      "Epoch 427/500\n",
      "756/756 [==============================] - 7s 10ms/step - loss: 0.0779\n",
      "Epoch 428/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0661\n",
      "Epoch 429/500\n",
      "756/756 [==============================] - 7s 10ms/step - loss: 0.0611\n",
      "Epoch 430/500\n",
      "756/756 [==============================] - 7s 10ms/step - loss: 0.0625\n",
      "Epoch 431/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0694\n",
      "Epoch 432/500\n",
      "756/756 [==============================] - 7s 10ms/step - loss: 0.0803\n",
      "Epoch 433/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0825\n",
      "Epoch 434/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0671\n",
      "Epoch 435/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0518\n",
      "Epoch 436/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0705\n",
      "Epoch 437/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0802\n",
      "Epoch 438/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0605\n",
      "Epoch 439/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0664\n",
      "Epoch 440/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0734\n",
      "Epoch 441/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0677\n",
      "Epoch 442/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0716\n",
      "Epoch 443/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.0613\n",
      "Epoch 444/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0648\n",
      "Epoch 445/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0598\n",
      "Epoch 446/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.0619\n",
      "Epoch 447/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0616\n",
      "Epoch 448/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0747\n",
      "Epoch 449/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0682\n",
      "Epoch 450/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0636\n",
      "Epoch 451/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0575\n",
      "Epoch 452/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.0630\n",
      "Epoch 453/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.0656\n",
      "Epoch 454/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.0649\n",
      "Epoch 455/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.0714\n",
      "Epoch 456/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.0613\n",
      "Epoch 457/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.0528\n",
      "Epoch 458/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.0738\n",
      "Epoch 459/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0697\n",
      "Epoch 460/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0548\n",
      "Epoch 461/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.0561\n",
      "Epoch 462/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.0717\n",
      "Epoch 463/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.0751\n",
      "Epoch 464/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.0647\n",
      "Epoch 465/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.0510\n",
      "Epoch 466/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.0522\n",
      "Epoch 467/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.0651\n",
      "Epoch 468/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.0701\n",
      "Epoch 469/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.0598\n",
      "Epoch 470/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.0559\n",
      "Epoch 471/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.0548\n",
      "Epoch 472/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.0647\n",
      "Epoch 473/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.0615\n",
      "Epoch 474/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.0540\n",
      "Epoch 475/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.0596\n",
      "Epoch 476/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.0634\n",
      "Epoch 477/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.0672\n",
      "Epoch 478/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.0574\n",
      "Epoch 479/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0575\n",
      "Epoch 480/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0624\n",
      "Epoch 481/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0650\n",
      "Epoch 482/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0489\n",
      "Epoch 483/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0547\n",
      "Epoch 484/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0797\n",
      "Epoch 485/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0617\n",
      "Epoch 486/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0467\n",
      "Epoch 487/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0454\n",
      "Epoch 488/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0544\n",
      "Epoch 489/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0604\n",
      "Epoch 490/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.0664\n",
      "Epoch 491/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.0644\n",
      "Epoch 492/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0576\n",
      "Epoch 493/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0493\n",
      "Epoch 494/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.0494\n",
      "Epoch 495/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0643\n",
      "Epoch 496/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0693\n",
      "Epoch 497/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0592\n",
      "Epoch 498/500\n",
      "756/756 [==============================] - 6s 8ms/step - loss: 0.0454\n",
      "Epoch 499/500\n",
      "756/756 [==============================] - 7s 9ms/step - loss: 0.0413\n",
      "Epoch 500/500\n",
      "756/756 [==============================] - 6s 9ms/step - loss: 0.0547\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n- epochs 467 - 0.1900\\n- epochs 100 해서 확인해보면 loss 가 너무 들쑥날쑥한 걸 확인할 수 있음 1.22\\n- epochs 500 0.2346\\n\\n- 데이터 셋 변경 epochs 500, loss - 0.0572, lr-0.001\\n  lr-0.003\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, LSTM, RepeatVector, Lambda\n",
    "from keras.models import Model\n",
    "\n",
    "def repeat_vector(args):\n",
    "    layer_to_repeat = args[0]\n",
    "    sequence_layer = args[1]\n",
    "    return RepeatVector(K.shape(sequence_layer)[1])(layer_to_repeat)\n",
    "    \n",
    "\n",
    "inputs = Input(shape=(None, 3))\n",
    "encoded = LSTM(128, return_sequences=True)(inputs)  #activation 안적으면 tanh\n",
    "encoded = LSTM(64)(encoded)\n",
    "\n",
    "decoded = Lambda(repeat_vector, output_shape=(None, 64)) ([encoded, inputs]) # inputs의 shape[1] 만큼 encoded 를 반복 생성\n",
    "\n",
    "decoded = LSTM(64, return_sequences=True)(decoded)\n",
    "decoded = LSTM(128, return_sequences=True)(decoded)\n",
    "decoded = TimeDistributed(Dense(3))(decoded)\n",
    "encoder = Model(inputs, encoded)\n",
    "\n",
    "lstm_autoencoder = Model(inputs, decoded)\n",
    "lstm_autoencoder.compile(loss='mean_squared_error', optimizer=Adam(lr=0.0006))\n",
    "\n",
    "def train_generator(x_train):\n",
    "    idx = 0\n",
    "    while True:\n",
    "        yield np.array([x_train[idx]]), np.array([x_train[idx]])\n",
    "        idx +=1\n",
    "        if idx >= len(x_train):\n",
    "            idx = 0\n",
    "lstm_autoencoder.fit_generator(train_generator(x_train), epochs=500, steps_per_epoch=756, verbose=1)#, validation_data=val_generator(x_val))\n",
    "\n",
    "'''\n",
    "- epochs 467 - 0.1900\n",
    "- epochs 100 해서 확인해보면 loss 가 너무 들쑥날쑥한 걸 확인할 수 있음 1.22\n",
    "- epochs 500 0.2346\n",
    "\n",
    "- 데이터 셋 변경 epochs 500, loss - 0.0572, lr-0.001\n",
    "  lr-0.0006 0.0547\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(inputs, encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_22\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, None, 3)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_41 (LSTM)                  (None, None, 128)    67584       input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_42 (LSTM)                  (None, 64)           49408       lstm_41[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, None, 64)     0           lstm_42[0][0]                    \n",
      "                                                                 input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_43 (LSTM)                  (None, None, 64)     33024       lambda_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_44 (LSTM)                  (None, None, 128)    98816       lstm_43[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_11 (TimeDistri (None, None, 3)      387         lstm_44[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 249,219\n",
      "Trainable params: 249,219\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        (None, None, 3)           0         \n",
      "_________________________________________________________________\n",
      "lstm_41 (LSTM)               (None, None, 128)         67584     \n",
      "_________________________________________________________________\n",
      "lstm_42 (LSTM)               (None, 64)                49408     \n",
      "=================================================================\n",
      "Total params: 116,992\n",
      "Trainable params: 116,992\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_22\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, None, 3)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_41 (LSTM)                  (None, None, 128)    67584       input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_42 (LSTM)                  (None, 64)           49408       lstm_41[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, None, 64)     0           lstm_42[0][0]                    \n",
      "                                                                 input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_43 (LSTM)                  (None, None, 64)     33024       lambda_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_44 (LSTM)                  (None, None, 128)    98816       lstm_43[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_11 (TimeDistri (None, None, 3)      387         lstm_44[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 249,219\n",
      "Trainable params: 249,219\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 8.931337 ,  7.4579573, 31.943764 ],\n",
       "        [ 8.431695 ,  7.737891 ,  8.375352 ],\n",
       "        [ 7.3948007,  5.0614414, 13.785835 ],\n",
       "        [ 7.925072 ,  5.933628 , 14.485416 ],\n",
       "        [ 2.0842404,  1.931783 ,  8.705457 ],\n",
       "        [ 2.2869372,  3.9760273,  7.5900183],\n",
       "        [ 7.5529623,  5.3252344, 26.848215 ],\n",
       "        [ 2.7739074,  3.8996158,  8.020057 ],\n",
       "        [ 1.5346651,  4.882212 , 11.469937 ]]], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_autoencoder.predict(x_test[0].reshape(1, x_test[0].shape[0], x_test[0].shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.  ,  2.  , 10.06],\n",
       "       [ 2.  ,  1.  ,  8.79],\n",
       "       [ 2.  ,  5.  ,  6.86],\n",
       "       [ 1.  ,  3.  , 11.79],\n",
       "       [ 3.  ,  6.  ,  7.86]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.  ,  3.  , 23.54],\n",
       "       [ 5.  ,  4.  , 42.39],\n",
       "       [ 3.  ,  1.  ,  7.22],\n",
       "       [ 4.  ,  2.  , 12.22],\n",
       "       [ 1.  ,  2.  ,  5.7 ]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.  ,  2.  ,  3.3 ],\n",
       "       [ 1.  ,  3.  ,  6.3 ],\n",
       "       [ 2.  ,  4.  , 12.72],\n",
       "       [ 3.  ,  5.  , 23.87],\n",
       "       [ 4.  ,  5.  , 41.95]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = x_test[0].reshape(1, 9, 3)\n",
    "latent_vector = []\n",
    "for x in x_test:\n",
    "    x = x.reshape(1, x.shape[0], x.shape[1])\n",
    "    latent_vector.append(encoder.predict(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.0,\n",
       " -0.00020211579,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.74070954,\n",
       " -0.029288653,\n",
       " -0.74629986,\n",
       " 0.0,\n",
       " 0.12747759,\n",
       " -0.0014260766,\n",
       " -0.0,\n",
       " 0.10494814,\n",
       " 0.2187337,\n",
       " -0.18827741,\n",
       " 0.040405355,\n",
       " -0.5645764,\n",
       " 0.0022893408,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.2676836,\n",
       " -0.7395759,\n",
       " 0.2642008,\n",
       " -0.41065842,\n",
       " 0.19330677,\n",
       " -0.3758831,\n",
       " -0.21791713,\n",
       " 0.055199105,\n",
       " -0.25927162,\n",
       " 0.23109113,\n",
       " 0.8425839,\n",
       " -0.03407281,\n",
       " -0.8132615,\n",
       " 0.0,\n",
       " 0.49131706,\n",
       " 0.16162786,\n",
       " 0.35315764,\n",
       " -0.0,\n",
       " -0.7594612,\n",
       " -0.6364968,\n",
       " 0.0,\n",
       " 0.009271684,\n",
       " 0.0,\n",
       " 0.30094486,\n",
       " 0.0,\n",
       " -0.0027280087,\n",
       " -0.25117594,\n",
       " 0.0,\n",
       " 0.19940257,\n",
       " 0.32026434,\n",
       " 0.5843708,\n",
       " 0.032265846,\n",
       " -0.0,\n",
       " -0.0140618635,\n",
       " -0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.0,\n",
       " 0.254268,\n",
       " -0.0,\n",
       " 0.0,\n",
       " -0.0,\n",
       " 0.19169566,\n",
       " 0.0,\n",
       " 0.7144473]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_vector = np.array(latent_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printMatrix(matrix):\n",
    "    length = int(max(max(matrix[:, 0]), max(matrix[:, 1])))\n",
    "    adj = [[0 for i in range(length)] for i in range(length)]\n",
    "    \n",
    "    for m in matrix:\n",
    "        (i, j, w) = m\n",
    "        i = int(i) -1\n",
    "        j = int(j) -1\n",
    "        adj[i][j] = adj[j][i] = w\n",
    "    for i in range(length):\n",
    "        for j in range(length):\n",
    "            print(str(adj[i][j]), end=',')\n",
    "        print()\n",
    "    print(\"===============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 0\n",
      "0,10.16,7.92,9.56,0,0,0,0,0,\n",
      "10.16,0,0,0,0,0,0,0,0,\n",
      "7.92,0,0,0,8.17,18.05,0,0,0,\n",
      "9.56,0,0,0,0,0,20.05,0,0,\n",
      "0,0,8.17,0,0,0,0,0,0,\n",
      "0,0,18.05,0,0,0,0,32.09,0,\n",
      "0,0,0,20.05,0,0,0,0,20.27,\n",
      "0,0,0,0,0,32.09,0,0,9.01,\n",
      "0,0,0,0,0,0,20.27,9.01,0,\n",
      "===============\n",
      "# 1\n",
      "0,8.79,11.79,0,0,0,\n",
      "8.79,0,0,10.06,6.86,0,\n",
      "11.79,0,0,0,0,7.86,\n",
      "0,10.06,0,0,0,0,\n",
      "0,6.86,0,0,0,0,\n",
      "0,0,7.86,0,0,0,\n",
      "===============\n",
      "# 2\n",
      "0,3.3,6.3,0,0,\n",
      "3.3,0,0,12.72,0,\n",
      "6.3,0,0,0,23.87,\n",
      "0,12.72,0,0,41.95,\n",
      "0,0,23.87,41.95,0,\n",
      "===============\n",
      "# 3\n",
      "0,5.7,7.22,0,0,\n",
      "5.7,0,0,12.22,0,\n",
      "7.22,0,0,0,23.54,\n",
      "0,12.22,0,0,42.39,\n",
      "0,0,23.54,42.39,0,\n",
      "===============\n",
      "# 4\n",
      "0,12.52,17.52,28.77,0,0,0,0,0,\n",
      "12.52,0,0,0,0,0,0,0,0,\n",
      "17.52,0,0,0,2.53,4.53,0,0,0,\n",
      "28.77,0,0,0,0,0,10.86,0,0,\n",
      "0,0,2.53,0,0,0,0,0,0,\n",
      "0,0,4.53,0,0,0,0,9.01,0,\n",
      "0,0,0,10.86,0,0,0,0,10.01,\n",
      "0,0,0,0,0,9.01,0,0,20.01,\n",
      "0,0,0,0,0,0,10.01,20.01,0,\n",
      "===============\n",
      "# 5\n",
      "0,4.27,9.29,0,0,\n",
      "4.27,0,0,12.29,0,\n",
      "9.29,0,0,0,17.29,\n",
      "0,12.29,0,0,3.95,\n",
      "0,0,17.29,3.95,0,\n",
      "===============\n",
      "# 6\n",
      "0,10.16,7.92,9.56,0,0,0,0,0,\n",
      "10.16,0,0,0,0,0,0,0,0,\n",
      "7.92,0,0,0,8.17,18.05,0,0,0,\n",
      "9.56,0,0,0,0,0,20.05,0,0,\n",
      "0,0,8.17,0,0,0,0,0,0,\n",
      "0,0,18.05,0,0,0,0,32.09,0,\n",
      "0,0,0,20.05,0,0,0,0,20.27,\n",
      "0,0,0,0,0,32.09,0,0,9.01,\n",
      "0,0,0,0,0,0,20.27,9.01,0,\n",
      "===============\n",
      "# 7\n",
      "0,9.1,12.1,23.12,0,0,\n",
      "9.1,0,11.14,0,21.31,0,\n",
      "12.1,11.14,0,0,0,38.69,\n",
      "23.12,0,0,0,0,0,\n",
      "0,21.31,0,0,0,76.3,\n",
      "0,0,38.69,0,76.3,0,\n",
      "===============\n",
      "# 8\n",
      "0,5.68,4.49,0,0,\n",
      "5.68,0,0,10.64,0,\n",
      "4.49,0,0,0,20.73,\n",
      "0,10.64,0,0,23.73,\n",
      "0,0,20.73,23.73,0,\n",
      "===============\n",
      "# 9\n",
      "0,2.81,9.59,11.34,0,\n",
      "2.81,0,21.48,0,8.05,\n",
      "9.59,21.48,0,0,0,\n",
      "11.34,0,0,0,0,\n",
      "0,8.05,0,0,0,\n",
      "===============\n",
      "# 10\n",
      "0,5.11,6.66,8.4,\n",
      "5.11,0,18.0,0,\n",
      "6.66,18.0,0,0,\n",
      "8.4,0,0,0,\n",
      "===============\n",
      "# 11\n",
      "0,5.95,15.4,29.79,0,0,\n",
      "5.95,0,14.98,0,4.4,0,\n",
      "15.4,14.98,0,0,0,13.89,\n",
      "29.79,0,0,0,0,0,\n",
      "0,4.4,0,0,0,24.58,\n",
      "0,0,13.89,0,24.58,0,\n",
      "===============\n",
      "# 12\n",
      "0,6.41,4.09,0,0,0,\n",
      "6.41,0,0,8.09,16.24,0,\n",
      "4.09,0,0,0,0,4.33,\n",
      "0,8.09,0,0,0,0,\n",
      "0,16.24,0,0,0,0,\n",
      "0,0,4.33,0,0,0,\n",
      "===============\n",
      "# 13\n",
      "0,5.46,10.39,0,0,0,0,0,0,0,\n",
      "5.46,0,0,4.69,6.99,0,0,0,0,0,\n",
      "10.39,0,0,0,0,6.57,0,0,0,0,\n",
      "0,4.69,0,0,0,0,5.81,8.41,0,0,\n",
      "0,6.99,0,0,0,0,0,0,0,0,\n",
      "0,0,6.57,0,0,0,0,0,11.41,0,\n",
      "0,0,0,5.81,0,0,0,0,0,0,\n",
      "0,0,0,8.41,0,0,0,0,0,0,\n",
      "0,0,0,0,0,11.41,0,0,0,22.01,\n",
      "0,0,0,0,0,0,0,0,22.01,0,\n",
      "===============\n",
      "# 14\n",
      "0,7.91,9.61,0,0,0,0,\n",
      "7.91,0,0,19.41,7.36,0,0,\n",
      "9.61,0,0,0,0,15.61,11.39,\n",
      "0,19.41,0,0,0,0,0,\n",
      "0,7.36,0,0,0,0,0,\n",
      "0,0,15.61,0,0,0,0,\n",
      "0,0,11.39,0,0,0,0,\n",
      "===============\n",
      "# 15\n",
      "0,2.49,10.47,0,0,0,0,\n",
      "2.49,0,0,20.49,24.49,0,0,\n",
      "10.47,0,0,0,0,0,0,\n",
      "0,20.49,0,0,0,44.3,60.58,\n",
      "0,24.49,0,0,0,0,0,\n",
      "0,0,0,44.3,0,0,0,\n",
      "0,0,0,60.58,0,0,0,\n",
      "===============\n",
      "# 16\n",
      "0,9.84,14.84,0,0,0,0,0,0,0,\n",
      "9.84,0,0,28.34,52.46,0,0,0,0,0,\n",
      "14.84,0,0,0,0,20.74,0,0,0,0,\n",
      "0,28.34,0,0,0,0,1.8,8.07,0,0,\n",
      "0,52.46,0,0,0,0,0,0,0,0,\n",
      "0,0,20.74,0,0,0,0,0,10.07,0,\n",
      "0,0,0,1.8,0,0,0,0,0,0,\n",
      "0,0,0,8.07,0,0,0,0,0,0,\n",
      "0,0,0,0,0,10.07,0,0,0,20.12,\n",
      "0,0,0,0,0,0,0,0,20.12,0,\n",
      "===============\n",
      "# 17\n",
      "0,5.68,4.49,0,0,\n",
      "5.68,0,0,10.64,0,\n",
      "4.49,0,0,0,20.73,\n",
      "0,10.64,0,0,23.73,\n",
      "0,0,20.73,23.73,0,\n",
      "===============\n",
      "# 18\n",
      "0,7.44,9.44,7.01,0,0,\n",
      "7.44,0,9.01,0,9.95,0,\n",
      "9.44,9.01,0,0,0,6.37,\n",
      "7.01,0,0,0,0,0,\n",
      "0,9.95,0,0,0,5.54,\n",
      "0,0,6.37,0,5.54,0,\n",
      "===============\n",
      "# 19\n",
      "0,11.21,1.03,3.72,0,\n",
      "11.21,0,7.76,0,6.56,\n",
      "1.03,7.76,0,0,0,\n",
      "3.72,0,0,0,0,\n",
      "0,6.56,0,0,0,\n",
      "===============\n",
      "# 20\n",
      "0,4.71,7.93,0,0,0,0,0,0,0,\n",
      "4.71,0,0,16.93,3.43,0,0,0,0,0,\n",
      "7.93,0,0,0,0,9.66,0,0,0,0,\n",
      "0,16.93,0,0,0,0,8.09,18.05,0,0,\n",
      "0,3.43,0,0,0,0,0,0,0,0,\n",
      "0,0,9.66,0,0,0,0,0,30.86,0,\n",
      "0,0,0,8.09,0,0,0,0,0,0,\n",
      "0,0,0,18.05,0,0,0,0,0,0,\n",
      "0,0,0,0,0,30.86,0,0,0,31.86,\n",
      "0,0,0,0,0,0,0,0,31.86,0,\n",
      "===============\n",
      "# 21\n",
      "0,4.54,12.82,23.2,0,0,\n",
      "4.54,0,12.56,0,12.48,0,\n",
      "12.82,12.56,0,0,0,2.44,\n",
      "23.2,0,0,0,0,0,\n",
      "0,12.48,0,0,0,6.95,\n",
      "0,0,2.44,0,6.95,0,\n",
      "===============\n",
      "# 22\n",
      "0,9.7,5.1,5.12,0,0,0,0,0,\n",
      "9.7,0,0,0,0,0,0,0,0,\n",
      "5.1,0,0,0,13.29,10.61,0,0,0,\n",
      "5.12,0,0,0,0,0,20.94,0,0,\n",
      "0,0,13.29,0,0,0,0,0,0,\n",
      "0,0,10.61,0,0,0,0,8.35,0,\n",
      "0,0,0,20.94,0,0,0,0,6.14,\n",
      "0,0,0,0,0,8.35,0,0,14.26,\n",
      "0,0,0,0,0,0,6.14,14.26,0,\n",
      "===============\n",
      "# 23\n",
      "0,6.81,0,0,\n",
      "6.81,0,9.81,11.46,\n",
      "0,9.81,0,21.99,\n",
      "0,11.46,21.99,0,\n",
      "===============\n",
      "# 24\n",
      "0,8.12,16.82,0,0,\n",
      "8.12,0,0,3.04,0,\n",
      "16.82,0,0,0,9.05,\n",
      "0,3.04,0,0,6.2,\n",
      "0,0,9.05,6.2,0,\n",
      "===============\n",
      "# 25\n",
      "0,5.09,2.06,4.17,0,0,\n",
      "5.09,0,11.95,0,13.95,0,\n",
      "2.06,11.95,0,0,0,2.21,\n",
      "4.17,0,0,0,0,0,\n",
      "0,13.95,0,0,0,11.03,\n",
      "0,0,2.21,0,11.03,0,\n",
      "===============\n",
      "# 26\n",
      "0,5.46,10.39,0,0,0,0,0,0,0,\n",
      "5.46,0,0,4.69,6.99,0,0,0,0,0,\n",
      "10.39,0,0,0,0,6.57,0,0,0,0,\n",
      "0,4.69,0,0,0,0,5.81,8.41,0,0,\n",
      "0,6.99,0,0,0,0,0,0,0,0,\n",
      "0,0,6.57,0,0,0,0,0,11.41,0,\n",
      "0,0,0,5.81,0,0,0,0,0,0,\n",
      "0,0,0,8.41,0,0,0,0,0,0,\n",
      "0,0,0,0,0,11.41,0,0,0,22.01,\n",
      "0,0,0,0,0,0,0,0,22.01,0,\n",
      "===============\n",
      "# 27\n",
      "0,5.09,2.06,4.17,0,0,\n",
      "5.09,0,11.95,0,13.95,0,\n",
      "2.06,11.95,0,0,0,2.21,\n",
      "4.17,0,0,0,0,0,\n",
      "0,13.95,0,0,0,11.03,\n",
      "0,0,2.21,0,11.03,0,\n",
      "===============\n",
      "# 28\n",
      "0,4.16,7.16,0,0,0,0,\n",
      "4.16,0,0,16.83,28.23,0,0,\n",
      "7.16,0,0,0,0,33.23,62.71,\n",
      "0,16.83,0,0,0,0,0,\n",
      "0,28.23,0,0,0,0,0,\n",
      "0,0,33.23,0,0,0,0,\n",
      "0,0,62.71,0,0,0,0,\n",
      "===============\n",
      "# 29\n",
      "0,6.13,5.95,0,0,0,0,0,0,0,\n",
      "6.13,0,0,10.95,21.64,0,0,0,0,0,\n",
      "5.95,0,0,0,0,23.64,0,0,0,0,\n",
      "0,10.95,0,0,0,0,6.23,8.77,0,0,\n",
      "0,21.64,0,0,0,0,0,0,0,0,\n",
      "0,0,23.64,0,0,0,0,0,10.77,0,\n",
      "0,0,0,6.23,0,0,0,0,0,0,\n",
      "0,0,0,8.77,0,0,0,0,0,0,\n",
      "0,0,0,0,0,10.77,0,0,0,1.34,\n",
      "0,0,0,0,0,0,0,0,1.34,0,\n",
      "===============\n",
      "# 30\n",
      "0,10.37,8.48,6.98,0,0,0,0,0,\n",
      "10.37,0,0,0,0,0,0,0,0,\n",
      "8.48,0,0,0,12.67,22.75,0,0,0,\n",
      "6.98,0,0,0,0,0,27.75,0,0,\n",
      "0,0,12.67,0,0,0,0,0,0,\n",
      "0,0,22.75,0,0,0,0,2.0,0,\n",
      "0,0,0,27.75,0,0,0,0,7.78,\n",
      "0,0,0,0,0,2.0,0,0,15.79,\n",
      "0,0,0,0,0,0,7.78,15.79,0,\n",
      "===============\n",
      "# 31\n",
      "0,3.09,4.09,0,0,0,0,0,0,0,\n",
      "3.09,0,0,7.31,6.24,0,0,0,0,0,\n",
      "4.09,0,0,0,0,10.78,0,0,0,0,\n",
      "0,7.31,0,0,0,0,5.75,8.53,0,0,\n",
      "0,6.24,0,0,0,0,0,0,0,0,\n",
      "0,0,10.78,0,0,0,0,0,6.23,0,\n",
      "0,0,0,5.75,0,0,0,0,0,0,\n",
      "0,0,0,8.53,0,0,0,0,0,0,\n",
      "0,0,0,0,0,6.23,0,0,0,9.5,\n",
      "0,0,0,0,0,0,0,0,9.5,0,\n",
      "===============\n",
      "# 32\n",
      "0,7.38,6.51,0,0,0,0,0,0,0,\n",
      "7.38,0,0,9.96,10.96,0,0,0,0,0,\n",
      "6.51,0,0,0,0,21.85,0,0,0,0,\n",
      "0,9.96,0,0,0,0,38.35,27.88,0,0,\n",
      "0,10.96,0,0,0,0,0,0,0,0,\n",
      "0,0,21.85,0,0,0,0,0,37.89,0,\n",
      "0,0,0,38.35,0,0,0,0,0,0,\n",
      "0,0,0,27.88,0,0,0,0,0,0,\n",
      "0,0,0,0,0,37.89,0,0,0,39.89,\n",
      "0,0,0,0,0,0,0,0,39.89,0,\n",
      "===============\n",
      "# 33\n",
      "0,8.46,9.46,0,0,0,0,\n",
      "8.46,0,0,8.28,11.91,0,0,\n",
      "9.46,0,0,0,0,0,0,\n",
      "0,8.28,0,0,0,1.22,9.42,\n",
      "0,11.91,0,0,0,0,0,\n",
      "0,0,0,1.22,0,0,0,\n",
      "0,0,0,9.42,0,0,0,\n",
      "===============\n",
      "# 34\n",
      "0,4.35,3.05,12.89,0,\n",
      "4.35,0,17.89,0,28.52,\n",
      "3.05,17.89,0,0,0,\n",
      "12.89,0,0,0,0,\n",
      "0,28.52,0,0,0,\n",
      "===============\n",
      "# 35\n",
      "0,7.21,4.47,0,0,0,0,\n",
      "7.21,0,0,13.78,2.09,0,0,\n",
      "4.47,0,0,0,0,4.09,5.11,\n",
      "0,13.78,0,0,0,0,0,\n",
      "0,2.09,0,0,0,0,0,\n",
      "0,0,4.09,0,0,0,0,\n",
      "0,0,5.11,0,0,0,0,\n",
      "===============\n",
      "# 36\n",
      "0,8.54,12.54,0,0,0,0,\n",
      "8.54,0,0,1.84,9.02,0,0,\n",
      "12.54,0,0,0,0,7.16,12.16,\n",
      "0,1.84,0,0,0,0,0,\n",
      "0,9.02,0,0,0,0,0,\n",
      "0,0,7.16,0,0,0,0,\n",
      "0,0,12.16,0,0,0,0,\n",
      "===============\n",
      "# 37\n",
      "0,4.65,9.65,0,0,0,\n",
      "4.65,0,0,6.96,4.92,0,\n",
      "9.65,0,0,0,0,5.92,\n",
      "0,6.96,0,0,0,0,\n",
      "0,4.92,0,0,0,0,\n",
      "0,0,5.92,0,0,0,\n",
      "===============\n",
      "# 38\n",
      "0,8.6,8.71,18.02,0,0,0,0,0,\n",
      "8.6,0,0,0,0,0,0,0,0,\n",
      "8.71,0,0,0,1.3,9.61,0,0,0,\n",
      "18.02,0,0,0,0,0,19.5,0,0,\n",
      "0,0,1.3,0,0,0,0,0,0,\n",
      "0,0,9.61,0,0,0,0,6.15,0,\n",
      "0,0,0,19.5,0,0,0,0,4.59,\n",
      "0,0,0,0,0,6.15,0,0,5.8,\n",
      "0,0,0,0,0,0,4.59,5.8,0,\n",
      "===============\n",
      "# 39\n",
      "0,11.2,9.42,0,0,0,0,\n",
      "11.2,0,0,8.63,11.63,0,0,\n",
      "9.42,0,0,0,0,0,0,\n",
      "0,8.63,0,0,0,6.9,10.69,\n",
      "0,11.63,0,0,0,0,0,\n",
      "0,0,0,6.9,0,0,0,\n",
      "0,0,0,10.69,0,0,0,\n",
      "===============\n",
      "# 40\n",
      "0,11.32,22.47,7.57,0,0,\n",
      "11.32,0,15.33,0,5.29,0,\n",
      "22.47,15.33,0,0,0,5.21,\n",
      "7.57,0,0,0,0,0,\n",
      "0,5.29,0,0,0,13.0,\n",
      "0,0,5.21,0,13.0,0,\n",
      "===============\n",
      "# 41\n",
      "0,7.64,5.28,10.28,0,0,\n",
      "7.64,0,9.69,0,10.17,0,\n",
      "5.28,9.69,0,0,0,20.32,\n",
      "10.28,0,0,0,0,0,\n",
      "0,10.17,0,0,0,21.32,\n",
      "0,0,20.32,0,21.32,0,\n",
      "===============\n",
      "# 42\n",
      "0,2.73,7.31,10.0,0,0,0,0,0,\n",
      "2.73,0,0,0,0,0,0,0,0,\n",
      "7.31,0,0,0,6.1,11.5,0,0,0,\n",
      "10.0,0,0,0,0,0,1.37,0,0,\n",
      "0,0,6.1,0,0,0,0,0,0,\n",
      "0,0,11.5,0,0,0,0,10.51,0,\n",
      "0,0,0,1.37,0,0,0,0,9.33,\n",
      "0,0,0,0,0,10.51,0,0,11.07,\n",
      "0,0,0,0,0,0,9.33,11.07,0,\n",
      "===============\n",
      "# 43\n",
      "0,10.47,15.47,26.68,0,\n",
      "10.47,0,5.92,0,9.13,\n",
      "15.47,5.92,0,0,0,\n",
      "26.68,0,0,0,0,\n",
      "0,9.13,0,0,0,\n",
      "===============\n",
      "# 44\n",
      "0,5.13,3.73,0,0,0,0,\n",
      "5.13,0,0,12.26,14.26,0,0,\n",
      "3.73,0,0,0,0,6.24,14.5,\n",
      "0,12.26,0,0,0,0,0,\n",
      "0,14.26,0,0,0,0,0,\n",
      "0,0,6.24,0,0,0,0,\n",
      "0,0,14.5,0,0,0,0,\n",
      "===============\n",
      "# 45\n",
      "0,7.44,9.44,7.01,0,0,\n",
      "7.44,0,9.01,0,9.95,0,\n",
      "9.44,9.01,0,0,0,6.37,\n",
      "7.01,0,0,0,0,0,\n",
      "0,9.95,0,0,0,5.54,\n",
      "0,0,6.37,0,5.54,0,\n",
      "===============\n",
      "# 46\n",
      "0,3.57,9.11,0,0,0,0,\n",
      "3.57,0,0,8.53,12.29,0,0,\n",
      "9.11,0,0,0,0,0,0,\n",
      "0,8.53,0,0,0,2.17,9.34,\n",
      "0,12.29,0,0,0,0,0,\n",
      "0,0,0,2.17,0,0,0,\n",
      "0,0,0,9.34,0,0,0,\n",
      "===============\n",
      "# 47\n",
      "0,6.13,5.95,0,0,0,0,0,0,0,\n",
      "6.13,0,0,10.95,21.64,0,0,0,0,0,\n",
      "5.95,0,0,0,0,23.64,0,0,0,0,\n",
      "0,10.95,0,0,0,0,6.23,8.77,0,0,\n",
      "0,21.64,0,0,0,0,0,0,0,0,\n",
      "0,0,23.64,0,0,0,0,0,10.77,0,\n",
      "0,0,0,6.23,0,0,0,0,0,0,\n",
      "0,0,0,8.77,0,0,0,0,0,0,\n",
      "0,0,0,0,0,10.77,0,0,0,1.34,\n",
      "0,0,0,0,0,0,0,0,1.34,0,\n",
      "===============\n",
      "# 48\n",
      "0,6.67,11.67,0,0,0,0,\n",
      "6.67,0,0,22.32,39.93,0,0,\n",
      "11.67,0,0,0,0,0,0,\n",
      "0,22.32,0,0,0,41.93,3.97,\n",
      "0,39.93,0,0,0,0,0,\n",
      "0,0,0,41.93,0,0,0,\n",
      "0,0,0,3.97,0,0,0,\n",
      "===============\n",
      "# 49\n",
      "0,5.97,10.97,21.23,0,0,\n",
      "5.97,0,23.23,0,10.81,0,\n",
      "10.97,23.23,0,0,0,6.14,\n",
      "21.23,0,0,0,0,0,\n",
      "0,10.81,0,0,0,6.19,\n",
      "0,0,6.14,0,6.19,0,\n",
      "===============\n",
      "# 50\n",
      "0,5.87,4.42,0,0,\n",
      "5.87,0,0,11.89,0,\n",
      "4.42,0,0,0,1.67,\n",
      "0,11.89,0,0,6.67,\n",
      "0,0,1.67,6.67,0,\n",
      "===============\n",
      "# 51\n",
      "0,6.45,15.17,2.95,\n",
      "6.45,0,7.95,0,\n",
      "15.17,7.95,0,0,\n",
      "2.95,0,0,0,\n",
      "===============\n",
      "# 52\n",
      "0,3.71,10.29,0,0,0,\n",
      "3.71,0,0,10.61,15.61,0,\n",
      "10.29,0,0,0,0,27.3,\n",
      "0,10.61,0,0,0,0,\n",
      "0,15.61,0,0,0,0,\n",
      "0,0,27.3,0,0,0,\n",
      "===============\n",
      "# 53\n",
      "0,11.2,9.42,0,0,0,0,\n",
      "11.2,0,0,8.63,11.63,0,0,\n",
      "9.42,0,0,0,0,0,0,\n",
      "0,8.63,0,0,0,6.9,10.69,\n",
      "0,11.63,0,0,0,0,0,\n",
      "0,0,0,6.9,0,0,0,\n",
      "0,0,0,10.69,0,0,0,\n",
      "===============\n",
      "# 54\n",
      "0,10.37,8.48,6.98,0,0,0,0,0,\n",
      "10.37,0,0,0,0,0,0,0,0,\n",
      "8.48,0,0,0,12.67,22.75,0,0,0,\n",
      "6.98,0,0,0,0,0,27.75,0,0,\n",
      "0,0,12.67,0,0,0,0,0,0,\n",
      "0,0,22.75,0,0,0,0,2.0,0,\n",
      "0,0,0,27.75,0,0,0,0,7.78,\n",
      "0,0,0,0,0,2.0,0,0,15.79,\n",
      "0,0,0,0,0,0,7.78,15.79,0,\n",
      "===============\n",
      "# 55\n",
      "0,11.34,22.67,0,0,0,\n",
      "11.34,0,0,23.67,42.83,0,\n",
      "22.67,0,0,0,0,45.83,\n",
      "0,23.67,0,0,0,0,\n",
      "0,42.83,0,0,0,0,\n",
      "0,0,45.83,0,0,0,\n",
      "===============\n",
      "# 56\n",
      "0,6.82,0,0,\n",
      "6.82,0,16.66,18.66,\n",
      "0,16.66,0,5.64,\n",
      "0,18.66,5.64,0,\n",
      "===============\n",
      "# 57\n",
      "0,6.0,8.84,10.6,0,0,\n",
      "6.0,0,10.99,0,12.99,0,\n",
      "8.84,10.99,0,0,0,1.37,\n",
      "10.6,0,0,0,0,0,\n",
      "0,12.99,0,0,0,3.77,\n",
      "0,0,1.37,0,3.77,0,\n",
      "===============\n",
      "# 58\n",
      "0,8.17,17.68,0,0,0,0,0,0,0,\n",
      "8.17,0,0,28.82,14.87,0,0,0,0,0,\n",
      "17.68,0,0,0,0,17.87,0,0,0,0,\n",
      "0,28.82,0,0,0,0,34.08,64.24,0,0,\n",
      "0,14.87,0,0,0,0,0,0,0,0,\n",
      "0,0,17.87,0,0,0,0,0,52.76,0,\n",
      "0,0,0,34.08,0,0,0,0,0,0,\n",
      "0,0,0,64.24,0,0,0,0,0,0,\n",
      "0,0,0,0,0,52.76,0,0,0,74.48,\n",
      "0,0,0,0,0,0,0,0,74.48,0,\n",
      "===============\n",
      "# 59\n",
      "0,12.31,13.31,0,0,0,\n",
      "12.31,0,0,1.24,9.71,0,\n",
      "13.31,0,0,0,0,19.48,\n",
      "0,1.24,0,0,0,0,\n",
      "0,9.71,0,0,0,0,\n",
      "0,0,19.48,0,0,0,\n",
      "===============\n",
      "# 60\n",
      "0,6.92,7.92,4.57,\n",
      "6.92,0,8.77,0,\n",
      "7.92,8.77,0,0,\n",
      "4.57,0,0,0,\n",
      "===============\n",
      "# 61\n",
      "0,9.7,5.1,5.12,0,0,0,0,0,\n",
      "9.7,0,0,0,0,0,0,0,0,\n",
      "5.1,0,0,0,13.29,10.61,0,0,0,\n",
      "5.12,0,0,0,0,0,20.94,0,0,\n",
      "0,0,13.29,0,0,0,0,0,0,\n",
      "0,0,10.61,0,0,0,0,8.35,0,\n",
      "0,0,0,20.94,0,0,0,0,6.14,\n",
      "0,0,0,0,0,8.35,0,0,14.26,\n",
      "0,0,0,0,0,0,6.14,14.26,0,\n",
      "===============\n",
      "# 62\n",
      "0,8.12,16.82,0,0,\n",
      "8.12,0,0,3.04,0,\n",
      "16.82,0,0,0,9.05,\n",
      "0,3.04,0,0,6.2,\n",
      "0,0,9.05,6.2,0,\n",
      "===============\n",
      "# 63\n",
      "0,8.17,17.68,0,0,0,0,0,0,0,\n",
      "8.17,0,0,28.82,14.87,0,0,0,0,0,\n",
      "17.68,0,0,0,0,17.87,0,0,0,0,\n",
      "0,28.82,0,0,0,0,34.08,64.24,0,0,\n",
      "0,14.87,0,0,0,0,0,0,0,0,\n",
      "0,0,17.87,0,0,0,0,0,52.76,0,\n",
      "0,0,0,34.08,0,0,0,0,0,0,\n",
      "0,0,0,64.24,0,0,0,0,0,0,\n",
      "0,0,0,0,0,52.76,0,0,0,74.48,\n",
      "0,0,0,0,0,0,0,0,74.48,0,\n",
      "===============\n"
     ]
    }
   ],
   "source": [
    "# sequence to matrix\n",
    "for index, x in enumerate(x_test):\n",
    "    print(\"#\", str(index))\n",
    "    printMatrix(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard = latent_vector[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)  (2, 38)  (3, 53)  (4, 5)  (5, 15)  (6, 44)  (7, 48)  (8, 21)  (9, 12)  (10, 43)  (11, 9)  (12, 18)  (13, 31)  (14, 59)  (15, 58)  (16, 4)  (17, 41)  (18, 27)  (19, 29)  (20, 33)  (21, 6)  (22, 3)  (23, 51)  (24, 35)  (25, 54)  (26, 55)  (27, 26)  (28, 62)  (29, 14)  (30, 46)  (31, 11)  (32, 45)  (33, 7)  (34, 50)  (35, 57)  (36, 8)  (37, 63)  (38, 60)  (39, 24)  (40, 52)  (41, 20)  (42, 13)  (43, 37)  (44, 23)  (45, 10)  (46, 1)  (47, 61)  (48, 42)  (49, 56)  (50, 34)  (51, 16)  (52, 32)  (53, 40)  (54, 39)  (55, 19)  (56, 17)  (57, 22)  (58, 49)  (59, 30)  (60, 36)  (61, 47)  (62, 28)  (63, 25)  "
     ]
    }
   ],
   "source": [
    "import copy\n",
    "dist = []\n",
    "left = 100000\n",
    "right = -10000\n",
    "for index, v in enumerate(latent_vector[1:]):\n",
    "    d = round(euclidean_distance(standard, v), 8)\n",
    "    dist.append(d)\n",
    "    if d > right:\n",
    "        right= d\n",
    "    if left > d:\n",
    "        left = d\n",
    "dist1 = copy.deepcopy(dist)\n",
    "dist1.sort()\n",
    "#print(dist, dist1)\n",
    "for index, d in enumerate(dist1):\n",
    "    print( (index+1, dist.index(d)+1), end='  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(a, b):\n",
    "    return np.sqrt(np.sum(a-b) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
